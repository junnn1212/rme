[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression Models for Epidemiology",
    "section": "",
    "text": "Preface\nThis web-book is derived from my lecture slides for Epidemiology 204: “Quantitative Epidemiology III: Statistical Models”, at UC Davis.\nI have drawn these materials from many sources, including but not limited to:",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#using-these-lecture-notes",
    "href": "index.html#using-these-lecture-notes",
    "title": "Regression Models for Epidemiology",
    "section": "Using these lecture notes",
    "text": "Using these lecture notes\nThese lecture notes are available online at https://d-morrison.github.io/rme/. The online notes are searchable and are currently being iteratively updated1. A pdf version of the notes is also downloadable from https://d-morrison.github.io/rme/Regression-Models-for-Epidemiology.pdf, and the source files are available at https://github.com/d-morrison/rme.\n\nCompiling chapters as lecture slide decks\nEach chapter’s source file can also be compiled as a lecture slide deck, using the _quarto-revealjs.yml Quarto profile included in the git repository on Github.\nFor example, to compile Chapter 3  Models for Binary Outcomes as a slide deck:\n\ninstall quarto\nclone the project repository from Github\nInstall the project dependencies using devtools:\n\nlibrary(devtools) # install from CRAN if needed\ndevtools::install_deps()\n\nRender the chapter using the revealjs profile using the following terminal shell command:\n\nquarto render logistic-regression.qmd --profile=revealjs\nYou can also render all the chapters listed in the _quarto-revealjs.yml Quarto profile as slide decks simultaneously:\nquarto render --profile=revealjs\n\n\n\nExtracting LaTeX commands from the online version of the notes\nIf you want to extract the LaTeX commands for any math expressions in the online lecture notes, you should be able to right-click and get this pop-up menu:\n\n\n\nFigure 1: Pop-up menu produced by right-clicking on math in online notes\n\n\n\n\n\n\nIf you select “TeX commands”, you will get a window with LaTeX code.2\n\n\n\nFigure 2: LaTeX source code window\n\n\n\n\n\n\nYou can also grab the TeX commands from the quarto source files on github, but those files use custom macros (defined in https://github.com/d-morrison/rme/blob/main/macros.qmd), so it’s a little harder to reuse code from the source files.\n\n\n\nDark Mode\nThe online notes have two color palette themes: light and dark. You can toggle between them using the oval button near the top-left corner:\n\n\n\nFigure 3: Palette toggle",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#other-resources",
    "href": "index.html#other-resources",
    "title": "Regression Models for Epidemiology",
    "section": "Other resources",
    "text": "Other resources\nThese notes represent my still-developing perspective on regression models in epidemiology. Many other statisticians and epidemiologists have published their own perspectives, and I encourage you to explore your many options and find ones that resonate with you. I have attempted to cite my sources throughout these notes.\nHere are some additional resources that I’ve come across; I haven’t had time to read some of them thoroughly yet, but they’re all on my to-do list. I’ll add my thoughts on them over time.\n\nDobson and Barnett (2018) is a classic textbook on GLMs. It was used in UCLA Biostatistics’s MS-level GLMs course (Biostat 200C) when I took it, and it helped me a lot. It is fairly mathematically rigorous and concise, bordering on terse. It covers GLMs in detail, and survival analysis briefly, and it also has helpful chapters on Bayesian methods. I have adapted examples and explanations from it extensively in these notes.\nHosmer, Lemeshow, and Sturdivant (2013) is a classic text on logistic regression. I haven’t read it yet.\nAgresti (2012) is another classic text for GLMs. I haven’t read it yet.\nAgresti (2018) appears to be a more applied version of Agresti (2012). I haven’t read it yet. There are extra exercises and other resources available on the Student Companion Site\nAgresti (2015) has “More than 400 exercises for readers to practice and extend the theory, methods, and data analysis”; might be more theoretical?\nDunn, Smyth, et al. (2018) is a recent textbook on GLMs. It doesn’t cover time-to-event models, and it doesn’t use the modern tidyverse packages (ggplot2, dplyr, etc.), but otherwise it seems great.\nMoore (2016) is a recent textbook on survival analysis. It also doesn’t use the tidyverse, but otherwise seems great.\nKlein and Moeschberger (2003) is a classic text for survival analysis. I read most of it in grad school, and it was very helpful. Examples and explanations from it are borrowed extensively in the second half of these notes (partially filtered through David Rocke’s course notes.)\nKalbfleisch and Prentice (2011) is another classic survival analysis text; I haven’t read it yet.\nKleinbaum and Klein (2010) is a mostly applied-level “self-learning” text for logistic regression; I read it cover-to-cover before grad school, and found it very helpful.\nKleinbaum and Klein (2012) is the corresponding “self-learning” text for survival analysis; I read it cover-to-cover before grad school, and found it very helpful.\nHarrell (2015) is another popular textbook. It uses ggplot2 but not dplyr, and covers logistic regression and survival analysis (no Poisson or NB models?). An abbreviated but continuously updated version with audio clips is available at https://hbiostat.org/rmsc/.\nFox (2015) is another standard text. 3\nMcCullagh and Nelder (1989) is a classic, theoretical textbook on GLMs 4\nDalgaard (2008) covers GLMs and survival analysis at an applied level, using base R\nVittinghoff et al. (2012) covers GLMs, survival analysis, and causal inference, using Stata. The authors are UCSF professors, and it is used for the core Epi PhD courses there. I read this book nearly cover-to-cover before grad school, and it was hugely helpful for me, both for statistical modeling and for causal inference (I think it provided my first exposure to DAGs).\nFaraway (2016) has GLMs but not survival analysis\nSelvin (2001) provides worked-out examples of applications for a wide range of statistical analysis techniques. The Author is a retired UC Berkeley Biostatistics professor; he used it in a graduate-level biostat/epi course.\nJewell (2003) is by another UC Berkeley professor; it mostly covers logistic regression, with one chapter on survival analysis.\nhttps://ucla-biostat-200c-2020spring.github.io/schedule/schedule.html provides course notes for “Biostat 200C - Methods in Biostatistics C” at UCLA, which is at the Biostatistics MS level.\nhttps://online.stat.psu.edu/stat504/book/ provides course notes for “STAT 504 - Analysis of Discrete Data” at Penn State University. It includes logistic regression and Poisson regression, as well as 2-way tables and other related topics, and includes SAS code.\nNahhas (2024) is currently in-development\nClayton and Hills (2013) covers binary regression, count regression, and survival analysis. Haven’t started it yet.\nhttps://thomaselove.github.io/2020-432-book/index.html is another set of lecture notes.\nWoodward (2013) covers GLMs and survival; haven’t read it yet, but it looks comprehensive.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Regression Models for Epidemiology",
    "section": "License",
    "text": "License\nThis book is licensed to you under Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.\nThe code samples in this book are licensed under Creative Commons CC0 1.0 Universal (CC0 1.0), i.e. public domain.\n\n\n\n\n\n\nAgresti, Alan. 2012. Categorical Data Analysis. Vol. 792. John Wiley & Sons. https://www.wiley.com/en-us/Categorical+Data+Analysis%2C+3rd+Edition-p-9780470463635.\n\n\n———. 2015. Foundations of Linear and Generalized Linear Models. John Wiley & Sons. https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034.\n\n\n———. 2018. An Introduction to Categorical Data Analysis. John Wiley & Sons. https://www.wiley.com/en-us/An+Introduction+to+Categorical+Data+Analysis%2C+3rd+Edition-p-9781119405283.\n\n\nClayton, David, and Michael Hills. 2013. Statistical Models in Epidemiology. Oxford University Press. https://global.oup.com/academic/product/statistical-models-in-epidemiology-9780199671182.\n\n\nDalgaard, Peter. 2008. Introductory Statistics with r. New York, NY: Springer New York. https://link.springer.com/book/10.1007/978-0-387-79054-1.\n\n\nDobson, Annette J, and Adrian G Barnett. 2018. An Introduction to Generalized Linear Models. 4th ed. CRC press. https://doi.org/10.1201/9781315182780.\n\n\nDunn, Peter K, Gordon K Smyth, et al. 2018. Generalized Linear Models with Examples in r. Vol. 53. Springer. https://link.springer.com/book/10.1007/978-1-4419-0118-7.\n\n\nFaraway, Julian J. 2016. Extending the Linear Model with r: Generalized Linear, Mixed Effects and Nonparametric Regression Models. 2nd ed. Chapman; Hall/CRC. https://doi.org/10.1201/9781315382722.\n\n\nFox, John. 2015. Applied Regression Analysis and Generalized Linear Models. Sage publications.\n\n\nHarrell, Frank E. 2015. Regression Modeling Strategies: With Applications to Linear Models, Logistic Regression, and Survival Analysis. 2nd ed. Springer. https://doi.org/10.1007/978-3-319-19425-7.\n\n\nHosmer, David W, Stanley Lemeshow, and Rodney X Sturdivant. 2013. Applied Logistic Regression. John Wiley & Sons. https://onlinelibrary.wiley.com/doi/book/10.1002/9781118548387.\n\n\nJewell, Nicholas P. 2003. Statistics for Epidemiology. Oxford, UK: Chapman; Hall/CRC. https://www.routledge.com/Statistics-for-Epidemiology/Jewell/p/book/9781584884330.\n\n\nKalbfleisch, John D, and Ross L Prentice. 2011. The Statistical Analysis of Failure Time Data. John Wiley & Sons.\n\n\nKlein, John P, and Melvin L Moeschberger. 2003. Survival Analysis: Techniques for Censored and Truncated Data. Vol. 1230. Springer. https://link.springer.com/book/10.1007/b97377.\n\n\nKleinbaum, David G, and Mitchel Klein. 2010. Logistic Regression. 3rd ed. Springer. https://link.springer.com/book/10.1007/978-1-4419-1742-3.\n\n\n———. 2012. Survival Analysis a Self-Learning Text. 3rd ed. Springer. https://link.springer.com/book/10.1007/978-1-4419-6646-9.\n\n\nMcCullagh, Peter, and J. A. Nelder. 1989. Generalized Linear Models. 2nd ed. Routledge. https://www.utstat.toronto.edu/~brunner/oldclass/2201s11/readings/glmbook.pdf.\n\n\nMoore, Dirk F. 2016. Applied Survival Analysis Using r. Vol. 473. Springer. https://doi.org/10.1007/978-3-319-31245-3.\n\n\nNahhas, Ramzi W. 2024. Introduction to Regression Methods for Public Health Using r. CRC Press. https://www.bookdown.org/rwnahhas/RMPH/.\n\n\nSelvin, Steve. 2001. Epidemiologic Analysis: A Case-Oriented Approach. Oxford University Press.\n\n\nVittinghoff, Eric, David V Glidden, Stephen C Shiboski, and Charles E McCulloch. 2012. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. 2nd ed. Springer. https://doi.org/10.1007/978-1-4614-1353-0.\n\n\nWoodward, Mark. 2013. Epidemiology: Study Design and Data Analysis. CRC press. https://www.routledge.com/Epidemiology-Study-Design-and-Data-Analysis-Third-Edition/Woodward/p/book/9781439839706.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Regression Models for Epidemiology",
    "section": "",
    "text": "see the source file repository for recent changes: https://github.com/d-morrison/rme↩︎\nMathJax is more or less a dialect of LaTeX↩︎\nI don’t have anything to say about this book, because I haven’t opened it yet, but I’ve heard it’s great!↩︎\nhaven’t opened it either↩︎",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro-to-GLMs.html",
    "href": "intro-to-GLMs.html",
    "title": "\n1  Introduction\n",
    "section": "",
    "text": "Configuring R\nFunctions from these packages will be used throughout this document:\nShow R codelibrary(conflicted) # check for conflicting function definitions\n# library(printr) # inserts help-file output into markdown output\nlibrary(rmarkdown) # Convert R Markdown documents into a variety of formats.\nlibrary(pander) # format tables for markdown\nlibrary(ggplot2) # graphics\nlibrary(ggeasy) # help with graphics\nlibrary(ggfortify) # help with graphics\nlibrary(dplyr) # manipulate data\nlibrary(tibble) # `tibble`s extend `data.frame`s\nlibrary(magrittr) # `%&gt;%` and other additional piping tools\nlibrary(haven) # import Stata files\nlibrary(knitr) # format R output for markdown\nlibrary(tidyr) # Tools to help to create tidy data\nlibrary(plotly) # interactive graphics\nlibrary(dobson) # datasets from Dobson and Barnett 2018\nlibrary(parameters) # format model output tables for markdown\nlibrary(haven) # import Stata files\nlibrary(latex2exp) # use LaTeX in R code (for figures and tables)\nlibrary(fs) # filesystem path manipulations\nlibrary(survival) # survival analysis\nlibrary(survminer) # survival analysis graphics\nlibrary(KMsurv) # datasets from Klein and Moeschberger\nlibrary(parameters) # format model output tables for\nlibrary(webshot2) # convert interactive content to static for pdf\nlibrary(forcats) # functions for categorical variables (\"factors\")\nlibrary(stringr) # functions for dealing with strings\nlibrary(lubridate) # functions for dealing with dates and times\nHere are some R settings I use in this document:\nShow R coderm(list = ls()) # delete any data that's already loaded into R\n\nconflicts_prefer(dplyr::filter)\nggplot2::theme_set(\n  ggplot2::theme_bw() + \n        # ggplot2::labs(col = \"\") +\n    ggplot2::theme(\n      legend.position = \"bottom\",\n      text = ggplot2::element_text(size = 12, family = \"serif\")))\n\nknitr::opts_chunk$set(message = FALSE)\noptions('digits' = 4)\n\npanderOptions(\"big.mark\", \",\")\npander::panderOptions(\"table.emphasize.rownames\", FALSE)\npander::panderOptions(\"table.split.table\", Inf)\nconflicts_prefer(dplyr::filter) # use the `filter()` function from dplyr() by default\nlegend_text_size = 9",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro-to-GLMs.html#introduction-to-epi-204",
    "href": "intro-to-GLMs.html#introduction-to-epi-204",
    "title": "\n1  Introduction\n",
    "section": "\n1.1 Introduction to Epi 204",
    "text": "1.1 Introduction to Epi 204\nWelcome to Epidemiology 204: Quantitative Epidemiology III (Statistical Models).\nIn this course, we will start where Epi 203 left off: with linear regression models.\n\n\n\n\n\n\nNote\n\n\n\nEpi 203/STA 130B/STA 131B is a prerequisite for this course. If you haven’t passed one of these courses, please talk to me ASAP.\n\n\n\n1.1.1 What you should already know\nEpi 202: probability models for different data types\n\nProbability distributions\n\nbinomial\nPoisson\nGaussian\nexponential\n\n\nCharacteristics of probability distributions\n\nMean, median, mode, quantiles\nVariance, standard deviation, overdispersion\n\n\nCharacteristics of samples\n\nindependence, dependence, covariance, correlation\nranks\nidentical vs nonidentical distribution (homogeneity vs heterogeneity)\nLaws of Large Numbers\nCentral Limit Theorem for the mean of an iid sample\n\n\n\nEpi 203: inference for one or several homogenous populations\n\nthe maximum likelihood inference framework:\n\nlikelihood functions\nlog-likelihood functions\nscore functions\nestimating equations\ninformation matrices\npoint estimates\nstandard errors\nconfidence intervals\nhypothesis tests\np-values\n\n\nHypothesis tests for one, two, and &gt;2 groups:\n\nt-tests/ANOVA for Gaussian models\nchi-square tests for binomial and Poisson models\nnonparametric tests:\n\nWilcoxon signed-rank test for matched pairs\nMann–Whitney/Kruskal-Wallis rank sum test for ≥2 independent samples\nFisher’s exact test for contingency tables\nCochran–Mantel–Haenszel-Cox log-rank test\n\n\n\n\nSome linear regression\n\nFor all of the quantities above, and especially for confidence intervals and p-values, you should know how both: - how to compute them - how to interpret them\nStat 108: linear regression models\n\nbuilding models for Gaussian outcomes\n\nmultiple predictors\ninteractions\n\n\nregression diagnostics\nfundamentals of R programming; e.g.:\n\nWickham, Çetinkaya-Rundel, and Grolemund (2023)\nDalgaard (2008)\n\n\n\nRMarkdown or Quarto for formatting homework\n\nLaTeX for writing math in RMarkdown/Quarto\n\n\n\n1.1.2 What we will cover in this course\n\nLinear (Gaussian) regression models (review and more details)\n\nRegression models for non-Gaussian outcomes\n\nbinary\ncount\ntime to event\n\n\nStatistical analysis using R",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "intro-to-GLMs.html#regression-models",
    "href": "intro-to-GLMs.html#regression-models",
    "title": "\n1  Introduction\n",
    "section": "\n1.2 Regression models",
    "text": "1.2 Regression models\nWhy do we need them?\n\ncontinuous predictors\nnot enough data to analyze some subgroups individually\n\n\n1.2.1 Example: Adelie penguins\n\n\n\n\nFigure 1.1: Palmer penguins\n\n\n\n\n\n\n\n\n1.2.2 Linear regression\n\nShow R codeggpenguins2 = \n  ggpenguins +\n  stat_smooth(method = \"lm\",\n              formula = y ~ x,\n              geom = \"smooth\")\n\nggpenguins2 |&gt; print()\n\n\n\nFigure 1.2: Palmer penguins with linear regression fit\n\n\n\n\n\n\n\n\n1.2.3 Curved regression lines\n\nShow R codeggpenguins2 = ggpenguins +\n  stat_smooth(\n    method = \"lm\",\n    formula = y ~ log(x),\n    geom = \"smooth\") +\n  xlab(\"Bill length (mm)\") + \n  ylab(\"Body mass (g)\")\nggpenguins2\n\n\n\nFigure 1.3: Palmer penguins - curved regression lines\n\n\n\n\n\n\n\n\n1.2.4 Multiple regression\n\nShow R codeggpenguins =\n  palmerpenguins::penguins |&gt; \n  ggplot(\n    aes(x = bill_length_mm , \n        y = body_mass_g,\n        color = species\n    )\n  ) +\n  geom_point() +\n  stat_smooth(\n    method = \"lm\",\n    formula = y ~ x,\n    geom = \"smooth\") +\n  xlab(\"Bill length (mm)\") + \n  ylab(\"Body mass (g)\")\nggpenguins |&gt; print()\n\n\n\nFigure 1.4: Palmer penguins - multiple groups\n\n\n\n\n\n\n\n\n1.2.5 Modeling non-Gaussian outcomes\n\nShow R codelibrary(glmx)\ndata(BeetleMortality)\nbeetles = BeetleMortality |&gt;\n  mutate(\n    pct = died/n,\n    survived = n - died\n  )\n\nplot1 = \n  beetles |&gt; \n  ggplot(aes(x = dose, y = pct)) +\n  geom_point(aes(size = n)) +\n  xlab(\"Dose (log mg/L)\") +\n  ylab(\"Mortality rate (%)\") +\n  scale_y_continuous(labels = scales::percent) +\n  # xlab(bquote(log[10]), bquote(CS[2])) +\n  scale_size(range = c(1,2))\n\nprint(plot1)\n\n\n\nFigure 1.5: Mortality rates of adult flour beetles after five hours’ exposure to gaseous carbon disulphide (Bliss 1935)\n\n\n\n\n\n\n\n\n1.2.6 Why don’t we use linear regression?\n\nShow R codebeetles_long = \n  beetles  |&gt; \n  reframe(.by = everything(),\n          outcome = c(\n            rep(1, times = died), \n            rep(0, times = survived))\n  )\n\nlm1 = \n  beetles_long |&gt; \n  lm(\n    formula = outcome ~ dose, \n    data = _)\n\n\nrange1 = range(beetles$dose) + c(-.2, .2)\n\nf.linear = function(x) predict(lm1, newdata = data.frame(dose = x))\n\nplot2 = \n  plot1 + \n  geom_function(fun = f.linear, aes(col = \"Straight line\")) +\n  labs(colour=\"Model\", size = \"\")\nprint(plot2)\n\n\n\nFigure 1.6: Mortality rates of adult flour beetles after five hours’ exposure to gaseous carbon disulphide (Bliss 1935)\n\n\n\n\n\n\n\n\n1.2.7 Zoom out\n\n\n\n\nFigure 1.7: Mortality rates of adult flour beetles after five hours’ exposure to gaseous carbon disulphide (Bliss 1935)\n\n\n\n\n\n\n\n\n1.2.8 log transformation of dose?\n\nShow R code\nlm2 = \n  beetles_long |&gt; \n  lm(formula = outcome ~ log(dose), data = _)\n\nf.linearlog = function(x) predict(lm2, newdata = data.frame(dose = x))\n\nplot3 = plot2 + \n  expand_limits(x = c(1.6, 2)) +\n  geom_function(fun = f.linearlog, aes(col = \"Log-transform dose\"))\n\nprint(plot3  + expand_limits(x = c(1.6, 2)))\n\n\n\nFigure 1.8: Mortality rates of adult flour beetles after five hours’ exposure to gaseous carbon disulphide (Bliss 1935)\n\n\n\n\n\n\n\n\n1.2.9 Logistic regression\n\nShow R codeglm1 = beetles |&gt; \n  glm(formula = cbind(died, survived) ~ dose, family = \"binomial\")\n\nf = function(x) predict(glm1, newdata = data.frame(dose = x), type = \"response\")\n\nplot4 = plot3 + geom_function(fun = f, aes(col = \"Logistic regression\"))\nprint(plot4)\n\n\n\nFigure 1.9: Mortality rates of adult flour beetles after five hours’ exposure to gaseous carbon disulphide (Bliss 1935)\n\n\n\n\n\n\n\n\n1.2.10 Three parts to regression models\n\nWhat distribution does the outcome have for a specific sub-population defined by covariates? (outcome model)\nHow does the combination of covariates relate to the mean? (link function)\nHow do the covariates combine? (linear predictor/linear component) \\[\\eta \\stackrel{\\text{def}}{=}\\tilde{x}^{\\top} \\tilde{\\beta}= \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ...\\]\n\n\n\n\n\n\n\nDalgaard, Peter. 2008. Introductory Statistics with r. New York, NY: Springer New York. https://link.springer.com/book/10.1007/978-0-387-79054-1.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\". https://r4ds.hadley.nz/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "glms.html",
    "href": "glms.html",
    "title": "Generalized Linear Models",
    "section": "",
    "text": "This section is primarily adapted starting from the textbook “An Introduction to Generalized Linear Models” (4th edition, 2018) by Annette J. Dobson and Adrian G. Barnett:\nhttps://doi.org/10.1201/9781315182780\nThe type of predictive model one uses depends on several issues; one is the type of response.\n\nMeasured values such as quantity of a protein, age, weight usually can be handled in an ordinary linear regression model, possibly after a log transformation.\nPatient survival, which may be censored, calls for a different method (survival analysis, Cox regression).\nIf the response is binary, then can we use logistic regression models\nIf the response is a count, we can use Poisson regression\nIf the count has a higher variance than is consistent with the Poisson, we can use a negative binomial or over-dispersed Poisson\nOther forms of response can generate other types of generalized linear models\n\nWe need a linear predictor of the same form as in linear regression \\(\\beta x\\). In theory, such a linear predictor can generate any type of number as a prediction, positive, negative, or zero\nWe choose a suitable distribution for the type of data we are predicting (normal for any number, gamma for positive numbers, binomial for binary responses, Poisson for counts)\nWe create a link function which maps the mean of the distribution onto the set of all possible linear prediction results, which is the whole real line (\\(-\\infty, \\infty\\)). The inverse of the link function takes the linear predictor to the actual prediction.\n\nOrdinary linear regression has identity link (no transformation by the link function) and uses the normal distribution\nIf one is predicting an inherently positive quantity, one may want to use the log link since ex is always positive.\nAn alternative to using a generalized linear model with a log link, is to transform the data using the log. This is a device that works well with measurement data and may be usable in other cases, but it cannot be used for 0/1 data or for count data that may be 0.\n\n\nR glm() Families\n\n\n\n\n\n\nFamily\nLinks\n\n\n\n\ngaussian\nidentity, log, inverse\n\n\nbinomial\nlogit, probit, cauchit, log, cloglog\n\n\ngamma\ninverse, identity, log\n\n\ninverse.gaussian\n1/mu^2, inverse, identity, log\n\n\nPoisson\nlog, identity, sqrt\n\n\nquasi\nidentity, logit, probit, cloglog, inverse, log, 1/mu^2 and sqrt\n\n\nquasibinomial\nlogit, probit, identity, cloglog, inverse, log, 1/mu^2 and sqrt\n\n\nquasipoisson\nlog, identity, logit, probit, cloglog, inverse, 1/mu^2 and sqrt\n\n\n\n\nR glm() Link Functions; \\(\\eta = X\\beta = g(\\mu)\\)\n\n\n\n\n\n\n\n\n\nName\nDomain\nRange\nLink Function\nInverse Link Function\n\n\n\n\nidentity\n\\((-\\infty, \\infty)\\)\n\\((-\\infty, \\infty)\\)\n\\(\\eta = \\mu\\).\n\\(\\mu = \\eta\\)\n\n\nlog\n\\((0,\\infty)\\)\n\\((-\\infty, \\infty)\\)\n\\(\\eta = \\text{log}\\left\\{\\mu\\right\\}\\)\n\\(\\mu = \\text{exp}\\left\\{\\eta\\right\\}\\)\n\n\ninverse\n\\((0, \\infty)\\)\n\\((0,\\infty)\\)\n\\(\\eta = 1/\\mu\\)\n\\(\\mu = 1/\\eta\\)\n\n\nlogit\n\\((0,1)\\)\n\\((-\\infty, \\infty)\\)\n\\(\\eta = \\text{log}\\left\\{\\mu/(1-\\mu)\\right\\}\\)\n\\(\\mu = \\text{exp}\\left\\{\\eta\\right\\}/(1+\\text{exp}\\left\\{\\eta\\right\\})\\)\n\n\nprobit\n\\((0,1)\\)\n\\((-\\infty, \\infty)\\)\n\\(\\eta = \\Phi^{-1}(\\mu)\\)\n\\(\\mu = \\Phi(\\eta)\\)\n\n\ncloglog\n\\((0,1)\\)\n\\((-\\infty, \\infty)\\)\n\\(\\eta = \\text{log}\\left\\{-\\text{log}\\left\\{1-\\mu\\right\\}\\right\\}\\)\n\\(\\mu = {1-\\text{exp}\\left\\{-\\text{exp}\\left\\{\\eta\\right\\}\\right\\}}\\)\n\n\n1/mu^2\n\\((0,\\infty)\\)\n\\((0, \\infty)\\)\n\\(\\eta = 1/\\mu^2\\)\n\\(\\mu = 1/\\sqrt{\\eta}\\)\n\n\nsqrt\n\\((0,\\infty)\\)\n\\((0,\\infty)\\)\n\\(\\eta = \\sqrt{\\mu}\\)\n\\(\\mu = \\eta^2\\)",
    "crumbs": [
      "Generalized Linear Models"
    ]
  },
  {
    "objectID": "Linear-models-overview.html",
    "href": "Linear-models-overview.html",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "",
    "text": "Configuring R\nFunctions from these packages will be used throughout this document:\nShow R codelibrary(conflicted) # check for conflicting function definitions\n# library(printr) # inserts help-file output into markdown output\nlibrary(rmarkdown) # Convert R Markdown documents into a variety of formats.\nlibrary(pander) # format tables for markdown\nlibrary(ggplot2) # graphics\nlibrary(ggeasy) # help with graphics\nlibrary(ggfortify) # help with graphics\nlibrary(dplyr) # manipulate data\nlibrary(tibble) # `tibble`s extend `data.frame`s\nlibrary(magrittr) # `%&gt;%` and other additional piping tools\nlibrary(haven) # import Stata files\nlibrary(knitr) # format R output for markdown\nlibrary(tidyr) # Tools to help to create tidy data\nlibrary(plotly) # interactive graphics\nlibrary(dobson) # datasets from Dobson and Barnett 2018\nlibrary(parameters) # format model output tables for markdown\nlibrary(haven) # import Stata files\nlibrary(latex2exp) # use LaTeX in R code (for figures and tables)\nlibrary(fs) # filesystem path manipulations\nlibrary(survival) # survival analysis\nlibrary(survminer) # survival analysis graphics\nlibrary(KMsurv) # datasets from Klein and Moeschberger\nlibrary(parameters) # format model output tables for\nlibrary(webshot2) # convert interactive content to static for pdf\nlibrary(forcats) # functions for categorical variables (\"factors\")\nlibrary(stringr) # functions for dealing with strings\nlibrary(lubridate) # functions for dealing with dates and times\nHere are some R settings I use in this document:\nShow R coderm(list = ls()) # delete any data that's already loaded into R\n\nconflicts_prefer(dplyr::filter)\nggplot2::theme_set(\n  ggplot2::theme_bw() + \n        # ggplot2::labs(col = \"\") +\n    ggplot2::theme(\n      legend.position = \"bottom\",\n      text = ggplot2::element_text(size = 12, family = \"serif\")))\n\nknitr::opts_chunk$set(message = FALSE)\noptions('digits' = 4)\n\npanderOptions(\"big.mark\", \",\")\npander::panderOptions(\"table.emphasize.rownames\", FALSE)\npander::panderOptions(\"table.split.table\", Inf)\nconflicts_prefer(dplyr::filter) # use the `filter()` function from dplyr() by default\nlegend_text_size = 9",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear (Gaussian) Models</span>"
    ]
  },
  {
    "objectID": "Linear-models-overview.html#overview",
    "href": "Linear-models-overview.html#overview",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "\n2.1 Overview",
    "text": "2.1 Overview\n\n2.1.1 Why this course includes linear regression\n\n\nThis course is about generalized linear models (for non-Gaussian outcomes)\n\n\n\n\nUC Davis STA 108 (“Applied Statistical Methods: Regression Analysis”) is a prerequisite for this course, so everyone here should have some understanding of linear regression already.\n\n\n\n\nWe will review linear regression to:\n\nmake sure everyone is caught up\nto provide an epidemiological perspective on model intepretation.\n\n\n\n\n\n2.1.2 Chapter overview\n\nSection 2.2: how to interpret linear regression models\nSection 2.3: how to estimate linear regression models\nSection 2.4: how to quantify uncertainty about our estimates\nSection 2.8: how to tell if your model is insufficiently complex",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear (Gaussian) Models</span>"
    ]
  },
  {
    "objectID": "Linear-models-overview.html#sec-understand-LMs",
    "href": "Linear-models-overview.html#sec-understand-LMs",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "\n2.2 Understanding Gaussian Linear Regression Models",
    "text": "2.2 Understanding Gaussian Linear Regression Models\n\n2.2.1 Motivating example: birthweights and gestational age\nSuppose we want to learn about the distributions of birthweights (outcome \\(Y\\)) for (human) babies born at different gestational ages (covariate \\(A\\)) and with different chromosomal sexes (covariate \\(S\\)) (Dobson and Barnett (2018) Example 2.2.2).\n\n\nData as table\nReshape data for graphing\nData as graph\n\n\n\n\nShow R codelibrary(dobson)\ndata(\"birthweight\", package = \"dobson\")\nbirthweight |&gt; knitr::kable()\n\n\nTable 2.1: birthweight data (Dobson and Barnett (2018) Example 2.2.2)\n\n\n\n\n\n\n\n\n\n\nboys gestational age\nboys weight\ngirls gestational age\ngirls weight\n\n\n\n40\n2968\n40\n3317\n\n\n38\n2795\n36\n2729\n\n\n40\n3163\n40\n2935\n\n\n35\n2925\n38\n2754\n\n\n36\n2625\n42\n3210\n\n\n37\n2847\n39\n2817\n\n\n41\n3292\n40\n3126\n\n\n40\n3473\n37\n2539\n\n\n37\n2628\n36\n2412\n\n\n38\n3176\n38\n2991\n\n\n40\n3421\n39\n2875\n\n\n38\n2975\n40\n3231\n\n\n\n\n\n\n\n\n\n\n\nShow R codebw = \n  birthweight |&gt; \n  pivot_longer(\n    cols = everything(),\n    names_to = c(\"sex\", \".value\"),\n    names_sep = \"s \"\n  ) |&gt; \n  rename(age = `gestational age`) |&gt; \n  mutate(\n    sex = sex |&gt; \n      case_match(\n        \"boy\" ~ \"male\",\n        \"girl\" ~ \"female\") |&gt; \n      factor(levels = c(\"female\", \"male\")))\n\nbw\n\n\nTable 2.2: birthweight data reshaped\n\n\n\n  \n\n\n\n\n\n\n\n\n\nShow R codeplot1 = bw |&gt; \n  ggplot(aes(\n    x = age, \n    y = weight,\n    linetype = sex,\n    shape = sex,\n    col = sex))  +\n  theme_bw() +\n  xlab(\"Gestational age (weeks)\") +\n  ylab(\"Birthweight (grams)\") +\n  theme(legend.position = \"bottom\") +\n  # expand_limits(y = 0, x = 0) +\n  geom_point(alpha = .7)\nprint(plot1 + facet_wrap(~ sex))\n\n\n\nFigure 2.1: birthweight data (Dobson and Barnett (2018) Example 2.2.2)\n\n\n\n\n\n\n\n\n\n\n\nData notation\nLet’s define some notation to represent this data.\n\n\\(Y\\): birthweight (measured in grams)\n\\(S\\): chromosomal sex: “male” (XY) or “female” (XX)\n\\(M\\): indicator variable for \\(S\\) = “male”1\n\\(M = 0\\) if female (XX)\n\\(M = 1\\) if male (XY)\n\\(F\\): indicator variable for \\(S\\) = “female”2\n\\(F = 1\\) if female (XX)\n\\(F = 0\\) if male (XY)\n\\(A\\): estimated gestational age at birth (measured in weeks).\n\n\n\n\n\n\n\nNote\n\n\n\nFemale is the reference level for the categorical variable \\(S\\) (chromosomal sex) and corresponding indicator variable \\(M\\) . The choice of a reference level is arbitrary and does not limit what we can do with the resulting model; it only makes it more computationally convenient to make inferences about comparisons involving that reference group.\n\n\n\n2.2.2 Parallel lines regression\nWe don’t have enough data to model the distribution of birth weight separately for each combination of gestational age and sex, so let’s instead consider a (relatively) simple model for how that distribution varies with gestational age and sex:\n\\[p(Y=y|A=a,S=s) \\ \\sim_{\\text{iid}}\\ N(\\mu(a,s), \\sigma^2)\\]\n\\[\n\\begin{aligned}\n\\mu(a,s)\n&\\stackrel{\\text{def}}{=}\\mathbb{E}\\left[Y|A=a, S=s\\right] \\\\\n&= \\beta_0 + \\beta_A a+ \\beta_M m\n\\end{aligned}\n\\tag{2.1}\\]\n\nTable 2.3 shows the parameter estimates from R. Figure 2.2 shows the estimated model, superimposed on the data.\n\n\nShow R codebw_lm1 = lm(\n  formula = weight ~ sex + age, \n  data = bw)\n\nbw_lm1 |&gt; \n  parameters() |&gt;\n  print_md(\n    include_reference = TRUE,\n    # show_sigma = TRUE,\n    select = \"{estimate}\")\n\n\nTable 2.3: Estimate of Model 2.1 for birthweight data\n\n\n\n\nParameter\nEstimate\n\n\n\n(Intercept)\n-1773.32\n\n\nsex (female)\n0.00\n\n\nsex (male)\n163.04\n\n\nage\n120.89\n\n\n\n\n\n\n\n\n\n\n\n\nShow R codebw = \n  bw |&gt; \n  mutate(`E[Y|X=x]` = fitted(bw_lm1)) |&gt; \n  arrange(sex, age)\n\nplot2 = \n  plot1 %+% bw +\n  geom_line(aes(y = `E[Y|X=x]`))\n\nprint(plot2)\n\n\n\nFigure 2.2: Parallel-slopes model of birthweight\n\n\n\n\n\n\n\n\n\nModel assumptions and predictions\n\nTo learn what this model is assuming, let’s plug in a few values.\n\n\nExercise 2.1 According to this model, what’s the mean birthweight for a female born at 36 weeks?\n\n\nTable 2.4: Estimated coefficients for model 2.1\n\nShow R codecoef(bw_lm1)\n#&gt; (Intercept)     sexmale         age \n#&gt;     -1773.3       163.0       120.9\n\n\n\n\n\n\n\nSolution.  \n\nShow R codepred_female = coef(bw_lm1)[\"(Intercept)\"] + coef(bw_lm1)[\"age\"]*36\ncoef(bw_lm1)\n#&gt; (Intercept)     sexmale         age \n#&gt;     -1773.3       163.0       120.9\n# print(pred_female)\n### built-in prediction: \n# predict(bw_lm1, newdata = tibble(sex = \"female\", age = 36))\n\n\n\\[\n\\begin{aligned}\nE[Y|A = 0, A = 36]\n&= \\beta_0 + \\beta_M \\cdot 0+ \\beta_A \\cdot 36 \\\\\n&=  2578.8739\n\\end{aligned}\n\\]\n\n\n\nExercise 2.2 What’s the mean birthweight for a male born at 36 weeks?\n\nShow R codecoef(bw_lm1)\n#&gt; (Intercept)     sexmale         age \n#&gt;     -1773.3       163.0       120.9\n\n\n\n\n\nSolution.  \n\nShow R codepred_male = \n  coef(bw_lm1)[\"(Intercept)\"] + \n  coef(bw_lm1)[\"sexmale\"] + \n  coef(bw_lm1)[\"age\"]*36\ncoef(bw_lm1)\n#&gt; (Intercept)     sexmale         age \n#&gt;     -1773.3       163.0       120.9\n\n\n\\[\n\\begin{aligned}\nE[Y|M = 1, A = 36]\n&= \\beta_0 + \\beta_M \\cdot 1+ \\beta_A \\cdot 36 \\\\\n&=  2741.9132\n\\end{aligned}\n\\]\n\n\n\nExercise 2.3 What’s the difference in mean birthweights between males born at 36 weeks and females born at 36 weeks?\n\n\nShow R codecoef(bw_lm1)\n#&gt; (Intercept)     sexmale         age \n#&gt;     -1773.3       163.0       120.9\n\n\n\n\nSolution. \\[\n\\begin{aligned}\n& E[Y|M = 1, A = 36] - E[Y|M = 0, A = 36]\\\\\n&=\n2741.9132 - 2578.8739\\\\\n&=\n163.0393\n\\end{aligned}\n\\]\nShortcut:\n\\[\n\\begin{aligned}\n& E[Y|M = 1, A = 36] - E[Y|M = 0, A = 36]\\\\\n&= (\\beta_0 + \\beta_M \\cdot 1+ \\beta_A \\cdot 36) -\n(\\beta_0 + \\beta_M \\cdot 0+ \\beta_A \\cdot 36) \\\\\n&= \\beta_M \\\\\n&=  163.0393\n\\end{aligned}\n\\]\n\n\nNote that age doesn’t show up in this difference: in other words, according to this model, the difference between females and males with the same gestational age is the same for every age.\nThat’s an assumption of the model; it’s built-in to the parametric structure, even before we plug in the estimated values of those parameters.\nThat’s why the lines are parallel.\n\n\n2.2.3 Interactions\n\nWhat if we don’t like that parallel lines assumption?\nThen we need to allow an “interaction” between age \\(A\\) and sex \\(S\\):\n\n\\[\nE[Y|A=a, S=s] = \\beta_0 + \\beta_A a+ \\beta_M m + \\beta_{AM} (a \\cdot m)\n\\tag{2.2}\\]\n\nNow, the slope of mean birthweight \\(E[Y|A,S]\\) with respect to gestational age \\(A\\) depends on the value of sex \\(S\\).\n\n\n\nShow R codebw_lm2 = lm(weight ~ sex + age + sex:age, data = bw)\nbw_lm2 |&gt; \n  parameters() |&gt;\n  print_md(\n    include_reference = TRUE,\n    # show_sigma = TRUE,\n    select = \"{estimate}\")\n\n\nTable 2.5: Birthweight model with interaction term\n\n\n\n\nParameter\nEstimate\n\n\n\n(Intercept)\n-2141.67\n\n\nsex (female)\n0.00\n\n\nsex (male)\n872.99\n\n\nage\n130.40\n\n\nsex (male) × age\n-18.42\n\n\n\n\n\n\n\n\n\n\n\n\nShow R codebw = \n  bw |&gt; \n  mutate(\n    predlm2 = predict(bw_lm2)\n  ) |&gt; \n  arrange(sex, age)\n\nplot1_interact = \n  plot1 %+% bw +\n  geom_line(aes(y = predlm2))\n\nprint(plot1_interact)\n\n\n\nFigure 2.3: Birthweight model with interaction term\n\n\n\n\n\n\n\n\n\nNow we can see that the lines aren’t parallel.\n\n\nHere’s another way we could rewrite this model (by collecting terms involving \\(S\\)):\n\\[\nE[Y|A, M] = \\beta_0 + \\beta_M M+ (\\beta_A + \\beta_{AM} M) A\n\\]\n\n\n\n\n\n\nNote\n\n\n\nIf you want to understand a coefficient in a model with interactions, collect terms for the corresponding variable, and you will see what other variables are interacting with the variable you are interested in.\n\n\n\nIn this case, the coefficient \\(S\\) is interacting with \\(A\\). So the slope of \\(Y\\) with respect to \\(A\\) depends on the value of \\(M\\).\nAccording to this model, there is no such thing as “the slope of birthweight with respect to age”. There are two slopes, one for each sex.3 We can only talk about “the slope of birthweight with respect to age among males” and “the slope of birthweight with respect to age among females”.\nThen: that coefficient is the difference in means per unit change in its corresponding coefficient, when the other collected variables are set to 0.\n\n\n\nTo learn what this model is assuming, let’s plug in a few values.\n\n\nExercise 2.4 According to this model, what’s the mean birthweight for a female born at 36 weeks?\n\n\n\nSolution.  \n\nShow R codepred_female = coef(bw_lm2)[\"(Intercept)\"] + coef(bw_lm2)[\"age\"]*36\n\n\n\\[\nE[Y|A = 0, X_2 = 36] =\n\\beta_0 + \\beta_M \\cdot 0+ \\beta_A \\cdot 36 + \\beta_{AM} \\cdot (0 * 36)\n=  2552.7333\n\\]\n\n\n\nExercise 2.5 What’s the mean birthweight for a male born at 36 weeks?\n\n\n\nSolution.  \n\nShow R codepred_male = \n  coef(bw_lm2)[\"(Intercept)\"] + \n  coef(bw_lm2)[\"sexmale\"] + \n  coef(bw_lm2)[\"age\"]*36 + \n  coef(bw_lm2)[\"sexmale:age\"] * 36\n\n\n\\[\n\\begin{aligned}\nE[Y|A = 0, X_2 = 36]\n&= \\beta_0 + \\beta_M \\cdot 1+ \\beta_A \\cdot 36 + \\beta_{AM} \\cdot 1 \\cdot 36\\\\\n&= 2762.7069\n\\end{aligned}\n\\]\n\n\n\nExercise 2.6 What’s the difference in mean birthweights between males born at 36 weeks and females born at 36 weeks?\n\n\n\nSolution. \\[\n\\begin{aligned}\n& E[Y|M = 1, A = 36] - E[Y|M = 0, A = 36]\\\\\n&= (\\beta_0 + \\beta_M \\cdot 1+ \\beta_A \\cdot 36 + \\beta_{AM} \\cdot 1 \\cdot 36)\\\\\n&\\ \\ \\ \\ \\  -(\\beta_0 + \\beta_M \\cdot 0+ \\beta_A \\cdot 36 + \\beta_{AM} \\cdot 0 \\cdot 36) \\\\\n&= \\beta_{S} + \\beta_{AM}\\cdot 36\\\\\n&=  209.9736\n\\end{aligned}\n\\]\n\n\nNote that age now does show up in the difference: in other words, according to this model, the difference in mean birthweights between females and males with the same gestational age can vary by gestational age.\nThat’s how the lines in the graph ended up non-parallel.\n\n\n2.2.4 Stratified regression\n\nWe could re-write the interaction model as a stratified model, with a slope and intercept for each sex:\n\n\\[\n\\mathbb{E}\\left[Y|A=a, S=s\\right] =\n\\beta_M m + \\beta_{AM} (a \\cdot m) +\n\\beta_F f + \\beta_{AF} (a \\cdot f)\n\\tag{2.3}\\]\nCompare this stratified model with our interaction model, Equation 2.2:\n\\[\n\\mathbb{E}\\left[Y|A=a, S=s\\right] =\n\\beta_0 + \\beta_A a + \\beta_M m + \\beta_{AM} (a \\cdot m)\n\\]\n\nIn the stratified model, the intercept term \\(\\beta_0\\) has been relabeled as \\(\\beta_F\\).\n\n\nShow R codebw_lm2 = lm(weight ~ sex + age + sex:age, data = bw)\nbw_lm2 |&gt; \n  parameters() |&gt;\n  print_md(\n    include_reference = TRUE,\n    # show_sigma = TRUE,\n    select = \"{estimate}\")\n\n\nTable 2.6: Birthweight model with interaction term\n\n\n\n\nParameter\nEstimate\n\n\n\n(Intercept)\n-2141.67\n\n\nsex (female)\n0.00\n\n\nsex (male)\n872.99\n\n\nage\n130.40\n\n\nsex (male) × age\n-18.42\n\n\n\n\n\n\n\n\n\n\n\n\nShow R codebw_lm_strat = \n  bw |&gt; \n  lm(\n    formula = weight ~ sex + sex:age - 1, \n    data = _)\n\nbw_lm_strat |&gt; \n  parameters() |&gt;\n  print_md(\n    # show_sigma = TRUE,\n    select = \"{estimate}\")\n\n\nTable 2.7: Birthweight model - stratified betas\n\n\n\n\nParameter\nEstimate\n\n\n\nsex (female)\n-2141.67\n\n\nsex (male)\n-1268.67\n\n\nsex (female) × age\n130.40\n\n\nsex (male) × age\n111.98\n\n\n\n\n\n\n\n\n\n\n2.2.5 Curved-line regression\n\nIf we transform some of our covariates (\\(X\\)s) and plot the resulting model on the original covariate scale, we end up with curved regression lines:\n\n\nShow R codebw_lm3 = lm(weight ~ sex:log(age) - 1, data = bw)\nlibrary(palmerpenguins)\n\nggpenguins &lt;- \n  palmerpenguins::penguins |&gt; \n  dplyr::filter(species == \"Adelie\") |&gt; \n  ggplot(\n    aes(x = bill_length_mm , y = body_mass_g)) +\n  geom_point() + \n  xlab(\"Bill length (mm)\") + \n  ylab(\"Body mass (g)\")\n\nggpenguins2 = ggpenguins +\n  stat_smooth(\n    method = \"lm\",\n    formula = y ~ log(x),\n    geom = \"smooth\") +\n  xlab(\"Bill length (mm)\") + \n  ylab(\"Body mass (g)\")\n\n\nggpenguins2 |&gt; print()\n\n\n\nFigure 2.4: palmerpenguins model with bill_length entering on log scale",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear (Gaussian) Models</span>"
    ]
  },
  {
    "objectID": "Linear-models-overview.html#sec-est-LMs",
    "href": "Linear-models-overview.html#sec-est-LMs",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "\n2.3 Estimating Linear Models via Maximum Likelihood",
    "text": "2.3 Estimating Linear Models via Maximum Likelihood\n\n2.3.1 Likelihood, log-likelihood, and score functions for linear regression\n\nIn EPI 203 and Appendix E, we learned how to fit outcome-only models of the form \\(p(X=x|\\theta)\\) to iid data \\(\\tilde{x}= (x_1,…,x_n)\\) using maximum likelihood estimation.\nNow, we apply the same procedure to linear regression models:\n\n\\[\n\\mathcal L(\\tilde{y}|\\mathbf{x},\\beta, \\sigma^2) =\n\\prod_{i=1}^n (2\\pi\\sigma^2)^{-1/2}\n\\text{exp}\\left\\{-\\frac{1}{2\\sigma^2}(y_i - \\tilde{x_i}'\\beta)^2\\right\\}\n\\tag{2.4}\\]\n\\[\n\\ell(\\tilde{y}|\\mathbf{x},\\beta, \\sigma^2)\n= -\\frac{n}{2}\\text{log}\\left\\{\\sigma^2\\right\\} -\n\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - \\tilde{x_i}' \\beta)^2\n\\tag{2.5}\\]\n\\[\n\\ell'_{\\beta}(\\tilde{y}|\\mathbf{x},\\beta, \\sigma^2)\n= -\n\\frac{1}{2\\sigma^2}\\frac{\\partial}{\\partial \\beta}\n\\left(\\sum_{i=1}^n (y_i - \\tilde{x_i}^{\\top} \\beta)^2\\right)\n\\tag{2.6}\\]\n\n\nLet’s switch to matrix-vector notation:\n\n\\[\n\\sum_{i=1}^n (y_i - \\tilde{x}_i^{\\top} \\tilde{\\beta})^2\n= (\\tilde{y}- \\mathbf{X}\\tilde{\\beta})'(\\tilde{y}- \\mathbf{X}\\tilde{\\beta})\n\\]\n\nSo\n\\[\n\\begin{aligned}\n(\\tilde{y}- \\mathbf{X}\\tilde{\\beta})'(\\tilde{y}- \\mathbf{X}\\tilde{\\beta})\n&= (\\tilde{y}' - \\tilde{\\beta}'X')(\\tilde{y}- \\mathbf{X}\\tilde{\\beta})\n\\\\ &= y'y - \\tilde{\\beta}'X'y - y'\\mathbf{X}\\tilde{\\beta}+\\tilde{\\beta}'\\mathbf{X}'\\mathbf{X}\\beta\n\\\\ &= y'y - 2y'\\mathbf{X}\\beta +\\beta'\\mathbf{X}'\\mathbf{X}\\beta\n\\end{aligned}\n\\]\n\n2.3.2 Deriving the linear regression score function\n\nWe will use some results from vector calculus:\n\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\beta}\\left(\\sum_{i=1}^n (y_i - x_i' \\beta)^2\\right)\n   &= \\frac{\\partial}{\\partial \\beta}(\\tilde{y}- X\\beta)'(\\tilde{y}- X\\beta)\n\\\\ &= \\frac{\\partial}{\\partial \\beta} (y'y - 2y'X\\beta +\\beta'X'X\\beta)\n\\\\ &= (- 2X'y +2X'X\\beta)\n\\\\ &= - 2X'(y - X\\beta)\n\\\\ &= - 2X'(y - \\mathbb{E}[y])\n\\\\ &= - 2X' \\varepsilon(y)\n\\end{aligned}\n\\tag{2.7}\\]\n\nSo if \\(\\ell(\\beta,\\sigma^2) =0\\), then\n\\[\n\\begin{aligned}\n0 &= (- 2X'y +2X'X\\beta)\\\\\n2X'y &= 2X'X\\beta\\\\\nX'y &= X'X\\beta\\\\\n(X'X)^{-1}X'y &= \\beta\n\\end{aligned}\n\\]\n\nThe second derivative matrix \\(\\ell_{\\beta, \\beta'} ''(\\beta, \\sigma^2;\\mathbf X,\\tilde{y})\\) is negative definite at \\(\\beta = (X'X)^{-1}X'y\\), so \\(\\hat \\beta_{ML} = (X'X)^{-1}X'y\\) is the MLE for \\(\\beta\\).\n\nSimilarly (not shown):\n\\[\n\\hat\\sigma^2_{ML} = \\frac{1}{n} (Y-X\\hat\\beta)'(Y-X\\hat\\beta)\n\\]\nAnd\n\\[\n\\begin{aligned}\n\\mathcal I_{\\beta} &= E[-\\ell_{\\beta, \\beta'} ''(Y|X,\\beta, \\sigma^2)]\\\\\n&= \\frac{1}{\\sigma^2}X'X\n\\end{aligned}\n\\]\n\nSo:\n\\[\nVar(\\hat \\beta) \\approx (\\mathcal I_{\\beta})^{-1} = \\sigma^2 (X'X)^{-1}\n\\]\nand\n\\[\n\\hat\\beta \\dot \\sim N(\\beta, \\mathcal I_{\\beta}^{-1})\n\\]\n\nThese are all results you have hopefully seen before.\n\n\nIn the Gaussian linear regression case, we also have exact results:\n\\[\n\\frac{\\hat\\beta_j}{\\hat{\\text{se}}\\left(\\hat\\beta_j\\right)} \\ \\sim \\ t_{n-p}\n\\]\n\nIn our model 2 above, \\(\\hat{\\mathcal{I}}(\\beta)\\) is:\n\nShow R codebw_lm2 |&gt; vcov()\n#&gt;             (Intercept)  sexmale      age sexmale:age\n#&gt; (Intercept)     1353968 -1353968 -34871.0     34871.0\n#&gt; sexmale        -1353968  2596387  34871.0    -67211.0\n#&gt; age              -34871    34871    899.9      -899.9\n#&gt; sexmale:age       34871   -67211   -899.9      1743.5\n\n\nIf we take the square roots of the diagonals, we get the standard errors listed in the model output:\n\nShow R code\nbw_lm2 |&gt; vcov() |&gt; diag() |&gt; sqrt()\n#&gt; (Intercept)     sexmale         age sexmale:age \n#&gt;     1163.60     1611.33       30.00       41.76\n\n\n\nShow R codebw_lm2 |&gt; parameters() |&gt; print_md()\n\n\nTable 2.8: Estimated model for birthweight data with interaction term\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(20)\np\n\n\n\n(Intercept)\n-2141.67\n1163.60\n(-4568.90, 285.56)\n-1.84\n0.081\n\n\nsex (male)\n872.99\n1611.33\n(-2488.18, 4234.17)\n0.54\n0.594\n\n\nage\n130.40\n30.00\n(67.82, 192.98)\n4.35\n&lt; .001\n\n\nsex (male) × age\n-18.42\n41.76\n(-105.52, 68.68)\n-0.44\n0.664\n\n\n\n\n\n\n\n\nSo we can do confidence intervals, hypothesis tests, and p-values exactly as in the one-variable case we looked at previously.\n\n2.3.3 Residual Standard Deviation\n\n\\(\\hat \\sigma\\) represents an estimate of the Residual Standard Deviation parameter, \\(\\sigma\\). We can extract \\(\\hat \\sigma\\) from the fitted model, using the sigma() function:\n\n\nShow R codesigma(bw_lm2)\n#&gt; [1] 180.6\n\n\n\n\n\\(\\sigma\\) is NOT “Residual standard error”\n\nIn the summary.lm() output, this estimate is labeled as \"Residual standard error\":\n\n\nShow R codesummary(bw_lm2)\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = weight ~ sex + age + sex:age, data = bw)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt; -246.7 -138.1  -39.1  176.6  274.3 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  -2141.7     1163.6   -1.84  0.08057 .  \n#&gt; sexmale        873.0     1611.3    0.54  0.59395    \n#&gt; age            130.4       30.0    4.35  0.00031 ***\n#&gt; sexmale:age    -18.4       41.8   -0.44  0.66389    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 181 on 20 degrees of freedom\n#&gt; Multiple R-squared:  0.643,  Adjusted R-squared:  0.59 \n#&gt; F-statistic:   12 on 3 and 20 DF,  p-value: 0.000101\n\n\n\n\nHowever, this is a misnomer:\n\n\nShow R codelibrary(printr) # captures ? documentation\n?stats::sigma\n\n\n\n\nsigma\nR Documentation\n\nExtract Residual Standard Deviation 'Sigma'\n\nDescription\n\nExtract the estimated standard deviation of the errors, the\n“residual standard deviation” (misnamed also\n“residual standard error”, e.g., in\nsummary.lm()'s output, from a fitted model).\n\nMany classical statistical models have a scale parameter,\ntypically the standard deviation of a zero-mean normal (or Gaussian)\nrandom variable which is denoted as \\sigma.\nsigma(.) extracts the estimated parameter from a fitted\nmodel, i.e., \\hat\\sigma.\n\n\n\nNote\n\nThe misnomer “Residual standard error” has been part of\ntoo many R (and S) outputs to be easily changed there.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear (Gaussian) Models</span>"
    ]
  },
  {
    "objectID": "Linear-models-overview.html#sec-infer-LMs",
    "href": "Linear-models-overview.html#sec-infer-LMs",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "\n2.4 Inference about Gaussian Linear Regression Models",
    "text": "2.4 Inference about Gaussian Linear Regression Models\n\n2.4.1 Motivating example: birthweight data\nResearch question: is there really an interaction between sex and age?\n\\(H_0: \\beta_{AM} = 0\\)\n\\(H_A: \\beta_{AM} \\neq 0\\)\n\\(P(|\\hat\\beta_{AM}| &gt; |-18.4172| \\mid H_0)\\) = ?\n\n2.4.2 Wald tests and CIs\nR can give you Wald tests for single coefficients and corresponding CIs:\n\nShow R code\nbw_lm2 |&gt; \n  parameters() |&gt;\n  print_md(\n    include_reference = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(20)\np\n\n\n\n(Intercept)\n-2141.67\n1163.60\n(-4568.90, 285.56)\n-1.84\n0.081\n\n\nsex (female)\n0.00\n\n\n\n\n\n\nsex (male)\n872.99\n1611.33\n(-2488.18, 4234.17)\n0.54\n0.594\n\n\nage\n130.40\n30.00\n(67.82, 192.98)\n4.35\n&lt; .001\n\n\nsex (male) × age\n-18.42\n41.76\n(-105.52, 68.68)\n-0.44\n0.664\n\n\n\n\n\nTo understand what’s happening, let’s replicate these results by hand for the interaction term.\n\n2.4.3 P-values\n\nShow R codebw_lm2 |&gt; \n  parameters(keep = \"sexmale:age\") |&gt;\n  print_md(\n    include_reference = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(20)\np\n\n\nsex (male) × age\n-18.42\n41.76\n(-105.52, 68.68)\n-0.44\n0.664\n\n\n\n\n\nShow R codebeta_hat = coef(summary(bw_lm2))[\"sexmale:age\", \"Estimate\"]\nse_hat = coef(summary(bw_lm2))[\"sexmale:age\", \"Std. Error\"]\ndfresid = bw_lm2$df.residual\nt_stat = abs(beta_hat)/se_hat\npval_t = \n  pt(-t_stat, df = dfresid, lower.tail = TRUE) +\n  pt(t_stat, df = dfresid, lower.tail = FALSE)\n\n\n\\[\n\\begin{aligned}\n&P\\left(\n| \\hat \\beta_{AM}  | &gt;\n| -18.4172| \\middle| H_0\n\\right)\n\\\\\n&= \\Pr \\left(\n\\left| \\frac{\\hat\\beta_{AM}}{\\hat{SE}(\\hat\\beta_{AM})} \\right| &gt;\n\\left| \\frac{-18.4172}{41.7558} \\right| \\middle| H_0\n\\right)\\\\\n&= \\Pr \\left(\n\\left| T_{20} \\right|  &gt;  0.4411 | H_0\n\\right)\\\\\n&= 0.6639\n\\end{aligned}\n\\]\n\nThis matches the result in the table above.\n\n\n2.4.4 Confidence intervals\n\nShow R codebw_lm2 |&gt; \n  parameters(keep = \"sexmale:age\") |&gt;\n  print_md(\n    include_reference = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(20)\np\n\n\nsex (male) × age\n-18.42\n41.76\n(-105.52, 68.68)\n-0.44\n0.664\n\n\n\n\n\nShow R codeq_t = qt(\n  p = 0.975, \n  df = dfresid, \n  lower.tail = TRUE)\n\nq_t = qt(\n  p = 0.025, \n  df = dfresid, \n  lower.tail = TRUE)\n\n\nconfint_radius_t = \n  se_hat * q_t\n\nconfint_t = beta_hat + c(-1,1) * confint_radius_t\n\nprint(confint_t)\n#&gt; [1]   68.68 -105.52\n\n\n\nThis also matches.\n\n\n2.4.5 Gaussian approximations\nHere are the asymptotic (Gaussian approximation) equivalents:\n\n2.4.6 P-values\n\nShow R codebw_lm2 |&gt; \n  parameters(keep = \"sexmale:age\") |&gt;\n  print_md(\n    include_reference = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(20)\np\n\n\nsex (male) × age\n-18.42\n41.76\n(-105.52, 68.68)\n-0.44\n0.664\n\n\n\n\n\nShow R codepval_z = pnorm(abs(t_stat), lower = FALSE) * 2\n\nprint(pval_z)\n#&gt; [1] 0.6592\n\n\n\n2.4.7 Confidence intervals\n\nShow R codebw_lm2 |&gt; \n  parameters(keep = \"sexmale:age\") |&gt;\n  print_md(\n    include_reference = TRUE)\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(20)\np\n\n\nsex (male) × age\n-18.42\n41.76\n(-105.52, 68.68)\n-0.44\n0.664\n\n\n\n\n\nShow R codeconfint_radius_z = se_hat * qnorm(0.975, lower = TRUE)\nconfint_z = \n  beta_hat + c(-1,1) * confint_radius_z\nprint(confint_z)\n#&gt; [1] -100.26   63.42\n\n\n\n2.4.8 Likelihood ratio statistics\n\nShow R code\nlogLik(bw_lm2)\n#&gt; 'log Lik.' -156.6 (df=5)\nlogLik(bw_lm1)\n#&gt; 'log Lik.' -156.7 (df=4)\n\nlLR = (logLik(bw_lm2) - logLik(bw_lm1)) |&gt; as.numeric()\ndelta_df = (bw_lm1$df.residual - df.residual(bw_lm2))\n\n\nx_max = 1\n\n\n\n\nShow R coded_lLR = function(x, df = delta_df) dchisq(x, df = df)\n\nchisq_plot = \n  ggplot() + \n  geom_function(fun = d_lLR) +\n  stat_function( fun = d_lLR, xlim = c(lLR, x_max), geom = \"area\", fill = \"gray\") +\n  geom_segment(aes(x = lLR, xend = lLR, y = 0, yend = d_lLR(lLR)), col = \"red\") + \n  xlim(0.0001,x_max) + \n  ylim(0,4) + \n  ylab(\"p(X=x)\") + \n  xlab(\"log(likelihood ratio) statistic [x]\") +\n  theme_classic()\nchisq_plot |&gt; print()\n\n\n\nFigure 2.5: Chi-square distribution\n\n\n\n\n\n\n\n\nNow we can get the p-value:\n\nShow R codepchisq(\n  q = 2*lLR, \n  df = delta_df, \n  lower = FALSE) |&gt; \n  print()\n#&gt; [1] 0.6298\n\n\n\nIn practice you don’t have to do this by hand; there are functions to do it for you:\n\nShow R code\n# built in\nlibrary(lmtest)\nlrtest(bw_lm2, bw_lm1)\n\n\n\n#Df\nLogLik\nDf\nChisq\nPr(&gt;Chisq)\n\n\n\n5\n-156.6\nNA\nNA\nNA\n\n\n4\n-156.7\n-1\n0.2323\n0.6298",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear (Gaussian) Models</span>"
    ]
  },
  {
    "objectID": "Linear-models-overview.html#goodness-of-fit",
    "href": "Linear-models-overview.html#goodness-of-fit",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "\n2.5 Goodness of fit",
    "text": "2.5 Goodness of fit\n\n2.5.1 AIC and BIC\n\nWhen we use likelihood ratio tests, we are comparing how well different models fit the data.\nLikelihood ratio tests require “nested” models: one must be a special case of the other.\nIf we have non-nested models, we can instead use the Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC):\n\n\nAIC = \\(-2 * \\ell(\\hat\\theta) + 2 * p\\)\nBIC = \\(-2 * \\ell(\\hat\\theta) + p * \\text{log}(n)\\)\n\nwhere \\(\\ell\\) is the log-likelihood of the data evaluated using the parameter estimates \\(\\hat\\theta\\), \\(p\\) is the number of estimated parameters in the model (including \\(\\hat\\sigma^2\\)), and \\(n\\) is the number of observations.\nYou can calculate these criteria using the logLik() function, or use the built-in R functions:\nAIC in R\n\nShow R code\n-2 * logLik(bw_lm2) |&gt; as.numeric() + \n  2*(length(coef(bw_lm2))+1) # sigma counts as a parameter here\n#&gt; [1] 323.2\n\nAIC(bw_lm2)\n#&gt; [1] 323.2\n\n\nBIC in R\n\nShow R code\n-2 * logLik(bw_lm2) |&gt; as.numeric() + \n  (length(coef(bw_lm2))+1) * log(nobs(bw_lm2))\n#&gt; [1] 329\n\nBIC(bw_lm2)\n#&gt; [1] 329\n\n\nLarge values of AIC and BIC are worse than small values. There are no hypothesis tests or p-values associated with these criteria.\n\n2.5.2 (Residual) Deviance\nLet \\(q\\) be the number of distinct covariate combinations in a data set.\n\nShow R codebw.X.unique = \n  bw |&gt; \n  count(sex, age)\n\nn_unique.bw  = nrow(bw.X.unique)\n\n\nFor example, in the birthweight data, there are \\(q = 12\\) unique patterns (Table 2.9).\n\nShow R codebw.X.unique\n\n\nTable 2.9: Unique covariate combinations in the birthweight data, with replicate counts\n\n\n\n\nsex\nage\nn\n\n\n\nfemale\n36\n2\n\n\nfemale\n37\n1\n\n\nfemale\n38\n2\n\n\nfemale\n39\n2\n\n\nfemale\n40\n4\n\n\nfemale\n42\n1\n\n\nmale\n35\n1\n\n\nmale\n36\n1\n\n\nmale\n37\n2\n\n\nmale\n38\n3\n\n\nmale\n40\n4\n\n\nmale\n41\n1\n\n\n\n\n\n\n\n\n\n\nDefinition 2.1 (Replicates) If a given covariate pattern has more than one observation in a dataset, those observations are called replicates.\n\n\n\nExample 2.1 (Replicates in the birthweight data) In the birthweight dataset, there are 2 replicates of the combination “female, age 36” (Table 2.9).\n\n\n\nExercise 2.7 (Replicates in the birthweight data) Which covariate pattern(s) in the birthweight data has the most replicates?\n\n\n\nSolution 2.1 (Replicates in the birthweight data). Two covariate patterns are tied for most replicates: males at age 40 weeks and females at age 40 weeks. 40 weeks is the usual length for human pregnancy (Polin, Fox, and Abman (2011)), so this result makes sense.\n\nShow R codebw.X.unique |&gt; dplyr::filter(n == max(n))\n\n\n\nsex\nage\nn\n\n\n\nfemale\n40\n4\n\n\nmale\n40\n4\n\n\n\n\n\n\n\nSaturated models\nThe most complicated model we could fit would have one parameter (a mean) for each covariate pattern, plus a variance parameter:\n\nShow R codelm_max = \n  bw |&gt; \n  mutate(age = factor(age)) |&gt; \n  lm(\n    formula = weight ~ sex:age - 1, \n    data = _)\n\nlm_max |&gt; \n  parameters() |&gt; \n  print_md()\n\n\nTable 2.10: Saturated model for the birthweight data\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(12)\np\n\n\n\nsex (male) × age35\n2925.00\n187.92\n(2515.55, 3334.45)\n15.56\n&lt; .001\n\n\nsex (female) × age36\n2570.50\n132.88\n(2280.98, 2860.02)\n19.34\n&lt; .001\n\n\nsex (male) × age36\n2625.00\n187.92\n(2215.55, 3034.45)\n13.97\n&lt; .001\n\n\nsex (female) × age37\n2539.00\n187.92\n(2129.55, 2948.45)\n13.51\n&lt; .001\n\n\nsex (male) × age37\n2737.50\n132.88\n(2447.98, 3027.02)\n20.60\n&lt; .001\n\n\nsex (female) × age38\n2872.50\n132.88\n(2582.98, 3162.02)\n21.62\n&lt; .001\n\n\nsex (male) × age38\n2982.00\n108.50\n(2745.60, 3218.40)\n27.48\n&lt; .001\n\n\nsex (female) × age39\n2846.00\n132.88\n(2556.48, 3135.52)\n21.42\n&lt; .001\n\n\nsex (female) × age40\n3152.25\n93.96\n(2947.52, 3356.98)\n33.55\n&lt; .001\n\n\nsex (male) × age40\n3256.25\n93.96\n(3051.52, 3460.98)\n34.66\n&lt; .001\n\n\nsex (male) × age41\n3292.00\n187.92\n(2882.55, 3701.45)\n17.52\n&lt; .001\n\n\nsex (female) × age42\n3210.00\n187.92\n(2800.55, 3619.45)\n17.08\n&lt; .001\n\n\n\n\n\n\n\n\nWe call this model the full, maximal, or saturated model for this dataset.\nWe can calculate the log-likelihood of this model as usual:\n\nShow R codelogLik(lm_max)\n#&gt; 'log Lik.' -151.4 (df=13)\n\n\nWe can compare this model to our other models using chi-square tests, as usual:\n\nShow R codelrtest(lm_max, bw_lm2)\n\n\n\n#Df\nLogLik\nDf\nChisq\nPr(&gt;Chisq)\n\n\n\n13\n-151.4\nNA\nNA\nNA\n\n\n5\n-156.6\n-8\n10.36\n0.241\n\n\n\n\n\nThe likelihood ratio statistic for this test is \\[\\lambda = 2 * (\\ell_{\\text{full}} - \\ell) = 10.3554\\] where:\n\n\n\\(\\ell_{\\text{max}}\\) is the log-likelihood of the full model: -151.4016\n\n\\(\\ell\\) is the log-likelihood of our comparison model (two slopes, two intercepts): -156.5793\n\nThis statistic is called the deviance or residual deviance for our two-slopes and two-intercepts model; it tells us how much the likelihood of that model deviates from the likelihood of the maximal model.\nThe corresponding p-value tells us whether there we have enough evidence to detect that our two-slopes, two-intercepts model is a worse fit for the data than the maximal model; in other words, it tells us if there’s evidence that we missed any important patterns. (Remember, a nonsignificant p-value could mean that we didn’t miss anything and a more complicated model is unnecessary, or it could mean we just don’t have enough data to tell the difference between these models.)\n\n2.5.3 Null Deviance\nSimilarly, the least complicated model we could fit would have only one mean parameter, an intercept:\n\\[\\text E[Y|X=x] = \\beta_0\\] We can fit this model in R like so:\n\nShow R codelm0 = lm(weight ~ 1, data = bw)\n\nlm0 |&gt; parameters() |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(23)\np\n\n\n(Intercept)\n2967.67\n57.58\n(2848.56, 3086.77)\n51.54\n&lt; .001\n\n\n\n\nThis model also has a likelihood:\n\nShow R codelogLik(lm0)\n#&gt; 'log Lik.' -169 (df=2)\n\n\nAnd we can compare it to more complicated models using a likelihood ratio test:\n\nShow R code\nlrtest(bw_lm2, lm0)\n\n\n\n#Df\nLogLik\nDf\nChisq\nPr(&gt;Chisq)\n\n\n\n5\n-156.6\nNA\nNA\nNA\n\n\n2\n-169.0\n-3\n24.75\n0\n\n\n\n\n\nThe likelihood ratio statistic for the test comparing the null model to the maximal model is \\[\\lambda = 2 * (\\ell_{\\text{full}} - \\ell_{0}) = 35.1067\\] where:\n\n\n\\(\\ell_{\\text{0}}\\) is the log-likelihood of the null model: -168.955\n\n\\(\\ell_{\\text{full}}\\) is the log-likelihood of the maximal model: -151.4016\n\nIn R, this test is:\n\nShow R codelrtest(lm_max, lm0)\n\n\n\n#Df\nLogLik\nDf\nChisq\nPr(&gt;Chisq)\n\n\n\n13\n-151.4\nNA\nNA\nNA\n\n\n2\n-169.0\n-11\n35.11\n2e-04\n\n\n\n\n\nThis log-likelihood ratio statistic is called the null deviance. It tells us whether we have enough data to detect a difference between the null and full models.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear (Gaussian) Models</span>"
    ]
  },
  {
    "objectID": "Linear-models-overview.html#rescaling",
    "href": "Linear-models-overview.html#rescaling",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "\n2.6 Rescaling",
    "text": "2.6 Rescaling\n\n2.6.1 Rescale age\n\nShow R code\nbw = \n  bw |&gt;\n  mutate(\n    `age - mean` = age - mean(age),\n    `age - 36wks` = age - 36\n  )\n\nlm1c = lm(weight ~ sex + `age - 36wks`, data = bw)\n\nlm2c = lm(weight ~ sex + `age - 36wks` + sex:`age - 36wks`, data = bw)\n\nparameters(lm2c, ci_method = \"wald\") |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(20)\np\n\n\n\n(Intercept)\n2552.73\n97.59\n(2349.16, 2756.30)\n26.16\n&lt; .001\n\n\nsex (male)\n209.97\n129.75\n(-60.68, 480.63)\n1.62\n0.121\n\n\nage - 36wks\n130.40\n30.00\n(67.82, 192.98)\n4.35\n&lt; .001\n\n\nsex (male) × age - 36wks\n-18.42\n41.76\n(-105.52, 68.68)\n-0.44\n0.664\n\n\n\n\n\nCompare with what we got without rescaling:\n\nShow R codeparameters(bw_lm2, ci_method = \"wald\") |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(20)\np\n\n\n\n(Intercept)\n-2141.67\n1163.60\n(-4568.90, 285.56)\n-1.84\n0.081\n\n\nsex (male)\n872.99\n1611.33\n(-2488.18, 4234.17)\n0.54\n0.594\n\n\nage\n130.40\n30.00\n(67.82, 192.98)\n4.35\n&lt; .001\n\n\nsex (male) × age\n-18.42\n41.76\n(-105.52, 68.68)\n-0.44\n0.664",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear (Gaussian) Models</span>"
    ]
  },
  {
    "objectID": "Linear-models-overview.html#prediction",
    "href": "Linear-models-overview.html#prediction",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "\n2.7 Prediction",
    "text": "2.7 Prediction\n\n2.7.1 Prediction for linear models\n\nDefinition 2.2 (Predicted value) In a regression model \\(\\text{p}(y|x)\\), the predicted value of \\(y\\) given \\(x\\) is the estimated mean of \\(Y\\) given \\(X\\):\n\\[\\hat y \\stackrel{\\text{def}}{=}\\hat{\\text{E}}\\left[Y|X=x\\right]\\]\n\n\nFor linear models, the predicted value can be straightforwardly calculated by multiplying each predictor value \\(x_j\\) by its corresponding coefficient \\(\\beta_j\\) and adding up the results:\n\\[\n\\begin{aligned}\n\\hat Y &= \\hat E[Y|X=x] \\\\\n&= x'\\hat\\beta \\\\\n&= \\hat\\beta_0\\cdot 1 + \\hat\\beta_1 x_1 + ... + \\hat\\beta_p x_p\n\\end{aligned}\n\\]\n\n2.7.2 Example: prediction for the birthweight data\n\nShow R code\nX = c(1,1,40)\nsum(X * coef(bw_lm1))\n#&gt; [1] 3225\n\n\nR has built-in functions for prediction:\n\nShow R codex = tibble(age = 40, sex = \"male\")\nbw_lm1 |&gt; predict(newdata = x)\n#&gt;    1 \n#&gt; 3225\n\n\nIf you don’t provide newdata, R will use the covariate values from the original dataset:\n\nShow R codepredict(bw_lm1)\n#&gt;    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n#&gt; 3225 3062 2984 2579 3225 3062 2621 2821 2742 3304 2863 2942 3346 3062 3225 2700 \n#&gt;   17   18   19   20   21   22   23   24 \n#&gt; 2863 2579 2984 2821 3225 2942 2984 3062\n\n\nThese special predictions are called the fitted values of the dataset:\n\nDefinition 2.3 For a given dataset \\((\\tilde{Y}, \\mathbf{X})\\) and corresponding fitted model \\(\\text{p}_{\\hat \\beta}(\\tilde{y}|\\mathbf{x})\\), the fitted value of \\(y_i\\) is the predicted value of \\(y\\) when \\(\\tilde{X}=\\tilde{x}_i\\) using the estimate parameters \\(\\hat \\beta\\).\n\nR has an extra function to get these values:\n\nShow R codefitted(bw_lm1)\n#&gt;    1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n#&gt; 3225 3062 2984 2579 3225 3062 2621 2821 2742 3304 2863 2942 3346 3062 3225 2700 \n#&gt;   17   18   19   20   21   22   23   24 \n#&gt; 2863 2579 2984 2821 3225 2942 2984 3062\n\n\n\n2.7.3 Quantifying uncertainty in predictions\n\nShow R codebw_lm1 |&gt; \n  predict(\n    newdata = x,\n    se.fit = TRUE)\n#&gt; $fit\n#&gt;    1 \n#&gt; 3225 \n#&gt; \n#&gt; $se.fit\n#&gt; [1] 61.46\n#&gt; \n#&gt; $df\n#&gt; [1] 21\n#&gt; \n#&gt; $residual.scale\n#&gt; [1] 177.1\n\n\nThis is a list(); you can extract the elements with $ or magrittr::use_series():\n\nShow R codebw_lm1 |&gt; \n  predict(\n    newdata = x,\n    se.fit = TRUE) |&gt; \n  use_series(se.fit)\n#&gt; [1] 61.46\n\n\nYou can get confidence intervals for \\(\\mathbb{E}\\left[Y|X=x\\right]\\):\n\nShow R codebw_lm1 |&gt; predict(\n  newdata = x,\n  interval = \"confidence\")\n\n\n\nfit\nlwr\nupr\n\n\n3225\n3098\n3353\n\n\n\n\nYou can also get prediction intervals for the value of an individual outcome \\(Y\\):\n\nShow R codebw_lm1 |&gt; \n  predict(newdata = x, interval = \"predict\")\n\n\n\nfit\nlwr\nupr\n\n\n3225\n2836\n3615\n\n\n\n\nThe warning from the last command is: “predictions on current data refer to future responses” (since you already know what happened to the current data, and thus don’t need to predict it).\nSee ?predict.lm for more.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear (Gaussian) Models</span>"
    ]
  },
  {
    "objectID": "Linear-models-overview.html#sec-diagnose-LMs",
    "href": "Linear-models-overview.html#sec-diagnose-LMs",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "\n2.8 Diagnostics",
    "text": "2.8 Diagnostics\n\n\n\n\n\n\nTip\n\n\n\nThis section is adapted from Dobson and Barnett (2018, secs. 6.2–6.3) and Dunn, Smyth, et al. (2018) Chapter 3.\n\n\n\n2.8.1 Assumptions in linear regression models\n\\[Y|\\tilde{X}\\ \\sim_{⫫}\\ N(\\tilde{X}'\\beta,\\sigma^2)\\]\n\nNormality: The distribution conditional on a given \\(X\\) value is normal\nCorrect Functional Form: The conditional means have the structure\n\n\\[E[Y|\\tilde{X} = \\tilde{x}] = \\tilde{x}'\\beta\\] 3. Homoskedasticity: The variance \\(\\sigma^2\\) is constant (with respect to \\(\\tilde{x}\\))\n\nIndependence: The observations are statistically independent\n\n2.8.2 Direct visualization\n\nThe most direct way to examine the fit of a model is to compare it to the raw observed data.\n\n\nShow R codebw = \n  bw |&gt; \n  mutate(\n    predlm2 = predict(bw_lm2)\n  ) |&gt; \n  arrange(sex, age)\n\nplot1_interact = \n  plot1 %+% bw +\n  geom_line(aes(y = predlm2))\n\nprint(plot1_interact)\n\n\n\nFigure 2.6: Birthweight model with interaction term\n\n\n\n\n\n\n\n\nIt’s not easy to assess these assumptions from this model. If there are multiple continuous covariates, it becomes even harder to visualize the raw data.\n\n\n2.8.3 Residuals\n\nMaybe we can transform the data and model in some way to make it easier to inspect.\n\n\nDefinition 2.4 (Residual noise) The residual noise in a probabilistic model \\(p(Y)\\) is the difference between an observed value \\(y\\) and its distributional mean:\n\\[\\varepsilon(y) \\stackrel{\\text{def}}{=}y - \\mathbb{E}\\left[Y\\right] \\tag{2.8}\\]\n\n\nWe use the same notation for residual noise that we used for errors. \\(\\mathbb{E}\\left[Y\\right]\\) can be viewed as an estimate of \\(Y\\), before \\(y\\) is observed. Conversely, each observation \\(y\\) can be viewed as an estimate of \\(\\mathbb{E}\\left[Y\\right]\\) (albeit an imprecise one, individually, since \\(n=1\\)).\n\nWe can rearrange Equation 2.8 to view \\(y\\) as the sum of its mean plus the residual noise:\n\\[y = \\mathbb{E}\\left[Y\\right] + \\varepsilon{y}\\]\n\n\nTheorem 2.1 (Residuals in Gaussian models) If \\(Y\\) has a Gaussian distribution, then \\(\\varepsilon(Y)\\) also has a Gaussian distribution, and vice versa.\n\n\nProof. Left to the reader.\n\n\n\nDefinition 2.5 (Residual errors of a fitted model value) The residual of a fitted value \\(\\hat y\\) (shorthand: “residual”) is its error: \\[\n\\begin{aligned}\ne(\\hat y) &\\stackrel{\\text{def}}{=}\\varepsilon\\left(\\hat y\\right)\n\\\\&= y - \\hat y\n\\end{aligned}\n\\]\n\n\\(e(\\hat y)\\) can be seen as the maximum likelihood estimate of the residual noise:\n\\[\n\\begin{aligned}\ne(\\hat y) &= y - \\hat y\n\\\\ &= \\hat\\varepsilon_{ML}\n\\end{aligned}\n\\]\n\nGeneral characteristics of residuals\n\nTheorem 2.2 For unbiased estimators \\(\\hat\\theta\\):\n\\[\\mathbb{E}\\left[e(y)\\right] = 0 \\tag{2.9}\\] \\[\\text{Var}\\left(e(y)\\right) \\approx \\sigma^2 \\tag{2.10}\\]\n\n\nProof.  \nEquation 2.9:\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[e(y)\\right] &= \\mathbb{E}\\left[y - \\hat y\\right]\n\\\\ &= \\mathbb{E}\\left[y\\right] - \\mathbb{E}\\left[\\hat y\\right]\n\\\\ &= \\mathbb{E}\\left[y\\right] - \\mathbb{E}\\left[y\\right]\n\\\\ &= 0\n\\end{aligned}\n\\]\nEquation 2.10:\n\\[\n\\begin{aligned}\n\\text{Var}\\left(e(y)\\right) &= \\text{Var}\\left(y - \\hat y\\right)\n\\\\ &= \\text{Var}\\left(y\\right) + \\text{Var}\\left(\\hat y\\right) - 2 \\text{Cov}\\left(y, \\hat y\\right)\n\\\\ &{\\dot{\\approx}} \\text{Var}\\left(y\\right) + 0 - 2 \\cdot 0\n\\\\ &= \\text{Var}\\left(y\\right)\n\\\\ &= \\sigma^2\n\\end{aligned}\n\\]\n\nCharacteristics of residuals in Gaussian models\nWith enough data and a correct model, the residuals will be approximately Guassian distributed, with variance \\(\\sigma^2\\), which we can estimate using \\(\\hat\\sigma^2\\): that is:\n\\[\ne_i \\ \\sim_{\\text{iid}}\\ N(0, \\hat\\sigma^2)\n\\]\n\n\nExample 2.2 (residuals in birthweight data) R provides a function for residuals:\n\nShow R coderesid(bw_lm2)\n#&gt;       1       2       3       4       5       6       7       8       9      10 \n#&gt;  176.27 -140.73 -144.13  -59.53  177.47 -126.93  -68.93  242.67 -139.33   51.67 \n#&gt;      11      12      13      14      15      16      17      18      19      20 \n#&gt;  156.67 -125.13  274.28 -137.71  -27.69 -246.69 -191.67  189.33  -11.67 -242.64 \n#&gt;      21      22      23      24 \n#&gt;  -47.64  262.36  210.36  -30.62\n\n\n\n\nExercise 2.8 Check R’s output by computing the residuals directly.\n\n\nSolution.  \n\nShow R codebw$weight - fitted(bw_lm2)\n#&gt;       1       2       3       4       5       6       7       8       9      10 \n#&gt;  176.27 -140.73 -144.13  -59.53  177.47 -126.93  -68.93  242.67 -139.33   51.67 \n#&gt;      11      12      13      14      15      16      17      18      19      20 \n#&gt;  156.67 -125.13  274.28 -137.71  -27.69 -246.69 -191.67  189.33  -11.67 -242.64 \n#&gt;      21      22      23      24 \n#&gt;  -47.64  262.36  210.36  -30.62\n\n\nThis matches R’s output!\n\nGraph the residuals\n\nShow R codebw = bw |&gt; \n  mutate(resids_intxn = \n           weight - fitted(bw_lm2))\n\nplot_bw_resid =\n  bw |&gt; \n  ggplot(aes(\n    x = age, \n    y = resids_intxn,\n    linetype = sex,\n    shape = sex,\n    col = sex))  +\n  theme_bw() +\n  xlab(\"Gestational age (weeks)\") +\n  ylab(\"residuals (grams)\") +\n  theme(legend.position = \"bottom\") +\n  # expand_limits(y = 0, x = 0) +\n  geom_point(alpha = .7)\nprint(plot_bw_resid + facet_wrap(~ sex))\n\n\n\nFigure 2.7: Residuals of interaction model for birthweight data\n\n\n\n\n\n\n\n\n\nDefinition 2.6 (Standardized residuals) \\[r_i = \\frac{e_i}{\\widehat{SD}(e_i)}\\]\n\nHence, with enough data and a correct model, the standardized residuals will be approximately standard Gaussian; that is,\n\\[\nr_i \\ \\sim_{\\text{iid}}\\ N(0,1)\n\\]\n\n2.8.4 Marginal distributions of residuals\nTo look for problems with our model, we can check whether the residuals \\(e_i\\) and standardized residuals \\(r_i\\) look like they have the distributions that they are supposed to have, according to the model.\n\nStandardized residuals in R\n\nShow R code\nrstandard(bw_lm2)\n#&gt;        1        2        3        4        5        6        7        8 \n#&gt;  1.15982 -0.92601 -0.87479 -0.34723  1.03507 -0.73473 -0.39901  1.43752 \n#&gt;        9       10       11       12       13       14       15       16 \n#&gt; -0.82539  0.30606  0.92807 -0.87616  1.91428 -0.86559 -0.16430 -1.46376 \n#&gt;       17       18       19       20       21       22       23       24 \n#&gt; -1.11016  1.09658 -0.06761 -1.46159 -0.28696  1.58040  1.26717 -0.19805\nresid(bw_lm2)/sigma(bw_lm2)\n#&gt;        1        2        3        4        5        6        7        8 \n#&gt;  0.97593 -0.77920 -0.79802 -0.32962  0.98258 -0.70279 -0.38166  1.34357 \n#&gt;        9       10       11       12       13       14       15       16 \n#&gt; -0.77144  0.28606  0.86741 -0.69282  1.51858 -0.76244 -0.15331 -1.36584 \n#&gt;       17       18       19       20       21       22       23       24 \n#&gt; -1.06123  1.04825 -0.06463 -1.34341 -0.26376  1.45262  1.16471 -0.16954\n\n\n\nThese are not quite the same, because R is doing something more complicated and precise to get the standard errors. Let’s not worry about those details for now; the difference is pretty small in this case:\n\n\n\nShow R code\nrstandard_compare_plot = \n  tibble(\n    x = resid(bw_lm2)/sigma(bw_lm2), \n    y = rstandard(bw_lm2)) |&gt; \n  ggplot(aes(x = x, y = y)) +\n  geom_point() + \n  theme_bw() +\n  coord_equal() + \n  xlab(\"resid(bw_lm2)/sigma(bw_lm2)\") +\n  ylab(\"rstandard(bw_lm2)\") +\n  geom_abline(\n    aes(\n      intercept = 0,\n      slope = 1, \n      col = \"x=y\")) +\n  labs(colour=\"\") +\n  scale_colour_manual(values=\"red\")\n\nprint(rstandard_compare_plot)\n\n\n\n\n\n\n\n\nLet’s add these residuals to the tibble of our dataset:\n\nShow R code\nbw = \n  bw |&gt; \n  mutate(\n    fitted_lm2 = fitted(bw_lm2),\n    \n    resid_lm2 = resid(bw_lm2),\n    # resid_lm2 = weight - fitted_lm2,\n    \n    std_resid_lm2 = rstandard(bw_lm2),\n    # std_resid_lm2 = resid_lm2 / sigma(bw_lm2)\n  )\n\nbw |&gt; \n  select(\n    sex,\n    age,\n    weight,\n    fitted_lm2,\n    resid_lm2,\n    std_resid_lm2\n  )\n\n\n\nsex\nage\nweight\nfitted_lm2\nresid_lm2\nstd_resid_lm2\n\n\n\nfemale\n36\n2729\n2553\n176.27\n1.1598\n\n\nfemale\n36\n2412\n2553\n-140.73\n-0.9260\n\n\nfemale\n37\n2539\n2683\n-144.13\n-0.8748\n\n\nfemale\n38\n2754\n2814\n-59.53\n-0.3472\n\n\nfemale\n38\n2991\n2814\n177.47\n1.0351\n\n\nfemale\n39\n2817\n2944\n-126.93\n-0.7347\n\n\nfemale\n39\n2875\n2944\n-68.93\n-0.3990\n\n\nfemale\n40\n3317\n3074\n242.67\n1.4375\n\n\nfemale\n40\n2935\n3074\n-139.33\n-0.8254\n\n\nfemale\n40\n3126\n3074\n51.67\n0.3061\n\n\nfemale\n40\n3231\n3074\n156.67\n0.9281\n\n\nfemale\n42\n3210\n3335\n-125.13\n-0.8762\n\n\nmale\n35\n2925\n2651\n274.28\n1.9143\n\n\nmale\n36\n2625\n2763\n-137.71\n-0.8656\n\n\nmale\n37\n2847\n2875\n-27.69\n-0.1643\n\n\nmale\n37\n2628\n2875\n-246.69\n-1.4638\n\n\nmale\n38\n2795\n2987\n-191.67\n-1.1102\n\n\nmale\n38\n3176\n2987\n189.33\n1.0966\n\n\nmale\n38\n2975\n2987\n-11.67\n-0.0676\n\n\nmale\n40\n2968\n3211\n-242.64\n-1.4616\n\n\nmale\n40\n3163\n3211\n-47.64\n-0.2870\n\n\nmale\n40\n3473\n3211\n262.36\n1.5804\n\n\nmale\n40\n3421\n3211\n210.36\n1.2672\n\n\nmale\n41\n3292\n3323\n-30.62\n-0.1981\n\n\n\n\n\n\n\nNow let’s build histograms:\n\n\nShow R coderesid_marginal_hist = \n  bw |&gt; \n  ggplot(aes(x = resid_lm2)) +\n  geom_histogram()\n\nprint(resid_marginal_hist)\n\n\n\nFigure 2.8: Marginal distribution of (nonstandardized) residuals\n\n\n\n\n\n\n\n\nHard to tell with this small amount of data, but I’m a bit concerned that the histogram doesn’t show a bell-curve shape.\n\n\n\nShow R codestd_resid_marginal_hist = \n  bw |&gt; \n  ggplot(aes(x = std_resid_lm2)) +\n  geom_histogram()\n\nprint(std_resid_marginal_hist)\n\n\n\nFigure 2.9: Marginal distribution of standardized residuals\n\n\n\n\n\n\n\n\nThis looks similar, although the scale of the x-axis got narrower, because we divided by \\(\\hat\\sigma\\) (roughly speaking).\nStill hard to tell if the distribution is Gaussian.\n\n\n2.8.5 QQ plot of standardized residuals\n\nAnother way to assess normality is the QQ plot of the standardized residuals versus normal quantiles:\n\n\nShow R code\nlibrary(ggfortify) \n# needed to make ggplot2::autoplot() work for `lm` objects\n\nqqplot_lm2_auto = \n  bw_lm2 |&gt; \n  autoplot(\n    which = 2, # options are 1:6; can do multiple at once\n    ncol = 1) +\n  theme_classic()\n\nprint(qqplot_lm2_auto)\n\n\n\n\n\n\n\n\nIf the Gaussian model were correct, these points should follow the dotted line.\nFig 2.4 panel (c) in Dobson and Barnett (2018) is a little different; they didn’t specify how they produced it, but other statistical analysis systems do things differently from R.\nSee also Dunn, Smyth, et al. (2018) §3.5.4.\n\n\nQQ plot - how it’s built\n\nLet’s construct it by hand:\n\n\nShow R code\nbw = bw |&gt; \n  mutate(\n    p = (rank(std_resid_lm2) - 1/2)/n(), # \"Blom's method\"\n    expected_quantiles_lm2 = qnorm(p)\n  )\n\nqqplot_lm2 = \n  bw |&gt; \n  ggplot(\n    aes(\n      x = expected_quantiles_lm2, \n      y = std_resid_lm2, \n      col = sex, \n      shape = sex)\n  ) + \n  geom_point() +\n  theme_classic() +\n  theme(legend.position='none') + # removing the plot legend\n  ggtitle(\"Normal Q-Q\") +\n  xlab(\"Theoretical Quantiles\") + \n  ylab(\"Standardized residuals\")\n\n# find the expected line:\n\nps &lt;- c(.25, .75)                  # reference probabilities\na &lt;- quantile(rstandard(bw_lm2), ps)  # empirical quantiles\nb &lt;- qnorm(ps)                     # theoretical quantiles\n\nqq_slope = diff(a)/diff(b)\nqq_intcpt = a[1] - b[1] * qq_slope\n\nqqplot_lm2 = \n  qqplot_lm2 +\n  geom_abline(slope = qq_slope, intercept = qq_intcpt)\n\nprint(qqplot_lm2)\n\n\n\n\n\n\n\n\n2.8.6 Conditional distributions of residuals\nIf our Gaussian linear regression model is correct, the residuals \\(e_i\\) and standardized residuals \\(r_i\\) should have:\n\nan approximately Gaussian distribution, with:\na mean of 0\na constant variance\n\nThis should be true for every value of \\(x\\).\n\nIf we didn’t correctly guess the functional form of the linear component of the mean, \\[\\text{E}[Y|X=x] = \\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p\\]\nThen the the residuals might have nonzero mean.\nRegardless of whether we guessed the mean function correctly, ther the variance of the residuals might differ between values of \\(x\\).\n\nResiduals versus fitted values\n\nTo look for these issues, we can plot the residuals \\(e_i\\) against the fitted values \\(\\hat y_i\\) (Figure 2.10).\n\n\nShow R codeautoplot(bw_lm2, which = 1, ncol = 1) |&gt; print()\n\n\n\nFigure 2.10: birthweight model (Equation 2.2): residuals versus fitted values\n\n\n\n\n\n\n\n\nIf the model is correct, the blue line should stay flat and close to 0, and the cloud of dots should have the same vertical spread regardless of the fitted value.\nIf not, we probably need to change the functional form of linear component of the mean, \\[\\text{E}[Y|X=x] = \\beta_0 + \\beta_1 X_1 + ... + \\beta_p X_p\\]\n\nExample: PLOS Medicine title length data\n(Adapted from Dobson and Barnett (2018), §6.7.1)\n\nShow R codedata(PLOS, package = \"dobson\")\nlibrary(ggplot2)\nfig1 = \n  PLOS |&gt; \n  ggplot(\n    aes(x = authors,\n        y = nchar)\n  ) +\n  geom_point() +\n  theme(legend.position = \"bottom\") +\n  labs(col = \"\") +\n  guides(col=guide_legend(ncol=3))\nfig1\n\n\n\nFigure 2.11: Number of authors versus title length in PLOS Medicine articles\n\n\n\n\n\n\n\n\nLinear fit\n\nShow R codelm_PLOS_linear = lm(\n  formula = nchar ~ authors, \n  data = PLOS)\n\n\nShow R codefig2 = fig1 +\n  geom_smooth(\n    method = \"lm\", \n              fullrange = TRUE,\n              aes(col = \"lm(y ~ x)\"))\nfig2\n\nlibrary(ggfortify)\nautoplot(lm_PLOS_linear, which = 1, ncol = 1)\n\n\nFigure 2.12: Number of authors versus title length in PLOS Medicine, with linear model fit\n\n\n\n\n\n(a) Data and fit\n\n\n\n\n\n\n\n\n\n(b) Residuals vs fitted\n\n\n\n\n\n\n\n\n\n\nQuadratic fit\n\nShow R codelm_PLOS_quad = lm(\n  formula = nchar ~ authors + I(authors^2), \n  data = PLOS)\n\n\nShow R codefig3 = \n  fig2 + \ngeom_smooth(\n    method = \"lm\",\n    fullrange = TRUE,\n    formula = y ~ x + I(x ^ 2),\n    aes(col = \"lm(y ~ x + I(x^2))\")\n  )\nfig3\n\nautoplot(lm_PLOS_quad, which = 1, ncol = 1)\n\n\nFigure 2.13: Number of authors versus title length in PLOS Medicine, with quadratic model fit\n\n\n\n\n\n(a) Data and fit\n\n\n\n\n\n\n\n\n\n(b) Residuals vs fitted\n\n\n\n\n\n\n\n\n\n\nLinear versus quadratic fits\nShow R codelibrary(ggfortify)\nautoplot(lm_PLOS_linear, which = 1, ncol = 1)\n\nautoplot(lm_PLOS_quad, which = 1, ncol = 1)\n\n\nFigure 2.14: Residuals versus fitted plot for linear and quadratic fits to PLOS data\n\n\n\n\n\n(a) Linear\n\n\n\n\n\n\n\n\n\n(b) Quadratic\n\n\n\n\n\n\n\n\n\n\nCubic fit\n\nShow R codelm_PLOS_cub = lm(\n  formula = nchar ~ authors + I(authors^2) + I(authors^3), \n  data = PLOS)\n\n\nShow R codefig4 = \n  fig3 + \ngeom_smooth(\n    method = \"lm\",\n    fullrange = TRUE,\n    formula = y ~ x + I(x ^ 2) + I(x ^ 3),\n    aes(col = \"lm(y ~ x + I(x^2) + I(x ^ 3))\")\n  )\nfig4\n\nautoplot(lm_PLOS_cub, which = 1, ncol = 1)\n\n\nFigure 2.15: Number of authors versus title length in PLOS Medicine, with cubic model fit\n\n\n\n\n\n(a) Data and fit\n\n\n\n\n\n\n\n\n\n(b) Residuals vs fitted\n\n\n\n\n\n\n\n\n\n\nLogarithmic fit\n\nShow R codelm_PLOS_log = lm(nchar ~ log(authors), data = PLOS)\n\n\nShow R codefig5 = fig4 + \n  geom_smooth(\n    method = \"lm\",\n    fullrange = TRUE,\n    formula = y ~ log(x),\n    aes(col = \"lm(y ~ log(x))\")\n  )\nfig5\n\nautoplot(lm_PLOS_log, which = 1, ncol = 1)\n\n\nFigure 2.16: logarithmic fit\n\n\n\n\n\n(a) Data and fit\n\n\n\n\n\n\n\n\n\n(b) Residuals vs fitted\n\n\n\n\n\n\n\n\n\n\nModel selection\n\nShow R codeanova(lm_PLOS_linear, lm_PLOS_quad)\n\n\nTable 2.11: linear vs quadratic\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n876\n947502\nNA\nNA\nNA\nNA\n\n\n875\n880950\n1\n66552\n66.1\n0\n\n\n\n\n\n\n\n\n\nShow R codeanova(lm_PLOS_quad, lm_PLOS_cub)\n\n\nTable 2.12: quadratic vs cubic\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n875\n880950\nNA\nNA\nNA\nNA\n\n\n874\n865933\n1\n15018\n15.16\n1e-04\n\n\n\n\n\n\n\n\nAIC/BIC\n\nShow R codeAIC(lm_PLOS_quad)\n#&gt; [1] 8568\nAIC(lm_PLOS_cub)\n#&gt; [1] 8555\n\n\n\nShow R codeAIC(lm_PLOS_cub)\n#&gt; [1] 8555\nAIC(lm_PLOS_log)\n#&gt; [1] 8544\n\n\n\nShow R codeBIC(lm_PLOS_cub)\n#&gt; [1] 8578\nBIC(lm_PLOS_log)\n#&gt; [1] 8558\n\n\nExtrapolation is dangerous\n\nShow R codefig_all = fig5 +\n  xlim(0, 60)\nfig_all\n\n\n\nFigure 2.17: Number of authors versus title length in PLOS Medicine\n\n\n\n\n\n\n\nScale-location plot\n\nWe can also plot the square roots of the absolute values of the standardized residuals against the fitted values (Figure 2.18).\n\n\nShow R codeautoplot(bw_lm2, which = 3, ncol = 1) |&gt; print()\n\n\n\nFigure 2.18: Scale-location plot of birthweight data\n\n\n\n\n\n\n\n\nHere, the blue line doesn’t need to be near 0, but it should be flat. If not, the residual variance \\(\\sigma^2\\) might not be constant, and we might need to transform our outcome \\(Y\\) (or use a model that allows non-constant variance).\n\nResiduals versus leverage\n\nWe can also plot our standardized residuals against “leverage”, which roughly speaking is a measure of how unusual each \\(x_i\\) value is. Very unusual \\(x_i\\) values can have extreme effects on the model fit, so we might want to remove those observations as outliers, particularly if they have large residuals.\n\n\nShow R codeautoplot(bw_lm2, which = 5, ncol = 1) |&gt; print()\n\n\nbirthweight model with interactions (Equation 2.2): residuals versus leverage\n\n\n\n\n\nThe blue line should be relatively flat and close to 0 here.\n\n\n2.8.7 Diagnostics constructed by hand\n\nShow R code\nbw = \n  bw |&gt; \n  mutate(\n    predlm2 = predict(bw_lm2),\n    residlm2 = weight - predlm2,\n    std_resid = residlm2 / sigma(bw_lm2),\n    # std_resid_builtin = rstandard(bw_lm2), # uses leverage\n    sqrt_abs_std_resid = std_resid |&gt; abs() |&gt; sqrt()\n    \n  )\n\n\nResiduals vs fitted\n\nShow R code\nresid_vs_fit = bw |&gt; \n  ggplot(\n    aes(x = predlm2, y = residlm2, col = sex, shape = sex)\n  ) + \n  geom_point() +\n  theme_classic() +\n  geom_hline(yintercept = 0)\n\n\n\nShow R codeprint(resid_vs_fit)\n\n\n\n\n\n\n\nStandardized residuals vs fitted\n\nShow R codebw |&gt; \n  ggplot(\n    aes(x = predlm2, y = std_resid, col = sex, shape = sex)\n  ) + \n  geom_point() +\n  theme_classic() +\n  geom_hline(yintercept = 0)\n\n\n\n\n\n\n\nStandardized residuals vs gestational age\n\nShow R code\nbw |&gt; \n  ggplot(\n    aes(x = age, y = std_resid, col = sex, shape = sex)\n  ) + \n  geom_point() +\n  theme_classic() +\n  geom_hline(yintercept = 0)\n\n\n\n\n\n\n\n\nsqrt(abs(rstandard())) vs fitted\nCompare with autoplot(bw_lm2, 3)\n\nShow R code\n\nbw |&gt; \n  ggplot(\n    aes(x = predlm2, y = sqrt_abs_std_resid, col = sex, shape = sex)\n  ) + \n  geom_point() +\n  theme_classic() +\n  geom_hline(yintercept = 0)",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear (Gaussian) Models</span>"
    ]
  },
  {
    "objectID": "Linear-models-overview.html#model-selection-1",
    "href": "Linear-models-overview.html#model-selection-1",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "\n2.9 Model selection",
    "text": "2.9 Model selection\n(adapted from Dobson and Barnett (2018) §6.3.3; for more information on prediction, see James et al. (2013) and Harrell (2015)).\n\nIf we have a lot of covariates in our dataset, we might want to choose a small subset to use in our model.\nThere are a few possible metrics to consider for choosing a “best” model.\n\n\n2.9.1 Mean squared error\nWe might want to minimize the mean squared error, \\(\\text E[(y-\\hat y)^2]\\), for new observations that weren’t in our data set when we fit the model.\nUnfortunately, \\[\\frac{1}{n}\\sum_{i=1}^n (y_i-\\hat y_i)^2\\] gives a biased estimate of \\(\\text E[(y-\\hat y)^2]\\) for new data. If we want an unbiased estimate, we will have to be clever.\n\nCross-validation\n\nShow R codedata(\"carbohydrate\", package = \"dobson\")\nlibrary(cvTools)\nfull_model &lt;- lm(carbohydrate ~ ., data = carbohydrate)\ncv_full = \n  full_model |&gt; cvFit(\n    data = carbohydrate, K = 5, R = 10,\n    y = carbohydrate$carbohydrate)\n\nreduced_model = update(full_model, \n                       formula = ~ . - age)\n\ncv_reduced = \n  reduced_model |&gt; cvFit(\n    data = carbohydrate, K = 5, R = 10,\n    y = carbohydrate$carbohydrate)\n\n\n\n\nShow R coderesults_reduced = \n  tibble(\n      model = \"wgt+protein\",\n      errs = cv_reduced$reps[])\nresults_full = \n  tibble(model = \"wgt+age+protein\",\n           errs = cv_full$reps[])\n\ncv_results = \n  bind_rows(results_reduced, results_full)\n\ncv_results |&gt; \n  ggplot(aes(y = model, x = errs)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\ncomparing metrics\n\nShow R code\ncompare_results = tribble(\n  ~ model, ~ cvRMSE, ~ r.squared, ~adj.r.squared, ~ trainRMSE, ~loglik,\n  \"full\", cv_full$cv, summary(full_model)$r.squared,  summary(full_model)$adj.r.squared, sigma(full_model), logLik(full_model) |&gt; as.numeric(),\n  \"reduced\", cv_reduced$cv, summary(reduced_model)$r.squared, summary(reduced_model)$adj.r.squared, sigma(reduced_model), logLik(reduced_model) |&gt; as.numeric())\n\ncompare_results\n\n\n\nmodel\ncvRMSE\nr.squared\nadj.r.squared\ntrainRMSE\nloglik\n\n\n\nfull\n7.169\n0.4805\n0.3831\n5.956\n-61.84\n\n\nreduced\n6.737\n0.4454\n0.3802\n5.971\n-62.49\n\n\n\n\n\n\n\nShow R codeanova(full_model, reduced_model)\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n16\n567.7\nNA\nNA\nNA\nNA\n\n\n17\n606.0\n-1\n-38.36\n1.081\n0.3139\n\n\n\n\n\nstepwise regression\n\nShow R codelibrary(olsrr)\nolsrr:::ols_step_both_aic(full_model)\n#&gt; \n#&gt; \n#&gt;                              Stepwise Summary                              \n#&gt; -------------------------------------------------------------------------\n#&gt; Step    Variable         AIC        SBC       SBIC       R2       Adj. R2 \n#&gt; -------------------------------------------------------------------------\n#&gt;  0      Base Model     140.773    142.764    83.068    0.00000    0.00000 \n#&gt;  1      protein (+)    137.950    140.937    80.438    0.21427    0.17061 \n#&gt;  2      weight (+)     132.981    136.964    77.191    0.44544    0.38020 \n#&gt; -------------------------------------------------------------------------\n#&gt; \n#&gt; Final Model Output \n#&gt; ------------------\n#&gt; \n#&gt;                          Model Summary                          \n#&gt; ---------------------------------------------------------------\n#&gt; R                       0.667       RMSE                 5.505 \n#&gt; R-Squared               0.445       MSE                 35.648 \n#&gt; Adj. R-Squared          0.380       Coef. Var           15.879 \n#&gt; Pred R-Squared          0.236       AIC                132.981 \n#&gt; MAE                     4.593       SBC                136.964 \n#&gt; ---------------------------------------------------------------\n#&gt;  RMSE: Root Mean Square Error \n#&gt;  MSE: Mean Square Error \n#&gt;  MAE: Mean Absolute Error \n#&gt;  AIC: Akaike Information Criteria \n#&gt;  SBC: Schwarz Bayesian Criteria \n#&gt; \n#&gt;                                ANOVA                                \n#&gt; -------------------------------------------------------------------\n#&gt;                 Sum of                                             \n#&gt;                Squares        DF    Mean Square      F        Sig. \n#&gt; -------------------------------------------------------------------\n#&gt; Regression     486.778         2        243.389    6.827    0.0067 \n#&gt; Residual       606.022        17         35.648                    \n#&gt; Total         1092.800        19                                   \n#&gt; -------------------------------------------------------------------\n#&gt; \n#&gt;                                   Parameter Estimates                                    \n#&gt; ----------------------------------------------------------------------------------------\n#&gt;       model      Beta    Std. Error    Std. Beta      t        Sig      lower     upper \n#&gt; ----------------------------------------------------------------------------------------\n#&gt; (Intercept)    33.130        12.572                  2.635    0.017     6.607    59.654 \n#&gt;     protein     1.824         0.623        0.534     2.927    0.009     0.509     3.139 \n#&gt;      weight    -0.222         0.083       -0.486    -2.662    0.016    -0.397    -0.046 \n#&gt; ----------------------------------------------------------------------------------------\n\n\nLasso\n\\[\\arg min_{\\theta} \\ell(\\theta) + \\lambda \\sum_{j=1}^p|\\beta_j|\\]\n\nShow R codelibrary(glmnet)\ny = carbohydrate$carbohydrate\nx = carbohydrate |&gt; \n  select(age, weight, protein) |&gt; \n  as.matrix()\nfit = glmnet(x,y)\n\n\n\n\nShow R codeautoplot(fit, xvar = 'lambda')\n\n\n\nFigure 2.19: Lasso selection\n\n\n\n\n\n\n\n\n\nShow R codecvfit = cv.glmnet(x,y)\nplot(cvfit)\n\n\n\n\n\n\n\n\n\nShow R codecoef(cvfit, s = \"lambda.1se\")\n#&gt; 4 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                  s1\n#&gt; (Intercept) 34.1090\n#&gt; age          .     \n#&gt; weight      -0.1041\n#&gt; protein      0.9441",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear (Gaussian) Models</span>"
    ]
  },
  {
    "objectID": "Linear-models-overview.html#categorical-covariates-with-more-than-two-levels",
    "href": "Linear-models-overview.html#categorical-covariates-with-more-than-two-levels",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "\n2.10 Categorical covariates with more than two levels",
    "text": "2.10 Categorical covariates with more than two levels\n\n2.10.1 Example: birthweight\n\nIn the birthweight example, the variable sex had only two observed values:\n\nShow R codeunique(bw$sex)\n#&gt; [1] female male  \n#&gt; Levels: female male\n\n\nIf there are more than two observed values, we can’t just use a single variable with 0s and 1s.\n\n2.10.2 \n\nFor example, Table 2.13 shows the (in)famous iris data (Anderson (1935)), and Table 2.14 provides summary statistics. The data include three species: “setosa”, “versicolor”, and “virginica”.\n\n\nShow R codehead(iris)\n\n\nTable 2.13: The iris data\n\n\n\n\nSepal.Length\nSepal.Width\nPetal.Length\nPetal.Width\nSpecies\n\n\n\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n5.4\n3.9\n1.7\n0.4\nsetosa\n\n\n\n\n\n\n\n\n\nShow R codelibrary(table1)\ntable1(\n  x = ~ . | Species,\n  data = iris,\n  overall = FALSE\n)\n\n\nTable 2.14: Summary statistics for the iris data\n\n\n\n\n\n\n\n\n\n\n\n\n\nsetosa(N=50)\nversicolor(N=50)\nvirginica(N=50)\n\n\n\nSepal.Length\n\n\n\n\n\nMean (SD)\n5.01 (0.352)\n5.94 (0.516)\n6.59 (0.636)\n\n\nMedian [Min, Max]\n5.00 [4.30, 5.80]\n5.90 [4.90, 7.00]\n6.50 [4.90, 7.90]\n\n\nSepal.Width\n\n\n\n\n\nMean (SD)\n3.43 (0.379)\n2.77 (0.314)\n2.97 (0.322)\n\n\nMedian [Min, Max]\n3.40 [2.30, 4.40]\n2.80 [2.00, 3.40]\n3.00 [2.20, 3.80]\n\n\nPetal.Length\n\n\n\n\n\nMean (SD)\n1.46 (0.174)\n4.26 (0.470)\n5.55 (0.552)\n\n\nMedian [Min, Max]\n1.50 [1.00, 1.90]\n4.35 [3.00, 5.10]\n5.55 [4.50, 6.90]\n\n\nPetal.Width\n\n\n\n\n\nMean (SD)\n0.246 (0.105)\n1.33 (0.198)\n2.03 (0.275)\n\n\nMedian [Min, Max]\n0.200 [0.100, 0.600]\n1.30 [1.00, 1.80]\n2.00 [1.40, 2.50]\n\n\n\n\n\n\n\n\n\n\n\nIf we want to model Sepal.Length by species, we could create a variable \\(X\\) that represents “setosa” as \\(X=1\\), “virginica” as \\(X=2\\), and “versicolor” as \\(X=3\\).\n\nShow R codedata(iris) # this step is not always necessary, but ensures you're starting  \n# from the original version of a dataset stored in a loaded package\n\niris = \n  iris |&gt; \n  tibble() |&gt;\n  mutate(\n    X = case_when(\n      Species == \"setosa\" ~ 1,\n      Species == \"virginica\" ~ 2,\n      Species == \"versicolor\" ~ 3\n    )\n  )\n\niris |&gt; \n  distinct(Species, X)\n\n\nTable 2.15: iris data with numeric coding of species\n\n\n\n\nSpecies\nX\n\n\n\nsetosa\n1\n\n\nversicolor\n3\n\n\nvirginica\n2\n\n\n\n\n\n\n\n\nThen we could fit a model like:\n\nShow R codeiris_lm1 = lm(Sepal.Length ~ X, data = iris)\niris_lm1 |&gt; parameters() |&gt; print_md()\n\n\nTable 2.16: Model of iris data with numeric coding of Species\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(148)\np\n\n\n\n(Intercept)\n4.91\n0.16\n(4.60, 5.23)\n30.83\n&lt; .001\n\n\nX\n0.47\n0.07\n(0.32, 0.61)\n6.30\n&lt; .001\n\n\n\n\n\n\n\n\n\n2.10.3 Let’s see how that model looks:\n\nShow R codeiris_plot1 = iris |&gt; \n  ggplot(\n    aes(\n      x = X, \n      y = Sepal.Length)\n  ) +\n  geom_point(alpha = .1) +\n  geom_abline(\n    intercept = coef(iris_lm1)[1], \n    slope = coef(iris_lm1)[2]) +\n  theme_bw(base_size = 18)\nprint(iris_plot1)\n\n\n\nFigure 2.20: Model of iris data with numeric coding of Species\n\n\n\n\n\n\n\nWe have forced the model to use a straight line for the three estimated means. Maybe not a good idea?\n\n2.10.4 Let’s see what R does with categorical variables by default:\n\nShow R codeiris_lm2 = lm(Sepal.Length ~ Species, data = iris)\niris_lm2 |&gt; parameters() |&gt; print_md()\n\n\nTable 2.17: Model of iris data with Species as a categorical variable\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(147)\np\n\n\n\n(Intercept)\n5.01\n0.07\n(4.86, 5.15)\n68.76\n&lt; .001\n\n\nSpecies (versicolor)\n0.93\n0.10\n(0.73, 1.13)\n9.03\n&lt; .001\n\n\nSpecies (virginica)\n1.58\n0.10\n(1.38, 1.79)\n15.37\n&lt; .001\n\n\n\n\n\n\n\n\n\n2.10.5 Re-parametrize with no intercept\nIf you don’t want the default and offset option, you can use “-1” like we’ve seen previously:\n\nShow R codeiris.lm2b = lm(Sepal.Length ~ Species - 1, data = iris)\niris.lm2b |&gt; parameters() |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\nParameter\nCoefficient\nSE\n95% CI\nt(147)\np\n\n\n\nSpecies (setosa)\n5.01\n0.07\n(4.86, 5.15)\n68.76\n&lt; .001\n\n\nSpecies (versicolor)\n5.94\n0.07\n(5.79, 6.08)\n81.54\n&lt; .001\n\n\nSpecies (virginica)\n6.59\n0.07\n(6.44, 6.73)\n90.49\n&lt; .001\n\n\n\n\n\n\n2.10.6 Let’s see what these new models look like:\n\nShow R codeiris_plot2 = \n  iris |&gt; \n  mutate(\n    predlm2 = predict(iris_lm2)) |&gt; \n  arrange(X) |&gt; \n  ggplot(aes(x = X, y = Sepal.Length)) +\n  geom_point(alpha = .1) +\n  geom_line(aes(y = predlm2), col = \"red\") +\n  geom_abline(\n    intercept = coef(iris_lm1)[1], \n    slope = coef(iris_lm1)[2]) + \n  theme_bw(base_size = 18)\n\nprint(iris_plot2)\n\n\n\nFigure 2.21\n\n\n\n\n\n\n\n\n2.10.7 Let’s see how R did that:\n\nShow R codeformula(iris_lm2)\n#&gt; Sepal.Length ~ Species\nmodel.matrix(iris_lm2) |&gt; as_tibble() |&gt; unique()\n\n\n\n(Intercept)\nSpeciesversicolor\nSpeciesvirginica\n\n\n\n1\n0\n0\n\n\n1\n1\n0\n\n\n1\n0\n1\n\n\n\n\n\nThis is called a “corner point parametrization”.\n\nShow R codeformula(iris.lm2b)\n#&gt; Sepal.Length ~ Species - 1\nmodel.matrix(iris.lm2b) |&gt; as_tibble() |&gt; unique()\n\n\n\nSpeciessetosa\nSpeciesversicolor\nSpeciesvirginica\n\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n1\n\n\n\n\n\nThis can be called a “group point parametrization”.\nThere are more options; see Dobson and Barnett (2018) §6.4.1 and the codingMatrices package vignette (Venables (2023)).",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear (Gaussian) Models</span>"
    ]
  },
  {
    "objectID": "Linear-models-overview.html#ordinal-covariates",
    "href": "Linear-models-overview.html#ordinal-covariates",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "\n2.11 Ordinal covariates",
    "text": "2.11 Ordinal covariates\n(c.f. Dobson and Barnett (2018) §2.4.4)\n\n\nWe can create ordinal variables in R using the ordered() function4.\n\n\nExample 2.3  \n\nShow R codeurl = paste0(\n  \"https://regression.ucsf.edu/sites/g/files/tkssra6706/\",\n  \"f/wysiwyg/home/data/hersdata.dta\")\nlibrary(haven)\nhers = read_dta(url)\n\n\n\nShow R codehers |&gt; head()\n\n\nTable 2.18: HERS dataset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHT\nage\nraceth\nnonwhite\nsmoking\ndrinkany\nexercise\nphysact\nglobrat\npoorfair\nmedcond\nhtnmeds\nstatins\ndiabetes\ndmpills\ninsulin\nweight\nBMI\nwaist\nWHR\nglucose\nweight1\nBMI1\nwaist1\nWHR1\nglucose1\ntchol\nLDL\nHDL\nTG\ntchol1\nLDL1\nHDL1\nTG1\nSBP\nDBP\nage10\n\n\n\n0\n70\n2\n1\n0\n0\n0\n5\n3\n0\n0\n1\n1\n0\n0\n0\n73.8\n23.69\n96.0\n0.932\n84\n73.6\n23.63\n93.0\n0.912\n94\n189\n122.4\n52\n73\n201\n137.6\n48\n77\n138\n78\n7.0\n\n\n0\n62\n2\n1\n0\n0\n0\n1\n3\n0\n1\n1\n0\n0\n0\n0\n70.9\n28.62\n93.0\n0.964\n111\n73.4\n28.89\n95.0\n0.964\n78\n307\n241.6\n44\n107\n216\n150.6\n48\n87\n118\n70\n6.2\n\n\n1\n69\n1\n0\n0\n0\n0\n3\n3\n0\n0\n1\n0\n1\n0\n0\n102.0\n42.51\n110.2\n0.782\n114\n96.1\n40.73\n103.0\n0.774\n98\n254\n166.2\n57\n154\n254\n156.0\n66\n160\n134\n78\n6.9\n\n\n0\n64\n1\n0\n1\n1\n0\n1\n3\n0\n1\n1\n0\n0\n0\n0\n64.4\n24.39\n87.0\n0.877\n94\n58.6\n22.52\n77.0\n0.802\n93\n204\n116.2\n56\n159\n207\n122.6\n57\n137\n152\n72\n6.4\n\n\n0\n65\n1\n0\n0\n0\n0\n2\n3\n0\n0\n0\n0\n0\n0\n0\n57.9\n21.90\n77.0\n0.794\n101\n58.9\n22.28\n76.5\n0.757\n92\n214\n150.6\n42\n107\n235\n172.2\n35\n139\n175\n95\n6.5\n\n\n1\n68\n2\n1\n0\n1\n0\n3\n3\n0\n0\n0\n0\n0\n0\n0\n60.9\n29.05\n96.0\n1.000\n116\n57.7\n27.52\n86.0\n0.910\n115\n212\n137.8\n52\n111\n202\n126.6\n53\n112\n174\n98\n6.8\n\n\n\n\n\n\n\n\n\n\n\nShow R code\n# C(contr = codingMatrices::contr.diff)\n\n\n\n\n\n\n\n\nAnderson, Edgar. 1935. “The Irises of the Gaspe Peninsula.” Bulletin of American Iris Society 59: 2–5.\n\n\nChatterjee, Samprit, and Ali S Hadi. 2015. Regression Analysis by Example. John Wiley & Sons. https://www.wiley.com/en-us/Regression+Analysis+by+Example%2C+4th+Edition-p-9780470055458.\n\n\nDobson, Annette J, and Adrian G Barnett. 2018. An Introduction to Generalized Linear Models. 4th ed. CRC press. https://doi.org/10.1201/9781315182780.\n\n\nDunn, Peter K, Gordon K Smyth, et al. 2018. Generalized Linear Models with Examples in r. Vol. 53. Springer. https://link.springer.com/book/10.1007/978-1-4419-0118-7.\n\n\nHarrell, Frank E. 2015. Regression Modeling Strategies: With Applications to Linear Models, Logistic Regression, and Survival Analysis. 2nd ed. Springer. https://doi.org/10.1007/978-3-319-19425-7.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. 2013. An Introduction to Statistical Learning. Vol. 112. Springer. https://www.statlearning.com/.\n\n\nKleinbaum, David G, and Mitchel Klein. 2010. Logistic Regression. 3rd ed. Springer. https://link.springer.com/book/10.1007/978-1-4419-1742-3.\n\n\n———. 2012. Survival Analysis a Self-Learning Text. 3rd ed. Springer. https://link.springer.com/book/10.1007/978-1-4419-6646-9.\n\n\nKleinbaum, David G, Lawrence L Kupper, Azhar Nizam, K Muller, and ES Rosenberg. 2014. Applied Regression Analysis and Other Multivariable Methods. 5th ed. Cengage Learning. https://www.cengage.com/c/applied-regression-analysis-and-other-multivariable-methods-5e-kleinbaum/9781285051086/.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, and William Li. 2005. Applied Linear Statistical Models. McGraw-Hill.\n\n\nPolin, Richard A, William W Fox, and Steven H Abman. 2011. Fetal and Neonatal Physiology. 4th ed. Elsevier health sciences.\n\n\nSeber, George AF, and Alan J Lee. 2012. Linear Regression Analysis. 2nd ed. John Wiley & Sons. https://www.wiley.com/en-us/Linear+Regression+Analysis%2C+2nd+Edition-p-9781118274422.\n\n\nVenables, Bill. 2023. codingMatrices: Alternative Factor Coding Matrices for Linear Model Formulae. https://CRAN.R-project.org/package=codingMatrices.\n\n\nVittinghoff, Eric, David V Glidden, Stephen C Shiboski, and Charles E McCulloch. 2012. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. 2nd ed. Springer. https://doi.org/10.1007/978-1-4614-1353-0.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear (Gaussian) Models</span>"
    ]
  },
  {
    "objectID": "Linear-models-overview.html#footnotes",
    "href": "Linear-models-overview.html#footnotes",
    "title": "\n2  Linear (Gaussian) Models\n",
    "section": "",
    "text": "\\(M\\) is implicitly a deterministic function of \\(S\\)↩︎\n\\(F\\) is implicitly a deterministic function of \\(S\\)↩︎\nusing the definite article “the” would mean there is only one slope.↩︎\nor equivalently, factor(ordered = TRUE)↩︎",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear (Gaussian) Models</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html",
    "href": "logistic-regression.html",
    "title": "3  Models for Binary Outcomes",
    "section": "",
    "text": "Configuring R\nFunctions from these packages will be used throughout this document:\nShow R code\nlibrary(conflicted) # check for conflicting function definitions\n# library(printr) # inserts help-file output into markdown output\nlibrary(rmarkdown) # Convert R Markdown documents into a variety of formats.\nlibrary(pander) # format tables for markdown\nlibrary(ggplot2) # graphics\nlibrary(ggeasy) # help with graphics\nlibrary(ggfortify) # help with graphics\nlibrary(dplyr) # manipulate data\nlibrary(tibble) # `tibble`s extend `data.frame`s\nlibrary(magrittr) # `%&gt;%` and other additional piping tools\nlibrary(haven) # import Stata files\nlibrary(knitr) # format R output for markdown\nlibrary(tidyr) # Tools to help to create tidy data\nlibrary(plotly) # interactive graphics\nlibrary(dobson) # datasets from Dobson and Barnett 2018\nlibrary(parameters) # format model output tables for markdown\nlibrary(haven) # import Stata files\nlibrary(latex2exp) # use LaTeX in R code (for figures and tables)\nlibrary(fs) # filesystem path manipulations\nlibrary(survival) # survival analysis\nlibrary(survminer) # survival analysis graphics\nlibrary(KMsurv) # datasets from Klein and Moeschberger\nlibrary(parameters) # format model output tables for\nlibrary(webshot2) # convert interactive content to static for pdf\nlibrary(forcats) # functions for categorical variables (\"factors\")\nlibrary(stringr) # functions for dealing with strings\nlibrary(lubridate) # functions for dealing with dates and times\nHere are some R settings I use in this document:\nShow R code\nrm(list = ls()) # delete any data that's already loaded into R\n\nconflicts_prefer(dplyr::filter)\nggplot2::theme_set(\n  ggplot2::theme_bw() + \n        # ggplot2::labs(col = \"\") +\n    ggplot2::theme(\n      legend.position = \"bottom\",\n      text = ggplot2::element_text(size = 12, family = \"serif\")))\n\nknitr::opts_chunk$set(message = FALSE)\noptions('digits' = 4)\n\npanderOptions(\"big.mark\", \",\")\npander::panderOptions(\"table.emphasize.rownames\", FALSE)\npander::panderOptions(\"table.split.table\", Inf)\nconflicts_prefer(dplyr::filter) # use the `filter()` function from dplyr() by default\nlegend_text_size = 9",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models for Binary Outcomes</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#introduction",
    "href": "logistic-regression.html#introduction",
    "title": "3  Models for Binary Outcomes",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\n\n3.1.1 What is logistic regression?\nLogistic regression is a framework for modeling binary outcomes, conditional on one or more predictors (a.k.a. covariates).\n\n\nExercise 3.1 (Examples of binary outcomes) What are some examples of binary outcomes in the health sciences?\n\n\n\nSolution. Examples of binary outcomes include:\n\nexposure (exposed vs unexposed)\ndisease (diseased vs healthy)\nrecovery (recovered vs unrecovered)\nrelapse (relapse vs remission)\nreturn to hospital (returned vs not)\nvital status (dead vs alive)\n\n\n\nLogistic regression uses the Bernoulli distribution to model the outcome variable, conditional on one or more covariates.\n\n\nExercise 3.2 Write down a mathematical definition of the Bernoulli distribution.\n\n\n\nSolution. The Bernoulli distribution family for a random variable \\(X\\) is defined as:\n\\[\n\\begin{aligned}\n\\Pr(X=x) &= \\mathbb{1}_{x\\in \\left\\{0,1\\right\\}}\\pi^x(1-\\pi)^{1-x}\\\\\n&= \\left\\{{\\pi, x=1}\\atop{1-\\pi, x=0}\\right.\n\\end{aligned}\n\\]\n\n\n\n\n3.1.2 Logistic regression versus linear regression\nLogistic regression differs from linear regression, which uses the Gaussian (“normal”) distribution to model the outcome variable, conditional on the covariates.\n\n\nExercise 3.3 Recall: what kinds of outcomes is linear regression used for?\n\n\n\nSolution. Linear regression is typically used for numerical outcomes that aren’t event counts or waiting times for an event. Examples of outcomes that are often analyzed using linear regression include include weight, height, and income.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models for Binary Outcomes</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#risk-estimation-and-prediction",
    "href": "logistic-regression.html#risk-estimation-and-prediction",
    "title": "3  Models for Binary Outcomes",
    "section": "3.2 Risk Estimation and Prediction",
    "text": "3.2 Risk Estimation and Prediction\n\nIn Epi 203, you have already seen methods for modeling binary outcomes using one covariate that is also binary (such as exposure/non-exposure). In this section, we review one-covariate analyses, with a special focus on risk ratios and odds ratios, which are important concepts for interpreting logistic regression.\n\n\n\nExample 3.1 (Oral Contraceptive Use and Heart Attack)  \n\nResearch question: how does oral contraceptive (OC) use affect the risk of myocardial infarction (MI; a.k.a. heart attack)?\n\n\nThis was an issue when oral contraceptives were first developed, because the original formulations used higher concentrations of hormones. Modern OCs don’t have this issue.\nTable 3.1 contains simulated data for an imaginary follow-up (a.k.a. prospective) study in which two groups are identified, one using OCs and another not using OCs, and both groups are tracked for three years to determine how many in each groups have MIs.\n\n\n\nShow R code\nlibrary(dplyr)\noc_mi = \n  tribble(\n    ~OC, ~MI, ~Total,\n    \"OC use\", 13, 5000,\n    \"No OC use\", 7, 10000\n  ) |&gt; \n  mutate(`No MI` = Total - MI) |&gt; \n  relocate(`No MI`, .after = MI)\n\ntotals = \n  oc_mi |&gt; \n  summarize(across(c(MI, `No MI`, Total), sum)) |&gt; \n  mutate(OC = \"Total\")\n\ntbl_oc_mi = bind_rows(oc_mi, totals)\n\ntbl_oc_mi\n\n\n\n\nTable 3.1: Simulated data from study of oral contraceptive use and heart attack risk\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nExercise 3.4 Review: estimate the probabilities of MI for OC users and non-OC users in Example 3.1.\n\n\n\nSolution. \\[\\hat{\\text{p}}(MI|OC) = \\frac{13}{5000} = 0.0026\\]\n\\[\\hat{\\text{p}}(MI|\\neg OC) = \\frac{7}{10000} = 7\\times 10^{-4}\\]\n\n\n\nControls\n\n\n\n\n\n\nTwo meanings of “controls”\n\n\n\nDepending on context, “controls” can mean either individuals who don’t experience an exposure of interest, or individuals who don’t experience an outcome of interest.\n\n\n\n\nDefinition 3.1 (cases and controls in retrospective studies) In retrospective studies, participants who experience the outcome of interest are called cases, while participants who don’t experience that outcome are called controls.\n\n\n\nDefinition 3.2 (treatment groups and control groups in prospective studies) In prospective studies, the group of participants who experience the treatment or exposure of interest is called the treatment group, while the participants who receive the baseline or comparison treatment (for example, clinical trial participants who receive a placebo or a standard-of-care treatment rather than an experimental treatment) are called controls.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models for Binary Outcomes</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#comparing-probabilities",
    "href": "logistic-regression.html#comparing-probabilities",
    "title": "3  Models for Binary Outcomes",
    "section": "3.3 Comparing Probabilities",
    "text": "3.3 Comparing Probabilities\n\n3.3.1 Risk differences\n\nThe simplest comparison of two probabilities, \\(\\pi_1\\), and \\(\\pi_2\\), is the difference of their values:\n\n\nDefinition 3.3 (Risk difference) The risk difference of two probabilities, \\(\\pi_1\\), and \\(\\pi_2\\), is the difference of their values: \\[\\delta(\\pi_1,\\pi_2) \\stackrel{\\text{def}}{=}\\pi_1 - \\pi_2\\]\n\n\n\nExample 3.2 (Difference in MI risk) In Example 3.1, the maximum likelihood estimate of the difference in MI risk between OC users and OC non-users is:\n\\[\n\\begin{aligned}\n\\hat\\delta(\\pi(OC), \\pi(\\neg OC))\n&= \\delta(\\hat\\pi(OC), \\hat\\pi(\\neg OC))\\\\\n&= \\hat\\pi(OC) - \\hat\\pi(\\neg OC)\\\\\n&= 0.0026 - 7\\times 10^{-4}\\\\\n&= 0.0019\n\\end{aligned}\n\\]\n\n\n\n\n3.3.2 Risk ratios\n\nDefinition 3.4 (Relative risk ratios)  \n\nThe relative risk of probability \\(\\pi_1\\) compared to another probability \\(\\pi_2\\), also called the risk ratio, relative risk ratio, probability ratio, or rate ratio, is the ratio of those probabilities:\n\n\\[\\rho(\\pi_1,\\pi_2) = \\frac{\\pi_1}{\\pi_2}\\]\n\n\n\nExample 3.3  \n\nAbove, we estimated that:\n\n\\[\\hat{\\text{p}}(MI|OC) = 0.0026\\]\n\\[\\hat{\\text{p}}(MI|\\neg OC) = 7\\times 10^{-4}\\]\n\nSo we might estimate that the relative risk of MI for OC versus non-OC is:\n\n\n\nShow R code\nrr = (13/5000)/(7/10000)\n\n\n\\[\n\\begin{aligned}\n\\hat\\rho(OC, \\neg OC)\n&=\\frac{\\hat{\\text{p}}(MI|OC)}{\\hat{\\text{p}}(MI|\\neg OC)}\\\\\n&= \\frac{0.0026}{7\\times 10^{-4}}\\\\\n&= 3.7143\n\\end{aligned}\n\\]\n\nWe might summarize this result by saying that “the estimated probability of MI among OC users was 3.7143 as high as the estimated probability among OC non-users.\n\n\n\n\n\n3.3.3 Relative risk differences\n\nDefinition 3.5 (Relative risk difference)  \n\nSometimes, we divide the risk difference by the comparison probability; the result is called the relative risk difference:\n\n\\[\\xi(\\pi_1,\\pi_2) \\stackrel{\\text{def}}{=}\\frac{\\delta(\\pi_1,\\pi_2)}{\\pi_2}\\]\n\n\n\nTheorem 3.1 (Relative risk difference equals risk ratio minus 1) \\[\\xi(\\pi_1,\\pi_2) = \\rho(\\pi_1,\\pi_2) - 1\\]\n\n\n\nProof. \\[\n\\begin{aligned}\n\\xi(\\pi_1,\\pi_2)\n&\\stackrel{\\text{def}}{=}\\frac{\\delta(\\pi_1,\\pi_2)}{\\pi_2}\n\\\\&= \\frac{\\pi_1-\\pi_2}{\\pi_2}\n\\\\&= \\frac{\\pi_1}{\\pi_2} - 1\n\\\\&= \\rho(\\pi_1,\\pi_2) - 1\n\\end{aligned}\n\\]\n\n\n\n\n3.3.4 Changing reference groups in risk comparisons\n\nRisk differences, risk ratios, and relative risk differences are defined by two probabilities, plus a choice of which probability is the baseline or reference probability (i.e., which probability is the subtrahend of the risk difference or the denominator of the risk ratio).\n\n\\[\\delta(\\pi_2,\\pi_1) = -\\delta(\\pi_1, \\pi_2)\\]\n\\[\\rho(\\pi_2,\\pi_1) = \\left(\\rho(\\pi_1,\\pi_2)\\right)^{-1}\\] \\[\\xi(\\pi_2,\\pi_1) = \\left(\\xi(\\pi_2,\\pi_1) + 1\\right)^{-1} - 1\\]\n\nExercise 3.5 Prove the relationships above.\n\n\n\nExample 3.4 (Switching the reference group in a risk ratio) Above, we estimated that the risk ratio of OC versus non-OC is:\n\\[\n\\begin{aligned}\n\\rho(OC, \\neg OC)\n&= 3.7143\n\\end{aligned}\n\\]\nIn comparison, the risk ratio for non-OC versus OC is:\n\\[\n\\begin{aligned}\n\\rho(\\neg OC, OC)\n&=\\frac{\\hat{\\text{p}}(MI|\\neg OC)}{\\hat{\\text{p}}(MI|OC)}\\\\\n&= \\frac{7\\times 10^{-4}}{0.0026}\\\\\n&= 0.2692\\\\\n&= \\frac{1}{\\rho(OC, \\neg OC)}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models for Binary Outcomes</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#odds-and-odds-ratios",
    "href": "logistic-regression.html#odds-and-odds-ratios",
    "title": "3  Models for Binary Outcomes",
    "section": "3.4 Odds and Odds Ratios",
    "text": "3.4 Odds and Odds Ratios\n\n3.4.1 Odds and probabilities\n\nIn logistic regression, we will make use of a mathematically-convenient transformation of probability, called odds.\n\n\nDefinition 3.6 (Odds) The odds of an outcome \\(A\\), which we will represent using \\(\\omega\\) (“omega”), is the probability that the outcome occurs, divided by the probability that it doesn’t occur:\n\\[\\omega(A) \\stackrel{\\text{def}}{=}\\frac{\\Pr(A)}{\\Pr(\\neg A)}\\]\n\n\n\nTheorem 3.2 If the probability of an outcome \\(A\\) is \\(\\Pr(A)=\\pi\\), then the corresponding odds of \\(A\\) is:\n\\[\\text{odds}\\left\\{\\pi\\right\\} = \\frac{\\pi}{1-\\pi} \\tag{3.1}\\]\n\n\n\nProof. \\[\n\\begin{aligned}\n\\Pr(\\neg A) &= 1 - \\Pr(A)\n\\\\ &= 1 - \\pi\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\therefore\\omega(A) &\\stackrel{\\text{def}}{=}\\frac{\\Pr(A)}{\\Pr(\\neg A)}\n\\\\ &= \\frac{\\pi}{1-\\pi}\n\\end{aligned}\n\\]\n\n\n\nFunction 3.1, which transforms probabilities into odds, can be called the odds function. Figure 3.1 graphs the shape of this function.\n\n\n\nShow R code\nodds = function(pi) pi / (1 - pi)\nlibrary(ggplot2)\nggplot() +\n  geom_function(\n    fun = odds,\n    arrow = arrow(ends = \"last\"),\n    mapping = aes(col = \"odds function\")\n  ) +\n  xlim(0, .99) +\n  xlab(\"Probability\") +\n  ylab(\"Odds\") +\n  geom_abline(aes(\n    intercept = 0,\n    slope = 1,\n    col = \"y=x\"\n  )) +\n  theme_bw() +\n  labs(colour = \"\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nFigure 3.1: Odds versus probability\n\n\n\n\n\n\n\n\n\n\nExample 3.5 (Computing odds from probabilities) In Exercise 3.4, we estimated that the probability of MI, given OC use, is \\(\\pi(OC) \\stackrel{\\text{def}}{=}\\Pr(MI|OC) = 0.0026\\). If this estimate is correct, then the odds of MI, given OC use, is:\n\\[\n\\begin{aligned}\n\\omega(OC)\n&\\stackrel{\\text{def}}{=}\\frac{\\Pr(MI|OC)}{\\Pr(\\neg MI|OC)}\\\\\n&=\\frac{\\Pr(MI|OC)}{1-\\Pr(MI|OC)}\\\\\n&=\\frac{\\pi(OC)}{1-\\pi(OC)}\\\\\n&=\\frac{0.0026}{1-0.0026}\\\\\n&\\approx 0.002607\n\\end{aligned}\n\\]\n\n\n\nExercise 3.6 (Computing odds from probabilities) Estimate the odds of MI, for non-OC users.\n\nSolution. \\[\n\\omega(\\neg OC) = 7.0049\\times 10^{-4}\n\\]\n\n\n\n\nTheorem 3.3 (One-sample MLE for odds) Let \\(X_1,...X_n\\) be a set of \\(n\\) \\(\\text{iid}\\) Bernoulli trials, and let \\(X = \\sum_{i=1}^nX_i\\) be their sum.\nThen the maximum likelihood estimate of the odds of \\(X=1\\), \\(\\omega\\), is:\n\\[\n\\hat{\\omega}= \\frac{x}{n-x}\n\\]\n\n\n\nProof. \\[\n\\begin{aligned}\n1-\\hat\\pi\n&= 1-\\frac{x}{n}\\\\\n&= \\frac{n}{n} - \\frac{x}{n}\\\\\n&= \\frac{n - x}{n}\n\\end{aligned}\n\\]\nThus, the estimated odds is:\n\\[\n\\begin{aligned}\n\\frac{\\hat\\pi}{1-\\hat\\pi}\n&= \\frac{\\left(\\frac{x}{n}\\right)}{\\left(\\frac{n-x}{n}\\right)}\\\\\n&= \\frac{x}{n-x}\n\\end{aligned}\n\\]\n\nThat is, odds can be calculated directly as “# events” divided by “# nonevents” (without needing to calculate \\(\\hat\\pi\\) and \\(1-\\hat\\pi\\) first).\n\n\n\n\nExample 3.6 (calculating odds using the shortcut) In Example 3.5, we calculated \\[\n\\begin{aligned}\n\\omega(OC)\n&=0.0026\n\\end{aligned}\n\\]\nLet’s recalculate this result using our shortcut.\n\n\n\nSolution 3.1. \\[\n\\begin{aligned}\n\\omega(OC)\n&=\\frac{13}{5000-13}\\\\\n&=0.0026\n\\end{aligned}\n\\]\nSame answer as in Example 3.5!\n\n\n\nTheorem 3.4 (Simplified expression for odds function)  \n\nAn equivalent expression for the odds function is\n\n\\[\n\\text{odds}\\left\\{\\pi\\right\\} = \\left(\\left(\\pi\\right)^{-1}-1)\\right)^{-1}\n\\tag{3.2}\\]\n\n\n\nExercise 3.7 Prove that Equation 3.2 is equivalent to Definition 3.6.\n\n\n\nTheorem 3.5 (Derivative of odds function) \\[{\\text{odds}}'\\left\\{\\pi\\right\\} = \\frac{1}{\\left(1-\\pi\\right)^2}\\]\n\n\n\nProof. \\[\n\\begin{aligned}\n{\\text{odds}}'\\left\\{\\pi\\right\\}\n   &= \\frac{\\partial}{\\partial \\pi}\\left(\\frac{\\pi}{1-\\pi}\\right)\n\\\\ &=  \\frac {\\frac{\\partial}{\\partial \\pi}\\pi} {1-\\pi} -\n       \\left(\\frac{\\pi}{\\left(1-\\pi\\right)^2} \\cdot\\frac{\\partial}{\\partial \\pi}\\left(1-\\pi\\right)\\right)\n\\\\ &=  \\frac{1}{1-\\pi} - \\frac{\\pi}{\\left(1-\\pi\\right)^2} \\cdot(-1)\n\\\\ &=  \\frac{1}{1-\\pi} + \\frac{\\pi}{\\left(1-\\pi\\right)^2}\n\\\\ &=  \\frac{1-\\pi}{\\left(1-\\pi\\right)^2} + \\frac{\\pi}{\\left(1-\\pi\\right)^2}\n\\\\ &=  \\frac{1-\\pi + \\pi}{\\left(1-\\pi\\right)^2}\n\\\\ &=  \\frac{1}{\\left(1-\\pi\\right)^2}\n\\end{aligned}\n\\]\n\n\n\nOdds of rare events\nFor rare events (small \\(\\pi\\)), odds and probabilities are nearly equal, because \\(1-\\pi \\approx 1\\) (see Figure 3.1).\nFor example, in Example 3.5, the probability and odds differ by \\(6.7776\\times 10^{-6}\\).\n\n\nExercise 3.8 What odds value corresponds to the probability \\(\\pi = 0.2\\), and what is the numerical difference between these two values?\n\n\n\nSolution. \\[\n\\omega= \\frac{\\pi}{1-\\pi}\n=\\frac{.2}{.8}\n= .25\n\\]\n\n\n\nTheorem 3.6 Let \\(\\omega= \\frac{\\pi}{1-\\pi}\\). Then:\n\\[\\omega- \\pi =  \\omega\\cdot\\pi\\]\n\n\n\nExercise 3.9 Prove Theorem 3.6.\n\n\n\nSolution. \\[\n\\begin{aligned}\n\\omega- \\pi\n&= \\frac{\\pi}{1-\\pi} - \\pi\n\\\\ &= \\frac{\\pi}{1-\\pi} - \\frac{\\pi(1-\\pi)}{1-\\pi}\n\\\\ &= \\frac{\\pi}{1-\\pi} - \\frac{\\pi - \\pi^2}{1-\\pi}\n\\\\ &= \\frac{\\pi - (\\pi - \\pi^2)}{1-\\pi}\n\\\\ &= \\frac{\\pi - \\pi + \\pi^2}{1-\\pi}\n\\\\ &= \\frac{\\pi^2}{1-\\pi}\n\\\\ &= \\frac{\\pi}{1-\\pi} \\pi\n\\\\ &= \\omega\\pi\n\\end{aligned}\n\\]\n\n\n\nLemma 3.1 (Odds of a non-event) If \\(\\pi\\) is the odds of event \\(A\\) and \\(\\omega\\) is the corresponding odds of \\(A\\), then the odds of \\(\\neg A\\) are:\n\\[\n\\omega(\\neg A) = \\frac{1-\\pi}{\\pi}\n\\]\n\n\nProof. Left to the reader.\n\n\n\n\n\n3.4.2 The inverse odds function\n\nDefinition 3.7 (inverse odds function) The inverse odds function,\n\\[\\text{invodds}\\left\\{\\omega\\right\\} \\stackrel{\\text{def}}{=}\\frac{\\omega}{1 + \\omega}\\] converts odds into their corresponding probabilities (Figure 3.2).\n\n\nThe inverse-odds function takes an odds as input and produces a probability as output. Its domain of inputs is \\([0,\\infty)\\) and its range of outputs is \\([0,1]\\).\nI haven’t seen anyone give the inverse-odds function a concise name; maybe \\(\\text{prob}()\\)?\n\n\n\n\nShow R code\nodds_inv = function(omega) (1 + omega^-1)^-1\nggplot() +\n  geom_function(fun = odds_inv, aes(col = \"inverse-odds\")) +\n  xlab(\"Odds\") +\n  ylab(\"Probability\") +\n  xlim(0,5) +\n  ylim(0,1) +\n  geom_abline(aes(intercept = 0, slope = 1, col = \"x=y\"))\n\n\n\n\n\nFigure 3.2: The inverse odds function, \\(\\text{invodds}\\left\\{\\omega\\right\\}\\)\n\n\n\n\n\n\n\n\n\n\nExercise 3.10 What probability corresponds to an odds of \\(\\omega= 1\\), and what is the numerical difference between these two values?\n\n\n\nSolution. \\[\n\\pi(1) = \\frac{1}{1+1}\n=\\frac{1}{2}\n= .5\n\\] \\[\n1 - \\pi(1) = 1 - .5 = .5\n\\]\n\n\n\nLemma 3.2 (Simplified expression for inverse odds function)  \n\nAn equivalent expression for the inverse odds function is\n\n\\[\n\\pi(\\omega) = (1+\\omega^{-1})^{-1}\n\\tag{3.3}\\]\n\n\n\nExercise 3.11 Prove that Equation 3.3 is equivalent to Definition 3.7.\n\n\n\nLemma 3.3 (One minus inverse-odds) \\[1 - \\text{invodds}\\left\\{\\omega\\right\\} = \\frac{1}{1+\\omega}\\]\n\n\n\nProof. \\[\n\\begin{aligned}\n1 - \\text{invodds}\\left\\{\\omega\\right\\} &= 1 - \\frac{\\omega}{1 + \\omega}\n\\\\ &= \\frac{{\\color{red}1+\\omega}}{1 + \\omega} - \\frac{{\\color{blue}\\omega}}{1 + \\omega}\n\\\\ &= \\frac{{\\color{red}(1+\\omega)} - {\\color{blue}\\omega}}{1 + \\omega}\n\\\\ &= \\frac{1 + \\omega- \\omega}{1 + \\omega}\n\\\\ &= \\frac{1}{1 + \\omega}\n\\end{aligned}\n\\]\n\n\n\nTheorem 3.7 If \\(\\omega\\) is the odds of event \\(A\\), then the probability that \\(A\\) does not occur is:\n\\[\\Pr(\\neg A) = \\frac{1}{1+\\omega}\\]\n\n\n\nProof. \n\nUse Lemma 3.3:\n\n\\[\n\\begin{aligned}\n\\Pr(\\neg A)\n&= 1 - \\Pr(A)\n\\\\ &= 1 - \\text{invodds}\\left\\{\\omega\\right\\}\n\\\\ &= \\frac{1}{1 + \\omega}\n\\end{aligned}\n\\]\n\n\n\nTheorem 3.8 (Derivative of inverse odds function) \\[{\\text{invodds}}'\\left\\{\\omega\\right\\} = \\frac{1}{\\left(1+\\omega\\right)^2}\\]\n\n\n\nProof. \n\nUse the quotient rule:\n\n\\[\n\\begin{aligned}\n{\\text{invodds}}'(\\omega)\n&= \\frac{\\partial}{\\partial \\omega} \\text{invodds}\\left\\{\\omega\\right\\}\n\\\\ &= \\frac{\\partial}{\\partial \\omega} \\frac{\\omega}{1+\\omega}\n\\\\ &= \\frac{\\frac{\\partial}{\\partial \\omega}\\omega}{1+\\omega} - \\frac{\\omega}{\\left(1+\\omega\\right)^2} \\cdot\\frac{\\partial}{\\partial \\omega}(1+\\omega)\n\\\\ &= \\frac{1}{1+\\omega} - \\frac{\\omega}{\\left(1+\\omega\\right)^2} \\cdot 1\n\\\\ &= \\frac{1}{1+\\omega} - \\frac{\\omega}{\\left(1+\\omega\\right)^2}\n\\\\ &= \\frac{1+\\omega}{\\left(1+\\omega\\right)^2} - \\frac{\\omega}{\\left(1+\\omega\\right)^2}\n\\\\ &= \\frac{1+\\omega- \\omega}{\\left(1+\\omega\\right)^2}\n\\\\ &= \\frac{1}{\\left(1+\\omega\\right)^2}\n\\end{aligned}\n\\]\n\n\n\nCorollary 3.1 \\[{\\text{invodds}}'\\left\\{\\omega\\right\\} = \\left(1 - \\text{invodds}\\left\\{\\omega\\right\\}\\right)^2\\]\n\n\n\n\n3.4.3 Odds ratios\n\nNow that we have defined odds, we can introduce another way of comparing event probabilities: odds ratios.\n\n\nDefinition 3.8 (Odds ratio) The odds ratio for two odds \\(\\omega_1\\), \\(\\omega_2\\) is their ratio:\n\\[\\theta(\\omega_1, \\omega_2) = \\frac{\\omega_1}{\\omega_2}\\]\n\n\n\nExample 3.7 (Calculating odds ratios) In Example 3.1, the odds ratio for OC users versus OC-non-users is:\n\\[\n\\begin{aligned}\n\\theta(\\omega(OC), \\omega(\\neg OC))\n&= \\frac{\\omega(OC)}{\\omega(\\neg OC)}\\\\\n&= \\frac{0.0026}{7\\times 10^{-4}}\\\\\n&= 3.7143\\\\\n\\end{aligned}\n\\]\n\n\n\nA shortcut for calculating odds ratio estimates\n\nThe general form of a two-by-two table is shown in Table 3.2.\n\n\n\nTable 3.2: A generic 2x2 table\n\n\n\n\n\n\nEvent\nNon-Event\nTotal\n\n\n\n\nExposed\na\nb\na+b\n\n\nNon-exposed\nc\nd\nc+d\n\n\nTotal\na+c\nb+d\na+b+c+d\n\n\n\n\n\n\n\n\nFrom this table, we have:\n\n\n\\(\\hat\\pi(Event|Exposed) = a/(a+b)\\)\n\\(\\hat\\pi(\\neg Event|Exposed) = b/(a+b)\\)\n\\(\\hat\\omega(Event|Exposed) = \\frac{\\left(\\frac{a}{a+b}\\right)}{\\left(\\frac{b}{a+b}\\right)}=\\frac{a}{b}\\)\n\\(\\hat\\omega(Event|\\neg Exposed) = \\frac{c}{d}\\) (see Exercise 3.12)\n\\(\\theta(Exposed,\\neg Exposed) = \\frac{\\frac{a}{b}}{\\frac{c}{d}} = \\frac{ad}{bc}\\)\n\n\n\nExercise 3.12 Given Table 3.2, show that \\(\\hat\\omega(Event|\\neg Exposed) = \\frac{c}{d}\\).\n\n\n\n\nProperties of odds ratios\n\nOdds ratios have a special property: we can swap a covariate with the outcome, and the odds ratio remains the same.\n\n\nTheorem 3.9 (Odds ratios are reversible) For any two events \\(A\\), \\(B\\):\n\\[\\theta(A|B) = \\theta(B|A)\\]\n\n\n\nProof. \n\\[\n\\begin{aligned}\n\\theta(A|B) &\\stackrel{\\text{def}}{=}\\frac{\\omega(A|B)}{\\omega(A|\\neg B)}\n\\\\ &= \\frac\n{\\left(\\frac{\\text{p}(A|B)}{\\text{p}(\\neg A|B)}\\right)}\n{\\left(\\frac{\\text{p}(A|\\neg B)}{\\text{p}(\\neg A| \\neg B)}\\right)}\n\\\\ &=\n\\left(\\frac{\\text{p}(A|B)}{\\text{p}(\\neg A|B)}\\right)\n\\left(\\frac{\\text{p}(A|\\neg B)}{\\text{p}(\\neg A| \\neg B)}\\right)^{-1}\n\\\\ &=\n\\left(\\frac{\\text{p}(A|B)}{\\text{p}(\\neg A|B)}\\right)\n\\left(\\frac{\\text{p}(\\neg A| \\neg B)}{\\text{p}(A|\\neg B)}\\right)\n\\\\ &=\n\\left(\\frac{\\text{p}(A|B)}{\\text{p}(\\neg A|B)} \\cdot \\frac{\\text{p}(B)}{\\text{p}(B)}\\right)\n\\left(\\frac{\\text{p}(\\neg A| \\neg B)}{\\text{p}(A|\\neg B)} \\cdot \\frac{\\text{p}(\\neg B)}{\\text{p}(\\neg B)}\\right)\n\\\\ &=\n\\left(\\frac{\\text{p}(A,B)}{\\text{p}(\\neg A,B)}\\right)\n\\left(\\frac{\\text{p}(\\neg A, \\neg B)}{\\text{p}(A, \\neg B)}\\right)\n\\\\ &=\n\\left(\\frac{\\text{p}(B,A)}{{\\color{red}\\text{p}(B,\\neg A)}}\\right)\n\\left(\\frac{\\text{p}(\\neg B, \\neg A)}{{\\color{blue}\\text{p}(\\neg B, A)}}\\right)\n\\\\ &=\n\\left(\\frac{\\text{p}(B,A)}{{\\color{blue}\\text{p}(\\neg B, A)}}\\right)\n\\left(\\frac{\\text{p}(\\neg B, \\neg A)}{{\\color{red}\\text{p}(B,\\neg A)}}\\right)\n\\\\ &= \\text{[reverse the preceding steps]}\n\\\\ &= \\theta(B|A)\n\\end{aligned}\n\\]\n\n\n\nExample 3.8 In Example 3.1, we have:\n\\[\n\\begin{aligned}\n\\theta(MI; OC)\n&\\stackrel{\\text{def}}{=}\n\\frac{\\omega(MI|OC)}{\\omega(MI|\\neg OC)}\\\\\n&\\stackrel{\\text{def}}{=}\\frac\n{\\left(\\frac{\\Pr(MI|OC)}{\\Pr(\\neg MI|OC)}\\right)}\n{\\left(\\frac{\\Pr(MI|\\neg OC)}{\\Pr(\\neg MI|\\neg OC)}\\right)}\\\\\n&= \\frac\n{\\left(\\frac{\\Pr(MI,OC)}{\\Pr(\\neg MI,OC)}\\right)}\n{\\left(\\frac{\\Pr(MI,\\neg OC)}{\\Pr(\\neg MI,\\neg OC)}\\right)}\\\\\n&= \\left(\\frac{\\Pr(MI,OC)}{\\Pr(\\neg MI,OC)}\\right)\n\\left(\\frac{\\Pr(\\neg MI,\\neg OC)}{\\Pr(MI,\\neg OC)}\\right)\\\\\n&= \\left(\\frac{\\Pr(MI,OC)}{\\Pr(MI,\\neg OC)}\\right)\n\\left(\\frac{\\Pr(\\neg MI,\\neg OC)}{\\Pr(\\neg MI,OC)}\\right)\\\\\n&= \\left(\\frac{\\Pr(OC,MI)}{\\Pr(\\neg OC,MI)}\\right)\n\\left(\\frac{\\Pr(\\neg OC,\\neg MI)}{\\Pr(OC,\\neg MI)}\\right)\\\\\n&= \\left(\\frac{\\Pr(OC|MI)}{\\Pr(\\neg OC|MI)}\\right)\n\\left(\\frac{\\Pr(\\neg OC|\\neg MI)}{\\Pr(OC|\\neg MI)}\\right)\\\\\n&= \\frac{\\left(\\frac{\\Pr(OC|MI)}{\\Pr(\\neg OC|MI)}\\right)}\n{\\left(\\frac{\\Pr(OC|\\neg MI)}{\\Pr(\\neg OC|\\neg MI)}\\right)}\\\\\n&\\stackrel{\\text{def}}{=}\\frac{\\omega(OC|MI)}\n{\\omega(OC|\\neg MI)}\\\\\n&\\stackrel{\\text{def}}{=}\\theta(OC; MI)\n\\end{aligned}\n\\]\n\n\n\nExercise 3.13 For Table 3.2, show that \\(\\hat\\theta(Exposed, Unexposed) = \\hat\\theta(Event, \\neg Event)\\).\n\n\n\nConditional odds ratios have the same reversibility property:\n\n\nTheorem 3.10 (Conditional odds ratios are reversible) For any three events \\(A\\), \\(B\\), \\(C\\):\n\\[\\theta(A|B,C) = \\theta(B|A,C)\\]\n\n\n\nProof. Apply the same steps as for Theorem 3.9, inserting \\(C\\) into the conditions (RHS of \\(|\\)) of every expression.\n\n\n\n\nOdds Ratios vs Probability (Risk) Ratios\n\nWhen the outcome is rare (i.e., its probability is small) for both groups being compared in an odds ratio, the odds of the outcome will be similar to the probability of the outcome, and thus the risk ratio will be similar to the odds ratio.\n\n\nCase 1: rare events\nFor rare events, odds ratios and probability (a.k.a. risk, a.k.a. prevalence) ratios will be close:\n\\(\\pi_1 = .01\\) \\(\\pi_2 = .02\\)\n\n\nShow R code\npi1 = .01\npi2 = .02\npi2/pi1\n#&gt; [1] 2\nodds(pi2)/odds(pi1)\n#&gt; [1] 2.02\n\n\n\n\nExample 3.9 In Example 3.1, the outcome is rare for both OC and non-OC participants, so the odds for both groups are similar to the corresponding probabilities, and the odds ratio is similar the risk ratio.\n\n\n\n\nCase 2: frequent events\n\\(\\pi_1 = .4\\) \\(\\pi_2 = .5\\)\nFor more frequently-occurring outcomes, this won’t be the case:\n\n\nShow R code\npi1 = .4\npi2 = .5\npi2/pi1\n#&gt; [1] 1.25\nodds(pi2)/odds(pi1)\n#&gt; [1] 1.5\n\n\n\n\n\n\nOdds Ratios in Case-Control Studies\n\nTable 3.1 simulates a follow-up study in which two populations were followed and the number of MI’s was observed. The risks are \\(P(MI|OC)\\) and \\(P(MI|\\neg OC)\\) and we can estimate these risks from the data.\nBut suppose we had a case-control study in which we had 100 women with MI and selected a comparison group of 100 women without MI (matched as groups on age, etc.). Then MI is not random, and we cannot compute P(MI|OC) and we cannot compute the risk ratio. However, the odds ratio however can be computed.\nThe disease odds ratio is the odds for the disease in the exposed group divided by the odds for the disease in the unexposed group, and we cannot validly compute and use these separate parts.\nWe can still validly compute and use the exposure odds ratio, which is the odds for exposure in the disease group divided by the odds for exposure in the non-diseased group (because exposure can be treated as random):\n\n\\[\n\\hat\\theta(OC|MI) =\n\\frac{\\hat{\\omega}(OC|MI)}{\\hat{\\omega}(OC|\\neg MI)}\n\\]\n\nAnd these two odds ratios, \\(\\hat\\theta(MI|OC)\\) and \\(\\hat\\theta(OC|MI)\\), are mathematically equivalent, as we saw in Section 3.4.3.2:\n\n\\[\\hat\\theta(MI|OC) = \\hat\\theta(OC|MI)\\]\n\n\nExercise 3.14 Calculate the odds ratio of MI with respect to OC use, assuming that Table 3.1 comes from a case-control study. Confirm that the result is the same as in Example 3.7.\n\n\nSolution. \n\n\\(\\omega(OC|MI) = P(OC|MI)/(1 – P(OC|MI) = \\frac{13}{7} = 1.8571\\)\n\\(\\omega(OC|\\neg MI) = P(OC|\\neg MI)/(1 – P(OC|\\neg MI) = \\frac{4987}{9993} = 0.499\\)\n\\(\\theta(OC,MI) = \\frac{\\omega(OC|MI)}{\\omega(OC|\\neg MI)} = \\frac{13/7}{4987/9993} = 3.7214\\)\n\n\nThis is the same estimate we calculated in Example 3.7.\n\n\n\n\n\n\nOdds Ratios in Cross-Sectional Studies\n\nIf a cross-sectional study is a probability sample of a population (which it rarely is) then we can estimate risks.\nIf it is a sample, but not an unbiased probability sample, then we need to treat it in the same way as a case-control study.\nWe can validly estimate odds ratios in either case.\nBut we can usually not validly estimate risks and risk ratios.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models for Binary Outcomes</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#the-logit-and-expit-functions",
    "href": "logistic-regression.html#the-logit-and-expit-functions",
    "title": "3  Models for Binary Outcomes",
    "section": "3.5 The logit and expit functions",
    "text": "3.5 The logit and expit functions\n\n3.5.1 The logit function\n\nDefinition 3.9 (logit function)  \n\nThe logit function of a probability \\(\\pi\\) is the natural logarithm of the odds function of \\(\\pi\\):\n\n\\[\\text{logit}(\\pi) \\stackrel{\\text{def}}{=}\\text{log}\\left\\{\\omega(\\pi)\\right\\}\\]\n\n\n\nFigure 3.3 shows the shape of the \\(\\text{logit}()\\) function.\n\n\n\nShow R code\nlogit = function(p) log(odds(p))\n\nlogit_plot = \n  ggplot() + \n  geom_function(\n    fun = logit,\n    arrow = arrow(ends = \"both\")) + \n  xlim(.001, .999) + \n  ylab(\"logit(p)\") +\n  xlab(\"p\") +\n  theme_bw()\nprint(logit_plot)\n\n\n\n\n\nFigure 3.3: The logit function\n\n\n\n\n\n\n\n\n\n\nTheorem 3.11 \\[\\text{logit}(\\pi) = \\text{log}\\left\\{\\frac{\\pi}{1-\\pi}\\right\\} \\tag{3.4}\\]\n\n\n\nExercise 3.15 (Compose the logit function) Prove Theorem 3.11.\n\n\n\nProof. Apply Definition 3.9 and then Definition 3.6 (details left to the reader).\n\n\n\nTheorem 3.12 (Derivative of logit function) \\[\\text{logit}'(\\pi) = \\frac{1}{(\\pi)(1-\\pi)}\\]\n\n\n\nProof. \\[\n\\begin{aligned}\n\\text{logit}'(\\pi)\n   &= \\frac{\\partial}{\\partial \\pi}\\text{logit}(\\pi)\n\\\\ &= \\frac{\\partial}{\\partial \\pi}\\text{log}\\left\\{\\omega(\\pi)\\right\\}\n\\\\ &= \\frac{\\omega'(\\pi)}{\\omega(\\pi)}\n\\\\ &= \\omega'(\\pi) \\frac{1}{\\omega(\\pi)}\n\\\\ &= \\frac{1}{\\left(1-\\pi\\right)^2}\\frac{1-\\pi}{\\pi}\n\\\\ &= \\frac{1}{(\\pi)(1-\\pi)}\n\\end{aligned}\n\\]\n\n\n\n\n3.5.2 The expit function\n\nDefinition 3.10 (expit, logistic, inverse-logit) The expit function (Figure 3.4) of a log-odds \\(\\eta\\), also known as the inverse-logit function or logistic function, is the inverse-odds of the exponential of \\(\\eta\\):\n\\[\\text{expit}(\\eta) \\stackrel{\\text{def}}{=}\\text{invodds}\\left\\{\\text{exp}\\left\\{\\eta\\right\\}\\right\\}\\]\n\n\n\nShow R code\nexpit = function(eta)\n  exp(eta) / (1 + exp(eta))\nlibrary(ggplot2)\nexpit_plot =\n  ggplot() +\n  geom_function(fun = expit,\n                arrow = arrow(ends = \"both\")) +\n  xlim(-8, 8) +\n  ylim(0, 1) +\n  ylab(expression(expit(eta))) +\n  xlab(expression(eta)) +\n  theme_bw()\nprint(expit_plot)\n\n\n\n\n\nFigure 3.4: The expit function\n\n\n\n\n\n\n\n\n\n\nTheorem 3.13 (logit and expit are each others’ inverses) \\[\\text{logit}\\left\\{\\text{expit}\\left\\{\\eta\\right\\}\\right\\} = \\eta\\]\n\\[\\text{expit}\\left\\{\\text{logit}\\left\\{\\pi\\right\\}\\right\\} = \\pi\\]\n\n\n\nProof. Left to the reader.\n\n\n\nTheorem 3.14 (Expressions for expit function) \\[\n\\begin{aligned}\n\\text{expit}(\\eta)\n   &= \\frac{\\text{exp}\\left\\{\\eta\\right\\}}{1+\\text{exp}\\left\\{\\eta\\right\\}}\n\\\\ &= (1 + \\text{exp}\\left\\{-\\eta\\right\\})^{-1}\n\\end{aligned}\n\\]\n\n\n\nProof. Apply definitions and Lemma 3.2. Details left to the reader.\n\n\n\nLemma 3.4 \\[1-\\text{expit}\\left\\{\\eta\\right\\} = \\left(1+\\text{exp}\\left\\{\\eta\\right\\}\\right)^{-1}\\]\n\n\n\nProof. Using Lemma 3.3:\n\\[\n\\begin{aligned}\n1 - \\text{expit}\\left\\{\\eta\\right\\} &= 1 - \\text{invodds}\\left\\{\\text{exp}\\left\\{\\eta\\right\\}\\right\\}\n\\\\ &= \\frac{1}{1+\\text{exp}\\left\\{\\eta\\right\\}}\n\\\\ &= \\left(1 + \\text{exp}\\left\\{\\eta\\right\\}\\right)^{-1}\n\\end{aligned}\n\\]\n\n\n\nLemma 3.5 \\[\\text{expit}'\\left\\{\\eta\\right\\} = (\\text{expit}\\left\\{\\eta\\right\\}) (1 - \\text{expit}\\left\\{\\eta\\right\\})\\]\n\n\n\nProof. Using Theorem 3.8:\n\\[\n\\begin{aligned}\n\\text{expit}'\\left\\{\\eta\\right\\}\n   &= \\frac{\\partial}{\\partial \\eta} \\text{expit}\\left\\{\\eta\\right\\}   \n\\\\ &= \\frac{\\partial}{\\partial \\eta} \\text{invodds}\\left\\{\\text{exp}\\left\\{\\eta\\right\\}\\right\\}\n\\\\ &= {\\text{invodds}}'\\left\\{\\text{exp}\\left\\{\\eta\\right\\}\\right\\}  \\frac{\\partial}{\\partial \\eta}\\text{exp}\\left\\{\\eta\\right\\}\n\\\\ &=  \\frac{1}{\\left(1 + \\text{exp}\\left\\{\\eta\\right\\}\\right)^2} \\text{exp}\\left\\{\\eta\\right\\}\n\\\\ &=  \\frac{\\text{exp}\\left\\{\\eta\\right\\}}{\\left(1 + \\text{exp}\\left\\{\\eta\\right\\}\\right)^2}\n\\\\ &=  \\frac{\\text{exp}\\left\\{\\eta\\right\\}}{1 + \\text{exp}\\left\\{\\eta\\right\\}} \\frac{1}{1 + \\text{exp}\\left\\{\\eta\\right\\}}\n\\\\ &=  \\text{expit}\\left\\{\\eta\\right\\} (1-\\text{expit}\\left\\{\\eta\\right\\})\n\\end{aligned}\n\\]\n\n\n\nProof. Alternatively, we can use Theorem 3.14:\n\\[\n\\begin{aligned}\n\\text{expit}'\\left\\{\\eta\\right\\}\n   &= \\frac{\\partial}{\\partial \\eta} \\text{expit}\\left\\{\\eta\\right\\}\n\\\\ &= \\frac{\\partial}{\\partial \\eta} (1 + \\text{exp}\\left\\{-\\eta\\right\\})^{-1}\n\\\\ &= -(1 + \\text{exp}\\left\\{-\\eta\\right\\})^{-2} \\frac{\\partial}{\\partial \\eta} (1 + \\text{exp}\\left\\{-\\eta\\right\\})\n\\\\ &= -(1 + \\text{exp}\\left\\{-\\eta\\right\\})^{-2} (-\\text{exp}\\left\\{-\\eta\\right\\})\n\\\\ &= (1 + \\text{exp}\\left\\{-\\eta\\right\\})^{-2} (\\text{exp}\\left\\{-\\eta\\right\\})\n\\\\ &= (1 + \\text{exp}\\left\\{-\\eta\\right\\})^{-1} \\frac{\\text{exp}\\left\\{-\\eta\\right\\}}{1 + \\text{exp}\\left\\{-\\eta\\right\\}}\n\\\\ &= (1 + \\text{exp}\\left\\{-\\eta\\right\\})^{-1} \\frac{1}{1 + \\text{exp}\\left\\{\\eta\\right\\}}\n\\\\ &= \\text{expit}\\left\\{\\eta\\right\\} (1-\\text{expit}\\left\\{\\eta\\right\\})\n\\end{aligned}\n\\]\n\n\n\n3.5.3 Diagram of expit and logit\n\\[\n\\underbrace{\\pi}_{\\atop{\\Pr(Y=1)} }\n\\overbrace{\n\\underbrace{\n\\underset{\n\\xleftarrow[\\frac{\\omega}{1+\\omega}]{}\n}\n{\n\\xrightarrow{\\frac{\\pi}{1-\\pi}}\n}\n\\underbrace{\\omega}_{\\text{odds}(Y=1)}\n\\underset{\n\\xleftarrow[\\text{exp}\\left\\{\\eta\\right\\}]{}\n}\n{\n\\xrightarrow{\\text{log}\\left\\{\\omega\\right\\}}\n}\n}_{\\text{expit}(\\eta)}\n}^{\\text{logit}(\\pi)}\n\\underbrace{\\eta}_{\\atop{\\text{log-odds}(Y=1)}}\n\\]",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models for Binary Outcomes</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#introduction-to-logistic-regression",
    "href": "logistic-regression.html#introduction-to-logistic-regression",
    "title": "3  Models for Binary Outcomes",
    "section": "3.6 Introduction to logistic regression",
    "text": "3.6 Introduction to logistic regression\n\n\nIn Example 3.1, we estimated the risk and the odds of MI for two groups, defined by oral contraceptive use.\nIf the predictor is quantitative (dose) or there is more than one predictor, the task becomes more difficult.\nIn this case, we will use logistic regression, which is a generalization of the linear regression models you have been using that can account for a binary response instead of a continuous one.\n\n\n\n3.6.1 Binary outcomes models - one group, no covariates\n\\[\n\\begin{aligned}\n\\text{P}(Y=1) &= \\pi\\\\\n\\text{P}(Y=0) &= 1-\\pi\\\\\n\\text{P}(Y=y) &= \\pi^y (1-\\pi)^{1-y}\\\\\n\\mathbf y  &= (y_1, ..., y_n)\\\\\n\\mathcal L(\\pi;\\mathbf y) &= \\pi^{\\sum y_i} (1-\\pi)^{n - \\sum y_i}\\\\\n\\ell(\\pi, \\mathbf y) &= \\left({\\sum y_i}\\right) \\text{log}\\left\\{\\pi\\right\\} + \\left(n - \\sum y_i\\right) \\text{log}\\left\\{1-\\pi\\right\\}\\\\\n&= \\left({\\sum y_i}\\right) \\left(\\text{log}\\left\\{\\pi\\right\\} - \\text{log}\\left\\{1-\\pi\\right\\}\\right) + n \\cdot \\text{log}\\left\\{1-\\pi\\right\\}\\\\\n&= \\left({\\sum y_i}\\right) \\text{log}\\left\\{\\frac{\\pi}{ 1-\\pi}\\right\\} + n \\cdot \\text{log}\\left\\{1-\\pi\\right\\}\n\\\\ &= \\left({\\sum y_i}\\right) \\text{logit}(\\pi) + n \\cdot \\text{log}\\left\\{1-\\pi\\right\\}\n\\end{aligned}\n\\]\n\n\n3.6.2 Binary outcomes - general\n\\[\n\\begin{aligned}\n\\text{P}(Y_i=1) &= \\pi_i\n\\\\ \\text{P}(Y_i=0) &= 1-\\pi_i\n\\end{aligned}\n\\]\n\\[\\text{P}(Y_i=y_i) = (\\pi_i)^y_i (1-\\pi_i)^{1-y_i}\\]\n\\[\\mathcal{L}_i(\\pi_i) = \\text{P}(Y_i=y_i)\\]\n\\[\n\\begin{aligned}\n\\ell_i(\\pi_i)\n   &= \\text{log}\\left\\{\\mathcal{L}_i(\\pi_i)\\right\\}\n\\\\ &= y_i \\text{log}\\left\\{\\pi_i\\right\\} + (1-y_i) \\text{log}\\left\\{1-\\pi_i\\right\\}\n\\end{aligned}\n\\]\n\nFor \\(\\text{iid}\\) data \\(\\tilde{y}  = (y_1, ..., y_n)\\):\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\pi;\\tilde{y}) &= \\text{P}(Y_1=y_1, \\ldots, Y_n = y_n)\n\\\\ &= \\prod_{i=1}^n \\text{P}(Y_i=y_i)\n\\\\ &= \\prod_{i=1}^n \\mathcal{L}_i(\\pi_i)\n\\end{aligned}\n\\]\n\n\n3.6.3 Modeling \\(\\pi_i\\) as a function of \\(X_i\\)\nIf there are only a few distinct \\(X_i\\) values, we can model \\(\\pi_i\\) separately for each value of \\(X_i\\).\nOtherwise, we need regression.\n\\[\n\\begin{aligned}\n\\pi(x) &\\equiv \\text{E}(Y=1|X=x)\\\\\n&= f(x^\\top\\beta)\n\\end{aligned}\n\\]\nTypically, we use the \\(\\text{expit}\\) inverse-link:\n\\[\\pi(\\tilde{x}) = \\text{expit}(\\tilde{x}'\\beta) \\tag{3.5}\\]\n\n\n3.6.4 Meet the beetles\n\n\nShow R code\nlibrary(glmx)\n\ndata(BeetleMortality, package = \"glmx\")\nbeetles = BeetleMortality |&gt;\n  mutate(\n    pct = died/n,\n    survived = n - died\n  )\n\nplot1 = \n  beetles |&gt; \n  ggplot(aes(x = dose, y = pct)) +\n  geom_point(aes(size = n)) +\n  xlab(\"Dose (log mg/L)\") +\n  ylab(\"Mortality rate (%)\") +\n  scale_y_continuous(labels = scales::percent) +\n  scale_size(range = c(1,2)) +\n  theme_bw(base_size = 18)\n\nprint(plot1)\n\n\n\n\n\nFigure 3.5: Mortality rates of adult flour beetles after five hours’ exposure to gaseous carbon disulphide (Bliss 1935)\n\n\n\n\n\n\n\n\n\n\n3.6.5 Why don’t we use linear regression?\n\n\nShow R code\nbeetles_long = beetles  |&gt; \n   reframe(\n    .by = everything(),\n    outcome = c(\n      rep(1, times = died), \n      rep(0, times = survived))) |&gt; \n  as_tibble()\n\nlm1 = beetles_long |&gt; lm(formula = outcome ~ dose)\nf.linear = function(x) predict(lm1, newdata = data.frame(dose = x))\n\nrange1 = range(beetles$dose) + c(-.2, .2)\n\nplot2 = \n  plot1 + \n  geom_function(\n    fun = f.linear, \n    aes(col = \"Straight line\")) +\n  labs(colour=\"Model\", size = \"\")\n\nplot2 |&gt; print()\n\n\n\n\n\nFigure 3.6: Mortality rates of adult flour beetles after five hours’ exposure to gaseous carbon disulphide (Bliss 1935)\n\n\n\n\n\n\n\n\n\n\n3.6.6 Zoom out\n\n\nShow R code\n(plot2 + expand_limits(x = c(1.6, 2))) |&gt; print()\n\n\n\n\n\nFigure 3.7: Mortality rates of adult flour beetles after five hours’ exposure to gaseous carbon disulphide (Bliss 1935)\n\n\n\n\n\n\n\n\n\n\n3.6.7 log transformation of dose?\n\n\nShow R code\nlm2 = beetles_long |&gt; lm(formula = outcome ~ log(dose))\nf.linearlog = function(x) predict(lm2, newdata = data.frame(dose = x))\n\nplot3 = plot2 + \n  expand_limits(x = c(1.6, 2)) +\n  geom_function(fun = f.linearlog, aes(col = \"Log-transform dose\"))\n(plot3 + expand_limits(x = c(1.6, 2))) |&gt; print()\n\n\n\n\n\nFigure 3.8: Mortality rates of adult flour beetles after five hours’ exposure to gaseous carbon disulphide (Bliss 1935)\n\n\n\n\n\n\n\n\n\n\n3.6.8 Logistic regression\n\n\nShow R code\n\n#| label: fig-beetles_5\n#| fig-cap: \"Mortality rates of adult flour beetles after five hours' exposure to gaseous carbon disulphide (Bliss 1935)\"\n\nbeetles_glm_grouped = beetles |&gt;\n  glm(formula = cbind(died, survived) ~ dose, family = \"binomial\")\nf = function(x)\n  beetles_glm_grouped |&gt;\n  predict(newdata = data.frame(dose = x), type = \"response\")\n\nplot4 = plot3 + geom_function(fun = f, aes(col = \"Logistic regression\"))\nplot4 |&gt; print()\n\n\n\n\n\n\n\n\n\n\n\n\n3.6.9 Three parts to regression models\n\nWhat distribution does the outcome have for a specific subpopulation defined by covariates? (outcome model)\nHow does the combination of covariates relate to the mean? (link function)\nHow do the covariates combine? (linear predictor, interactions)\n\n\n\n\n3.6.10 Logistic regression in R\n\n\nShow R code\n\nbeetles_glm_grouped = \n  beetles |&gt; \n  glm(\n    formula = cbind(died, survived) ~ dose, \n    family = \"binomial\")\n\nlibrary(parameters)\nbeetles_glm_grouped |&gt; \n  parameters() |&gt; \n  print_md()\n\n\n\n\n\nParameter\nLog-Odds\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n-60.72\n5.18\n(-71.44, -51.08)\n-11.72\n&lt; .001\n\n\ndose\n34.27\n2.91\n(28.85, 40.30)\n11.77\n&lt; .001\n\n\n\n\n\n\nFitted values:\n\n\nShow R code\nfitted.values(beetles_glm_grouped)\n#&gt;      1      2      3      4      5      6      7      8 \n#&gt; 0.0586 0.1640 0.3621 0.6053 0.7952 0.9032 0.9552 0.9790\npredict(beetles_glm_grouped, type = \"response\")\n#&gt;      1      2      3      4      5      6      7      8 \n#&gt; 0.0586 0.1640 0.3621 0.6053 0.7952 0.9032 0.9552 0.9790\npredict(beetles_glm_grouped, type = \"link\")\n#&gt;       1       2       3       4       5       6       7       8 \n#&gt; -2.7766 -1.6286 -0.5662  0.4277  1.3564  2.2337  3.0596  3.8444\n\nfit_y = beetles$n * fitted.values(beetles_glm_grouped)\n\n\n\n\n\n3.6.11 Individual observations\n\n\nShow R code\nbeetles_long\n\n\n\n\nTable 3.3: beetles data in long format\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\nShow R code\nbeetles_glm_ungrouped = \n  beetles_long |&gt; \n  glm(\n    formula = outcome ~ dose, \n    family = \"binomial\")\n\nbeetles_glm_ungrouped |&gt; parameters() |&gt; print_md()\n\n\n\n\nTable 3.4: logistic regression model for beetles data with individual Bernoulli data\n\n\n\n\n\n\nParameter\nLog-Odds\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n-60.72\n5.18\n(-71.44, -51.08)\n-11.72\n&lt; .001\n\n\ndose\n34.27\n2.91\n(28.85, 40.30)\n11.77\n&lt; .001\n\n\n\n\n\n\n\n\n\nHere’s the previous version again:\n\n\n\nShow R code\nbeetles_glm_grouped |&gt; parameters() |&gt; print_md()\n\n\n\n\nTable 3.5: logistic regression model for beetles data with grouped (binomial) data\n\n\n\n\n\n\nParameter\nLog-Odds\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n-60.72\n5.18\n(-71.44, -51.08)\n-11.72\n&lt; .001\n\n\ndose\n34.27\n2.91\n(28.85, 40.30)\n11.77\n&lt; .001\n\n\n\n\n\n\n\n\n\n\nThey seem the same! But not quite:\n\n\n\nShow R code\n\nlogLik(beetles_glm_grouped)\n#&gt; 'log Lik.' -18.72 (df=2)\nlogLik(beetles_glm_ungrouped)\n#&gt; 'log Lik.' -186.2 (df=2)\n\n\n\nThe difference is due to the binomial coefficient \\(\\left(n\\atop x \\right)\\) which isn’t included in the individual-observations (Bernoulli) version of the model.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models for Binary Outcomes</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#multiple-logistic-regression",
    "href": "logistic-regression.html#multiple-logistic-regression",
    "title": "3  Models for Binary Outcomes",
    "section": "3.7 Multiple logistic regression",
    "text": "3.7 Multiple logistic regression\n\n3.7.1 Coronary heart disease (WCGS) study data\n\nLet’s use the data from the Western Collaborative Group Study (WCGS) (Rosenman et al. (1975)) to explore multiple logistic regression:\n\nFrom Vittinghoff et al. (2012):\n“The Western Collaborative Group Study (WCGS) was a large epidemiological study designed to investigate the association between the”type A” behavior pattern and coronary heart disease (CHD)“.\nFrom Wikipedia, “Type A and Type B personality theory”:\n“The hypothesis describes Type A individuals as outgoing, ambitious, rigidly organized, highly status-conscious, impatient, anxious, proactive, and concerned with time management….\nThe hypothesis describes Type B individuals as a contrast to those of Type A. Type B personalities, by definition, are noted to live at lower stress levels. They typically work steadily and may enjoy achievement, although they have a greater tendency to disregard physical or mental stress when they do not achieve.”\n\n\nStudy design\nfrom ?faraway::wcgs:\n3154 healthy young men aged 39-59 from the San Francisco area were assessed for their personality type. All were free from coronary heart disease at the start of the research. Eight and a half years later change in CHD status was recorded.\nDetails (from faraway::wcgs)\nThe WCGS began in 1960 with 3,524 male volunteers who were employed by 11 California companies. Subjects were 39 to 59 years old and free of heart disease as determined by electrocardiogram. After the initial screening, the study population dropped to 3,154 and the number of companies to 10 because of various exclusions. The cohort comprised both blue- and white-collar employees.\n\n\n\n\n3.7.2 Baseline data collection\nsocio-demographic characteristics:\n\nage\neducation\nmarital status\nincome\noccupation\nphysical and physiological including:\nheight\nweight\nblood pressure\nelectrocardiogram\ncorneal arcus;\n\n\nbiochemical measurements:\n\ncholesterol and lipoprotein fractions;\nmedical and family history and use of medications;\n\n\nbehavioral data:\n\nType A interview,\nsmoking,\nexercise\nalcohol use.\n\n\nLater surveys added data on:\n\nanthropometry\ntriglycerides\nJenkins Activity Survey\ncaffeine use\n\nAverage follow-up continued for 8.5 years with repeat examinations.\n\n\n3.7.3 Load the data\nHere, I load the data:\n\n\nShow R code\n### load the data directly from a UCSF website:\nlibrary(haven)\nurl = paste0(\n    # I'm breaking up the url into two chunks for readability\n    \"https://regression.ucsf.edu/sites/g/files/\",\n    \"tkssra6706/f/wysiwyg/home/data/wcgs.dta\")\nwcgs = haven::read_dta(url)\n\n\n\n\nShow R code\nwcgs |&gt; head()\n\n\n\n\nTable 3.6: wcgs data\n\n\n\n\n  \n\n\n\n\n\n\n\n\n3.7.4 Data cleaning\n\nNow let’s do some data cleaning\n\n\n\nShow R code\nlibrary(arsenal) # provides `set_labels()`\nlibrary(forcats) # provides `as_factor()`\nlibrary(haven)\nlibrary(plotly)\nwcgs = wcgs |&gt; \n  mutate(\n    age = age |&gt; \n      arsenal::set_labels(\"Age (years)\"),\n    \n    arcus = \n      arcus |&gt; \n      as.logical() |&gt; \n      arsenal::set_labels(\"Arcus Senilis\"),\n    \n    time169 = \n      time169 |&gt; \n      as.numeric() |&gt; \n      arsenal::set_labels(\"Observation (follow up) time (days)\"),\n    \n    dibpat =\n      dibpat |&gt; \n      as_factor() |&gt; \n      relevel(ref = \"Type B\") |&gt; \n      arsenal::set_labels(\"Behavioral Pattern\"),\n    \n    typchd69 = typchd69 |&gt; \n      labelled(\n        label = \"Type of CHD Event\",\n        labels = \n          c(\n            \"None\" = 0, \n            \"infdeath\" = 1,\n            \"silent\" = 2,\n            \"angina\" = 3)),\n    \n    # turn stata-style labelled variables in to R-style factors:\n    across(\n      where(is.labelled), \n      haven::as_factor)\n  )\n\n\n\n\n3.7.5 What’s in the data\nHere’s a table of the data:\n\nShow R code\nwcgs |&gt;\n  select(-c(id, uni, t1)) |&gt;\n  tableby(chd69 ~ ., data = _) |&gt;\n  summary(\n    pfootnote = TRUE,\n    title =\n      \"Baseline characteristics by CHD status at end of follow-up\")\n\n\nBaseline characteristics by CHD status at end of follow-up\n\n\n\n\n\n\n\n\n\n\nNo (N=2897)\nYes (N=257)\nTotal (N=3154)\np value\n\n\n\n\nAge (years)\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n46.082 (5.457)\n48.490 (5.801)\n46.279 (5.524)\n\n\n\n   Range\n39.000 - 59.000\n39.000 - 59.000\n39.000 - 59.000\n\n\n\nArcus Senilis\n\n\n\n&lt; 0.0012\n\n\n   N-Miss\n0\n2\n2\n\n\n\n   FALSE\n2058 (71.0%)\n153 (60.0%)\n2211 (70.1%)\n\n\n\n   TRUE\n839 (29.0%)\n102 (40.0%)\n941 (29.9%)\n\n\n\nBehavioral Pattern\n\n\n\n&lt; 0.0012\n\n\n   A1\n234 (8.1%)\n30 (11.7%)\n264 (8.4%)\n\n\n\n   A2\n1177 (40.6%)\n148 (57.6%)\n1325 (42.0%)\n\n\n\n   B3\n1155 (39.9%)\n61 (23.7%)\n1216 (38.6%)\n\n\n\n   B4\n331 (11.4%)\n18 (7.0%)\n349 (11.1%)\n\n\n\nBody Mass Index (kg/m2)\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n24.471 (2.561)\n25.055 (2.579)\n24.518 (2.567)\n\n\n\n   Range\n11.191 - 37.653\n19.225 - 38.947\n11.191 - 38.947\n\n\n\nTotal Cholesterol\n\n\n\n&lt; 0.0011\n\n\n   N-Miss\n12\n0\n12\n\n\n\n   Mean (SD)\n224.261 (42.217)\n250.070 (49.396)\n226.372 (43.420)\n\n\n\n   Range\n103.000 - 400.000\n155.000 - 645.000\n103.000 - 645.000\n\n\n\nDiastolic Blood Pressure\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n81.723 (9.621)\n85.315 (10.311)\n82.016 (9.727)\n\n\n\n   Range\n58.000 - 150.000\n64.000 - 122.000\n58.000 - 150.000\n\n\n\nBehavioral Pattern\n\n\n\n&lt; 0.0012\n\n\n   Type B\n1486 (51.3%)\n79 (30.7%)\n1565 (49.6%)\n\n\n\n   Type A\n1411 (48.7%)\n178 (69.3%)\n1589 (50.4%)\n\n\n\nHeight (inches)\n\n\n\n0.2901\n\n\n   Mean (SD)\n69.764 (2.539)\n69.938 (2.410)\n69.778 (2.529)\n\n\n\n   Range\n60.000 - 78.000\n63.000 - 77.000\n60.000 - 78.000\n\n\n\nLn of Systolic Blood Pressure\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n4.846 (0.110)\n4.900 (0.125)\n4.850 (0.112)\n\n\n\n   Range\n4.585 - 5.438\n4.605 - 5.298\n4.585 - 5.438\n\n\n\nLn of Weight\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n5.126 (0.123)\n5.155 (0.118)\n5.128 (0.123)\n\n\n\n   Range\n4.357 - 5.670\n4.868 - 5.768\n4.357 - 5.768\n\n\n\nCigarettes per day\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n11.151 (14.329)\n16.665 (15.657)\n11.601 (14.518)\n\n\n\n   Range\n0.000 - 99.000\n0.000 - 60.000\n0.000 - 99.000\n\n\n\nSystolic Blood Pressure\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n128.034 (14.746)\n135.385 (17.473)\n128.633 (15.118)\n\n\n\n   Range\n98.000 - 230.000\n100.000 - 200.000\n98.000 - 230.000\n\n\n\nCurrent smoking\n\n\n\n&lt; 0.0012\n\n\n   No\n1554 (53.6%)\n98 (38.1%)\n1652 (52.4%)\n\n\n\n   Yes\n1343 (46.4%)\n159 (61.9%)\n1502 (47.6%)\n\n\n\nObservation (follow up) time (days)\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n2775.158 (562.205)\n1654.700 (859.297)\n2683.859 (666.524)\n\n\n\n   Range\n238.000 - 3430.000\n18.000 - 3229.000\n18.000 - 3430.000\n\n\n\nType of CHD Event\n\n\n\n\n\n\n   None\n0 (0.0%)\n0 (0.0%)\n0 (0.0%)\n\n\n\n   infdeath\n2897 (100.0%)\n0 (0.0%)\n2897 (91.9%)\n\n\n\n   silent\n0 (0.0%)\n135 (52.5%)\n135 (4.3%)\n\n\n\n   angina\n0 (0.0%)\n71 (27.6%)\n71 (2.3%)\n\n\n\n   4\n0 (0.0%)\n51 (19.8%)\n51 (1.6%)\n\n\n\nWeight (lbs)\n\n\n\n&lt; 0.0011\n\n\n   Mean (SD)\n169.554 (21.010)\n174.463 (21.573)\n169.954 (21.096)\n\n\n\n   Range\n78.000 - 290.000\n130.000 - 320.000\n78.000 - 320.000\n\n\n\nWeight Category\n\n\n\n&lt; 0.0012\n\n\n   &lt; 140\n217 (7.5%)\n15 (5.8%)\n232 (7.4%)\n\n\n\n   140-170\n1440 (49.7%)\n98 (38.1%)\n1538 (48.8%)\n\n\n\n   170-200\n1049 (36.2%)\n122 (47.5%)\n1171 (37.1%)\n\n\n\n   &gt; 200\n191 (6.6%)\n22 (8.6%)\n213 (6.8%)\n\n\n\nRECODE of age (Age)\n\n\n\n&lt; 0.0012\n\n\n   35-40\n512 (17.7%)\n31 (12.1%)\n543 (17.2%)\n\n\n\n   41-45\n1036 (35.8%)\n55 (21.4%)\n1091 (34.6%)\n\n\n\n   46-50\n680 (23.5%)\n70 (27.2%)\n750 (23.8%)\n\n\n\n   51-55\n463 (16.0%)\n65 (25.3%)\n528 (16.7%)\n\n\n\n   56-60\n206 (7.1%)\n36 (14.0%)\n242 (7.7%)\n\n\n\n\n\nLinear Model ANOVA\nPearson’s Chi-squared test\n\n\n\n3.7.6 Data by age and personality type\nFor now, we will look at the interaction between age and personality type (dibpat). To make it easier to visualize the data, we summarize the event rates for each combination of age:\n\n\nShow R code\nchd_grouped_data = \n  wcgs |&gt; \n  summarize(\n    .by = c(age, dibpat),\n    n = sum(chd69 %in% c(\"Yes\", \"No\")),\n    x = sum(chd69 == \"Yes\")) |&gt; \n  mutate(\n    `n - x` = n - x,\n    `p(chd)` = (x / n) |&gt; \n      labelled(label = \"CHD Event by 1969\"),\n    `odds(chd)` = `p(chd)` / (1 - `p(chd)`), \n    `logit(chd)` = log(`odds(chd)`)\n  )\n\nchd_grouped_data\n\n\n\n  \n\n\n\n\n\n3.7.7 Graphical exploration\n\n\nShow R code\nlibrary(ggplot2)\nlibrary(ggeasy)\nlibrary(scales)\nchd_plot_probs = \n  chd_grouped_data |&gt; \n  ggplot(\n    aes(\n      x = age, \n      y = `p(chd)`, \n      col = dibpat)\n  ) +\n  geom_point(aes(size = n), alpha = .7) + \n  scale_size(range = c(1,4)) +\n  geom_line() +\n  theme_bw() +\n  ylab(\"P(CHD Event by 1969)\") +\n  scale_y_continuous(\n    labels = scales::label_percent(),\n    sec.axis = sec_axis(\n      ~ odds(.),\n      name = \"odds(CHD Event by 1969)\")) +\n  ggeasy::easy_labs() +\n  theme(legend.position = \"bottom\")\n\nprint(chd_plot_probs)\n\n\n\n\n\nFigure 3.9: CHD rates by age group, probability scale\n\n\n\n\n\n\n\n\n\n\nOdds scale\n\n\nShow R code\ntrans_odds = trans_new(\n  name = \"odds\", \n  transform = odds, \n  inverse = odds_inv)\n\nchd_plot_odds = chd_plot_probs + \n  scale_y_continuous(\n    trans = trans_odds, # this line changes the vertical spacing\n    name = chd_plot_probs$labels$y,\n    sec.axis = sec_axis(\n      ~ odds(.),\n      name = \"odds(CHD Event by 1969)\"))\n\nprint(chd_plot_odds)\n\n\n\n\n\nFigure 3.10: CHD rates by age group, odds spacing\n\n\n\n\n\n\n\n\n\n\n\nLog-odds (logit) scale\n\n\nShow R code\ntrans_logit = trans_new(\n  name = \"logit\", \n  transform = logit, \n  inverse = expit)\n\nchd_plot_logit = \n  chd_plot_probs + \n  scale_y_continuous(\n    trans = trans_logit, # this line changes the vertical spacing\n    name = chd_plot_probs$labels$y,\n    breaks = c(seq(.01, .1, by = .01), .15, .2),\n    minor_breaks = NULL,\n    sec.axis = sec_axis(\n      ~ logit(.),\n      name = \"log(odds(CHD Event by 1969))\"))\n\nprint(chd_plot_logit)\n\n\n\n\n\nFigure 3.11: CHD data (logit-scale)\n\n\n\n\n\n\n\n\n\n\n\n3.7.8 Logistic regression models for CHD data\nHere, we fit stratified models for CHD by personality type.\n\n\nShow R code\nchd_glm_strat = glm(\n  \"formula\" = chd69 == \"Yes\" ~ dibpat + dibpat:age - 1, \n  \"data\" = wcgs,\n  \"family\" = binomial(link = \"logit\")\n)\n\nchd_glm_strat |&gt; parameters() |&gt; print_md()\n\n\n\n\nTable 3.7: CHD model, stratified parametrization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nLog-Odds\nSE\n95% CI\nz\np\n\n\n\n\ndibpat (Type B)\n-5.80\n0.98\n(-7.73, -3.90)\n-5.95\n&lt; .001\n\n\ndibpat (Type A)\n-5.50\n0.67\n(-6.83, -4.19)\n-8.18\n&lt; .001\n\n\ndibpat (Type B) × age\n0.06\n0.02\n(0.02, 0.10)\n3.01\n0.003\n\n\ndibpat (Type A) × age\n0.07\n0.01\n(0.05, 0.10)\n5.24\n&lt; .001\n\n\n\n\n\n\n\n\nWe can get the corresponding odds ratios (\\(e^{\\beta}\\)s) by passing exponentiate = TRUE to parameters():\n\n\nShow R code\nchd_glm_strat |&gt; \n  parameters(exponentiate = TRUE) |&gt; \n  print_md()\n\n\n\n\nTable 3.8: Odds ratio estimates for CHD model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nOdds Ratio\nSE\n95% CI\nz\np\n\n\n\n\ndibpat (Type B)\n3.02e-03\n2.94e-03\n(4.40e-04, 0.02)\n-5.95\n&lt; .001\n\n\ndibpat (Type A)\n4.09e-03\n2.75e-03\n(1.08e-03, 0.02)\n-8.18\n&lt; .001\n\n\ndibpat (Type B) × age\n1.06\n0.02\n(1.02, 1.11)\n3.01\n0.003\n\n\ndibpat (Type A) × age\n1.07\n0.01\n(1.05, 1.10)\n5.24\n&lt; .001\n\n\n\n\n\n\n\n\n\n\n3.7.9 Models superimposed on data\nWe can graph our fitted models on each scale (probability, odds, log-odds).\n\n\nprobability scale\n\n\nShow R code\n\ncurve_type_A = function(x) \n{\n  chd_glm_strat |&gt; predict(\n    type = \"response\",\n    newdata = tibble(age = x, dibpat = \"Type A\"))\n}\n\ncurve_type_B = function(x) \n{\n  chd_glm_strat |&gt; predict(\n    type = \"response\",\n    newdata = tibble(age = x, dibpat = \"Type B\"))\n}\n\nchd_plot_probs_2 =\n  chd_plot_probs +\n  geom_function(\n    fun = curve_type_A,\n    aes(col = \"Type A\")\n  ) +\n  geom_function(\n    fun = curve_type_B,\n    aes(col = \"Type B\")\n  )\nprint(chd_plot_probs_2)\n\n\n\n\n\n\n\n\n\n\n\n\nodds scale\n\n\nShow R code\n# curve_type_A = function(x) \n# {\n#   chd_glm_strat |&gt; predict(\n#     type = \"link\",\n#     newdata = tibble(age = x, dibpat = \"Type A\")) |&gt; exp()\n# }\n# curve_type_B = function(x) \n# {\n#   chd_glm_strat |&gt; predict(\n#     type = \"link\",\n#     newdata = tibble(age = x, dibpat = \"Type B\")) |&gt; exp()\n# }\n\nchd_plot_odds_2 =\n  chd_plot_odds +\n  geom_function(\n    fun = curve_type_A,\n    aes(col = \"Type A\")\n  ) +\n  geom_function(\n    fun = curve_type_B,\n    aes(col = \"Type B\")\n  )\nprint(chd_plot_odds_2)\n\n\n\n\n\nFigure 3.12\n\n\n\n\n\n\n\n\n\n\n\nlog-odds (logit) scale\n\n\nShow R code\nchd_plot_logit_2 =\n  chd_plot_logit +\n  geom_function(\n    fun = curve_type_A,\n    aes(col = \"Type A\")\n  ) +\n  geom_function(\n    fun = curve_type_B,\n    aes(col = \"Type B\")\n  )\n\nprint(chd_plot_logit_2)\n\n\n\n\n\nFigure 3.13\n\n\n\n\n\n\n\n\n\n\n\n3.7.10 reference-group and contrast parametrization\nWe can also use the corner-point parametrization (with reference groups and contrasts):\n\n\nShow R code\nchd_glm_contrasts = \n  wcgs |&gt; \n  glm(\n    \"data\" = _,\n    \"formula\" = chd69 == \"Yes\" ~ dibpat*I(age - 50), \n    \"family\" = binomial(link = \"logit\")\n  )\n\nchd_glm_contrasts |&gt; \n  parameters() |&gt; \n  print_md()\n\n\n\n\nTable 3.9: CHD model (corner-point parametrization)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nLog-Odds\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n-2.73\n0.13\n(-2.98, -2.49)\n-21.45\n&lt; .001\n\n\ndibpat (Type A)\n0.82\n0.15\n(0.53, 1.13)\n5.42\n&lt; .001\n\n\nage - 50\n0.06\n0.02\n(0.02, 0.10)\n3.01\n0.003\n\n\ndibpat (Type A) × age - 50\n0.01\n0.02\n(-0.04, 0.06)\n0.42\n0.674\n\n\n\n\n\n\n\n\n\nCompare with Table 3.8.\n\n\n\nExercise 3.16 If I give you model 1, how would you get the coefficients of model 2?\n\n\n\nTheorem 3.15 For the logistic regression model:\n\n\\(Y_i|\\tilde{X}_i \\ \\sim_{⫫}\\ \\text{Ber}(\\pi(\\tilde{X}_i))\\)\n\\(\\pi(\\tilde{x}) = \\text{expit}\\left\\{\\tilde{x}'\\tilde{\\beta}\\right\\}\\)\n\nConsider two covariate patterns, \\(\\tilde{x}\\) and \\(\\tilde{x^*}\\).\nThe odds ratio comparing these covariate patterns is:\n\\[\n\\omega(\\tilde{x},\\tilde{x^*}) = \\text{exp}\\left\\{(\\tilde{x}-\\tilde{x^*})^{\\top} \\tilde{\\beta}\\right\\}\n\\]\n\n\n\nProof. \\[\n\\begin{aligned}\n\\omega(\\tilde{x},\\tilde{x^*})\n&= \\frac {\\omega(Y=1 | \\tilde{X}= \\tilde{x})} {\\omega(Y=1 | \\tilde{X}= \\tilde{x^*})}\n\\\\ &= \\frac{\\text{exp}\\left\\{\\tilde{x}^{\\top}\\tilde{\\beta}\\right\\}}{\\text{exp}\\left\\{{\\tilde{x^*}}^{\\top} \\tilde{\\beta}\\right\\}}\n\\\\ &= \\text{exp}\\left\\{\\tilde{x}^{\\top}\\tilde{\\beta}- {\\tilde{x^*}}^{\\top} \\tilde{\\beta}\\right\\}\n\\\\ &= \\text{exp}\\left\\{(\\tilde{x}^{\\top} - {\\tilde{x^*}}^{\\top}) \\tilde{\\beta}\\right\\}\n\\\\ &= \\text{exp}\\left\\{{(\\tilde{x}- \\tilde{x^*})}^{\\top} \\tilde{\\beta}\\right\\}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models for Binary Outcomes</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#fitting-logistic-regression-models",
    "href": "logistic-regression.html#fitting-logistic-regression-models",
    "title": "3  Models for Binary Outcomes",
    "section": "3.8 Fitting logistic regression models",
    "text": "3.8 Fitting logistic regression models\n\n3.8.1 Maximum likelihood estimation for \\(\\text{ciid}\\) data\nAssume:\n\n\\(Y_i|\\tilde{X}_i \\ \\sim_{⫫}\\ \\text{Ber}(\\pi(X_i))\\)\n\\(\\pi(\\tilde{x}) = \\text{expit}\\left\\{\\tilde{x}'\\tilde{\\beta}\\right\\}\\)\n\n\n\nlog-likelihood function\n\\[\n\\begin{aligned}\n\\ell(\\tilde{\\beta}, \\tilde{y})\n   &= \\text{log}\\left\\{\\mathcal{L}(\\tilde{\\beta}, \\tilde{y}) \\right\\}\n\\\\ &= \\sum_{i=1}^n{\\color{red}\\ell_i}(\\pi(\\tilde{x}_i))\n\\end{aligned}\n\\tag{3.6}\\]\n\n\\[\n\\begin{aligned}\n{\\color{red}\\ell_i}(\\pi)\n   &= y_i \\text{log}\\left\\{\\pi\\right\\} + (1 - y_i) \\text{log}\\left\\{1-\\pi\\right\\}\n\\\\ &= y_i \\text{log}\\left\\{\\pi\\right\\} + (1 \\cdot\\text{log}\\left\\{1-\\pi\\right\\} - y_i \\cdot\\text{log}\\left\\{1-\\pi\\right\\})\n\\\\ &= y_i \\text{log}\\left\\{\\pi\\right\\} + (\\text{log}\\left\\{1-\\pi\\right\\} - y_i \\text{log}\\left\\{1-\\pi\\right\\})\n\\\\ &= y_i \\text{log}\\left\\{\\pi\\right\\} + \\text{log}\\left\\{1-\\pi\\right\\} - y_i \\text{log}\\left\\{{\\color{blue}1-\\pi}\\right\\}\n\\\\ &= y_i \\text{log}\\left\\{\\pi\\right\\} - y_i \\text{log}\\left\\{{\\color{blue}1-\\pi}\\right\\} + \\text{log}\\left\\{1-\\pi\\right\\}\n\\\\ &= (y_i \\text{log}\\left\\{\\pi\\right\\} - y_i \\text{log}\\left\\{{\\color{blue}1-\\pi}\\right\\}) + \\text{log}\\left\\{1-\\pi\\right\\}\n\\\\ &= y_i (\\text{log}\\left\\{{\\color{red}\\pi}\\right\\} - \\text{log}\\left\\{{\\color{blue}1-\\pi}\\right\\}) + \\text{log}\\left\\{1-\\pi\\right\\}\n\\\\ &= y_i \\left(\\text{log}\\left\\{\\frac{{\\color{red}\\pi}}{{\\color{blue}1-\\pi}}\\right\\}\\right) + \\text{log}\\left\\{1-\\pi\\right\\}\n\\\\ &= y_i (\\text{logit}(\\pi)) + \\text{log}\\left\\{1-\\pi\\right\\}\n\\end{aligned}\n\\]\n\n\n\nscore function\n\\[\n\\begin{aligned}\n\\ell'(\\tilde{\\beta})\n   &\\stackrel{\\text{def}}{=}\\frac{\\partial}{\\partial \\tilde{\\beta}} \\ell(\\tilde{\\beta})\n\\\\ &=      \\frac{\\partial}{\\partial \\tilde{\\beta}} \\sum_{i=1}^n\\ell_i(\\tilde{\\beta})\n\\\\ &=      \\sum_{i=1}^n\\frac{\\partial}{\\partial \\tilde{\\beta}} \\ell_i(\\tilde{\\beta})\n\\\\ &=      \\sum_{i=1}^n\\ell'_i(\\tilde{\\beta})\n\\end{aligned}\n\\]\n\n\\[\n\\begin{aligned}\n\\ell_i'(\\tilde{\\beta})\n   &= \\frac{\\partial}{\\partial \\tilde{\\beta}} y_i \\left(\\text{logit}\\left\\{\\pi_i\\right\\}\\right) + \\text{log}\\left\\{1-\\pi_i\\right\\}\n\\\\ &= \\frac{\\partial}{\\partial \\tilde{\\beta}}\\left\\{y_i \\left(\\tilde{x}_i'\\tilde{\\beta}\\right) + \\text{log}\\left\\{1-\\pi_i\\right\\}\\right\\}\n\\\\ &= \\left\\{y_i \\frac{\\partial}{\\partial \\tilde{\\beta}}\\left(\\tilde{x}_i'\\tilde{\\beta}\\right) + \\frac{\\partial}{\\partial \\tilde{\\beta}}\\text{log}\\left\\{1-\\pi_i\\right\\}\\right\\}\n\\\\ &= \\left\\{\\tilde{x}_i y_i + \\frac{\\partial}{\\partial \\tilde{\\beta}}\\text{log}\\left\\{1-\\text{expit}(\\tilde{x}_i'\\tilde{\\beta})\\right\\}\\right\\}\n\\\\ &= \\left\\{\\tilde{x}_i y_i + \\frac{\\partial}{\\partial \\tilde{\\beta}}\\text{log}\\left\\{\\left(1+\\text{exp}\\left\\{\\tilde{x}_i'\\tilde{\\beta}\\right\\}\\right)^{-1}\\right\\}\\right\\}\n\\\\ &= \\left\\{\\tilde{x}_i y_i - \\frac{\\partial}{\\partial \\tilde{\\beta}}\\text{log}\\left\\{1+\\text{exp}\\left\\{\\tilde{x}_i'\\tilde{\\beta}\\right\\}\\right\\}\\right\\}\n\\end{aligned}\n\\]\n\nNow we need to apply the chain rule:\n\\[\n\\frac{\\partial}{\\partial \\beta}\\text{log}\\left\\{1+\\text{exp}\\left\\{\\tilde{x}_i'\\beta\\right\\}\\right\\} =\n\\frac{1}{1+\\text{exp}\\left\\{\\tilde{x}_i'\\beta\\right\\}} \\frac{\\partial}{\\partial \\beta}\\left\\{1+\\text{exp}\\left\\{\\tilde{x}_i'\\beta\\right\\}\\right\\}\n\\]\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\beta}\\left\\{1+\\text{exp}\\left\\{\\tilde{x}_i'\\beta\\right\\}\\right\\}\n   &= \\text{exp}\\left\\{\\tilde{x}_i'\\beta\\right\\} \\frac{\\partial}{\\partial \\beta}\\tilde{x}_i'\\beta\n\\\\ &= \\tilde{x}_i \\text{exp}\\left\\{\\tilde{x}_i'\\beta\\right\\}\n\\end{aligned}\n\\]\nSo:\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\beta}\\text{log}\\left\\{1+\\text{exp}\\left\\{\\tilde{x}_i'\\beta\\right\\}\\right\\}\n   &= \\frac{1}{1+\\text{exp}\\left\\{\\tilde{x}_i'\\beta\\right\\}} \\text{exp}\\left\\{\\tilde{x}_i'\\beta\\right\\} \\tilde{x}_i\n\\\\ &= \\frac{\\text{exp}\\left\\{\\tilde{x}_i'\\beta\\right\\}}{1+\\text{exp}\\left\\{\\tilde{x}_i'\\beta\\right\\}}  \\tilde{x}_i\n\\\\ &= \\tilde{x}_i \\text{expit}\\left\\{\\tilde{x}_i'\\beta\\right\\}\n\\end{aligned}\n\\]\n\nSo:\n\\[\n\\begin{aligned}\n\\ell_i'(\\tilde{\\beta})\n&= \\tilde{x}_i y_i - \\tilde{x}_i \\text{expit}\\left\\{\\tilde{x}_i'\\beta\\right\\}\n\\\\ &= \\tilde{x}_i (y_i - \\text{expit}\\left\\{\\tilde{x}_i'\\beta\\right\\})\n\\\\ &= \\tilde{x}_i (y_i - \\pi_i)\n\\\\ &= \\tilde{x}_i (y_i - \\mathbb{E}[Y_i|\\tilde{X}_i=\\tilde{x}_i])\n\\\\ &= \\tilde{x}_i \\ \\varepsilon(y_i|\\tilde{X}_i=\\tilde{x}_i)\n\\end{aligned}\n\\]\n\nThis last expression is essentially the same as we found in linear regression.\n\n\nPutting the pieces of \\(\\ell'(\\tilde{\\beta})\\) back together, we have:\n\\[\n\\ell'(\\tilde{\\beta}) = \\sum_{i=1}^n\\left\\{\\tilde{x}_i(y_i - \\text{expit}\\left\\{\\tilde{x}_i'\\beta\\right\\}) \\right\\}\n\\]\nSetting \\(\\ell'(\\tilde{\\beta}; \\tilde{y}) = 0\\) gives us:\n\\[\\sum_{i=1}^n\\left\\{\\tilde{x}_i(y_i - \\text{expit}\\left\\{\\tilde{x}_i'\\beta\\right\\}) \\right\\} = 0 \\tag{3.7}\\]\n\n\nIn general, the estimating equation \\(\\ell'(\\tilde{\\beta}; \\tilde{y}) = 0\\) cannot be solved analytically.\nInstead, we can use the Newton-Raphson method:\n\n\\[\n{\\widehat{\\theta}}^*\n\\leftarrow {\\widehat{\\theta}}^* - \\left(\\ell''\\left(\\tilde{y};{\\widehat{\\theta}}^*\\right)\\right)^{-1}\n\\ell'\\left(\\tilde{y};{\\widehat{\\theta}}^*\\right)\n\\]\n\nWe make an iterative series of guesses, and each guess helps us make the next guess better (i.e., higher log-likelihood). You can see some information about this process like so:\n\n\noptions(digits = 8)\ntemp = \n  wcgs |&gt; \n  glm(\n    control = glm.control(trace = TRUE),\n    data = _,\n    formula = chd69 == \"Yes\" ~ dibpat*age, \n    family = binomial(link = \"logit\")\n  )\n#&gt; Deviance = 1775.7899 Iterations - 1\n#&gt; Deviance = 1708.5396 Iterations - 2\n#&gt; Deviance = 1704.0434 Iterations - 3\n#&gt; Deviance = 1703.9833 Iterations - 4\n#&gt; Deviance = 1703.9832 Iterations - 5\n#&gt; Deviance = 1703.9832 Iterations - 6\n\n\nAfter each iteration of the fitting procedure, the deviance (\\(2(\\ell_{\\text{full}} - \\ell(\\hat\\beta))\\) ) is printed. You can see that the algorithm took six iterations to converge to a solution where the likelihood wasn’t changing much anymore.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models for Binary Outcomes</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#sec-gof",
    "href": "logistic-regression.html#sec-gof",
    "title": "3  Models for Binary Outcomes",
    "section": "3.9 Model comparisons for logistic models",
    "text": "3.9 Model comparisons for logistic models\n\n3.9.1 Deviance test\nWe can compare the maximized log-likelihood of our model, \\(\\ell(\\hat\\beta; \\mathbf x)\\), versus the log-likelihood of the full model (aka saturated model aka maximal model), \\(\\ell_{\\text{full}}\\), which has one parameter per covariate pattern. With enough data, \\(2(\\ell_{\\text{full}} - \\ell(\\hat\\beta; \\mathbf x)) \\dot \\sim \\chi^2(N - p)\\), where \\(N\\) is the number of distinct covariate patterns and \\(p\\) is the number of \\(\\beta\\) parameters in our model. A significant p-value for this deviance statistic indicates that there’s some detectable pattern in the data that our model isn’t flexible enough to catch.\n\n\n\n\n\n\nCaution\n\n\n\nThe deviance statistic needs to have a large amount of data for each covariate pattern for the \\(\\chi^2\\) approximation to hold. A guideline from Dobson is that if there are \\(q\\) distinct covariate patterns \\(x_1...,x_q\\), with \\(n_1,...,n_q\\) observations per pattern, then the expected frequencies \\(n_k \\cdot \\pi(x_k)\\) should be at least 1 for every pattern \\(k\\in 1:q\\).\n\n\nIf you have covariates measured on a continuous scale, you may not be able to use the deviance tests to assess goodness of fit.\n\n\n3.9.2 Hosmer-Lemeshow test\nIf our covariate patterns produce groups that are too small, a reasonable solution is to make bigger groups by merging some of the covariate-pattern groups together.\nHosmer and Lemeshow (1980) proposed that we group the patterns by their predicted probabilities according to the model of interest. For example, you could group all of the observations with predicted probabilities of 10% or less together, then group the observations with 11%-20% probability together, and so on; \\(g=10\\) categories in all.\nThen we can construct a statistic \\[X^2 = \\sum_{c=1}^g \\frac{(o_c - e_c)^2}{e_c}\\] where \\(o_c\\) is the number of events observed in group \\(c\\), and \\(e_c\\) is the number of events expected in group \\(c\\) (based on the sum of the fitted values \\(\\hat\\pi_i\\) for observations in group \\(c\\)).\nIf each group has enough observations in it, you can compare \\(X^2\\) to a \\(\\chi^2\\) distribution; by simulation, the degrees of freedom has been found to be approximately \\(g-2\\).\nFor our CHD model, this procedure would be:\n\n\nShow R code\nwcgs = \n  wcgs |&gt; \n  mutate(\n    pred_probs_glm1 = chd_glm_strat |&gt; fitted(),\n    pred_prob_cats1 = \n      pred_probs_glm1 |&gt; \n      cut(breaks = seq(0, 1, by = .1), \n          include.lowest = TRUE))\n\nHL_table = \n  wcgs |&gt; \n  summarize(\n    .by = pred_prob_cats1,\n    n = n(),\n    o = sum(chd69 == \"Yes\"),\n    e = sum(pred_probs_glm1)\n  )\n\nlibrary(pander)\nHL_table |&gt; pander()\n\n\n\n\n\n\n\n\n\n\n\npred_prob_cats1\nn\no\ne\n\n\n\n\n(0.1,0.2]\n785\n116\n108\n\n\n(0.2,0.3]\n64\n12\n13.77\n\n\n[0,0.1]\n2,305\n129\n135.2\n\n\n\n\n\nShow R code\n\nX2 = HL_table |&gt; \n  summarize(\n    `X^2` = sum((o-e)^2/e)\n  ) |&gt; \n  pull(`X^2`)\nprint(X2)\n#&gt; [1] 1.1102871\n\npval1 = pchisq(X2, lower = FALSE, df = nrow(HL_table) - 2)\n\n\nOur statistic is \\(X^2 = 1.11028711\\); \\(p(\\chi^2(1) &gt; 1.11028711) = 0.29201955\\), which is our p-value for detecting a lack of goodness of fit.\nUnfortunately that grouping plan left us with just three categories with any observations, so instead of grouping by 10% increments of predicted probability, typically analysts use deciles of the predicted probabilities:\n\n\nShow R code\nwcgs = \n  wcgs |&gt; \n  mutate(\n    pred_probs_glm1 = chd_glm_strat |&gt; fitted(),\n    pred_prob_cats1 = \n      pred_probs_glm1 |&gt; \n      cut(breaks = quantile(pred_probs_glm1, seq(0, 1, by = .1)), \n          include.lowest = TRUE))\n\nHL_table = \n  wcgs |&gt; \n  summarize(\n    .by = pred_prob_cats1,\n    n = n(),\n    o = sum(chd69 == \"Yes\"),\n    e = sum(pred_probs_glm1)\n  )\n\nHL_table |&gt; pander()\n\n\n\n\n\n\n\n\n\n\n\npred_prob_cats1\nn\no\ne\n\n\n\n\n(0.114,0.147]\n275\n48\n36.81\n\n\n(0.147,0.222]\n314\n51\n57.19\n\n\n(0.0774,0.0942]\n371\n27\n32.56\n\n\n(0.0942,0.114]\n282\n30\n29.89\n\n\n(0.0633,0.069]\n237\n17\n15.97\n\n\n(0.069,0.0774]\n306\n20\n22.95\n\n\n(0.0487,0.0633]\n413\n27\n24.1\n\n\n(0.0409,0.0487]\n310\n14\n14.15\n\n\n[0.0322,0.0363]\n407\n16\n13.91\n\n\n(0.0363,0.0409]\n239\n7\n9.48\n\n\n\n\n\nShow R code\n\nX2 = HL_table |&gt; \n  summarize(\n    `X^2` = sum((o-e)^2/e)\n  ) |&gt; \n  pull(`X^2`)\n\nprint(X2)\n#&gt; [1] 6.7811383\n\npval1 = pchisq(X2, lower = FALSE, df = nrow(HL_table) - 2)\n\n\nNow we have more evenly split categories. The p-value is \\(0.56041994\\), still not significant.\nGraphically, we have compared:\n\n\nShow R code\n\nHL_plot = \n  HL_table |&gt; \n  ggplot(aes(x = pred_prob_cats1)) + \n  geom_line(aes(y = e, x = pred_prob_cats1, group = \"Expected\", col = \"Expected\")) +\n  geom_point(aes(y = e, size = n, col = \"Expected\")) +\n  geom_point(aes(y = o, size = n, col = \"Observed\")) +\n  geom_line(aes(y = o, col = \"Observed\", group = \"Observed\")) +\n  scale_size(range = c(1,4)) +\n  theme_bw() +\n  ylab(\"number of CHD events\") +\n  theme(axis.text.x = element_text(angle = 45))\n\n\n\n\nShow R code\nggplotly(HL_plot)\n\n\n\n\n\n\n\n\n3.9.3 Comparing models\n\nAIC = \\(-2 * \\ell(\\hat\\theta) + 2 * p\\) [lower is better]\nBIC = \\(-2 * \\ell(\\hat\\theta) + p * \\text{log}(n)\\) [lower is better]\nlikelihood ratio [higher is better]",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models for Binary Outcomes</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#residual-based-diagnostics",
    "href": "logistic-regression.html#residual-based-diagnostics",
    "title": "3  Models for Binary Outcomes",
    "section": "3.10 Residual-based diagnostics",
    "text": "3.10 Residual-based diagnostics\n\n3.10.1 Logistic regression residuals only work for grouped data\nResiduals only work if there is more than one observation for most covariate patterns.\nHere we will create the grouped-data version of our CHD model from the WCGS study:\n\n\nShow R code\n\nwcgs_grouped = \n  wcgs |&gt; \n  summarize(\n    .by = c(dibpat, age),\n    n = n(),\n    chd = sum(chd69 == \"Yes\"),\n    `!chd` = sum(chd69 == \"No\")\n  )\n\nchd_glm_strat_grouped = glm(\n  \"formula\" = cbind(chd, `!chd`) ~ dibpat + dibpat:age - 1, \n  \"data\" = wcgs_grouped,\n  \"family\" = binomial(link = \"logit\")\n)\n\nchd_glm_strat_grouped |&gt; parameters() |&gt; print_md()\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nLog-Odds\nSE\n95% CI\nz\np\n\n\n\n\ndibpat (Type B)\n-5.80\n0.98\n(-7.73, -3.90)\n-5.95\n&lt; .001\n\n\ndibpat (Type A)\n-5.50\n0.67\n(-6.83, -4.19)\n-8.18\n&lt; .001\n\n\ndibpat (Type B) × age\n0.06\n0.02\n(0.02, 0.10)\n3.01\n0.003\n\n\ndibpat (Type A) × age\n0.07\n0.01\n(0.05, 0.10)\n5.24\n&lt; .001\n\n\n\n\n\n\n\n3.10.2 (Response) residuals\n\\[e_k \\stackrel{\\text{def}}{=}\\bar y_k - \\hat{\\pi}(x_k)\\]\n(\\(k\\) indexes the covariate patterns)\nWe can graph these residuals \\(e_k\\) against the fitted values \\(\\hat\\pi(x_k)\\):\n\n\nShow R code\nwcgs_grouped = \n  wcgs_grouped |&gt; \n  mutate(\n    fitted = chd_glm_strat_grouped |&gt; fitted(),\n    fitted_logit = fitted |&gt; logit(),\n    response_resids = \n      chd_glm_strat_grouped |&gt; resid(type = \"response\")\n  )\n\nwcgs_response_resid_plot = \n  wcgs_grouped |&gt; \n  ggplot(\n    mapping = aes(\n      x = fitted,\n      y = response_resids\n    )\n  ) + \n  geom_point(\n    aes(col = dibpat)\n  ) +\n  geom_hline(yintercept = 0) + \n1  geom_smooth(\n    se = TRUE,\n    method.args = list(\n      span=2/3,\n      degree=1,\n      family=\"symmetric\",\n      iterations=3),\n    method = stats::loess)\n\n\n\n1\n\nDon’t worry about these options for now; I chose them to match autoplot() as closely as I can. plot.glm and autoplot use stats::lowess instead of stats::loess; stats::lowess is older, hard to use with geom_smooth, and hard to match exactly with stats::loess; see https://support.bioconductor.org/p/2323/.]\n\n\n\n\n\n\nShow R code\nwcgs_response_resid_plot |&gt; ggplotly()\n\n\n\n\n\n\nWe can see a slight fan-shape here: observations on the right have larger variance (as expected since \\(var(\\bar y) = \\pi(1-\\pi)/n\\) is maximized when \\(\\pi = 0.5\\)).\n\n\n3.10.3 Pearson residuals\nThe fan-shape in the response residuals plot isn’t necessarily a concern here, since we haven’t made an assumption of constant residual variance, as we did for linear regression.\nHowever, we might want to divide by the standard error in order to make the graph easier to interpret. Here’s one way to do that:\nThe Pearson (chi-squared) residual for covariate pattern \\(k\\) is: \\[\n\\begin{aligned}\nX_k &= \\frac{\\bar y_k - \\hat\\pi_k}{\\sqrt{\\hat \\pi_k (1-\\hat\\pi_k)/n_k}}\n\\end{aligned}\n\\]\nwhere \\[\n\\begin{aligned}\n\\hat\\pi_k\n&\\stackrel{\\text{def}}{=}\\hat\\pi(x_k)\\\\\n&\\stackrel{\\text{def}}{=}\\hat P(Y=1|X=x_k)\\\\\n&\\stackrel{\\text{def}}{=}\\text{expit}(x_i'\\hat \\beta)\\\\\n&\\stackrel{\\text{def}}{=}\\text{expit}(\\hat \\beta_0 + \\sum_{j=1}^p \\hat \\beta_j x_{ij})\n\\end{aligned}\n\\]\nLet’s take a look at the Pearson residuals for our CHD model from the WCGS data (graphed against the fitted values on the logit scale):\n\n\nShow R code\nlibrary(ggfortify)\n\n\n\n\nShow R code\nautoplot(chd_glm_strat_grouped, which = 1, ncol = 1) |&gt; print()\n\n\n\n\n\n\n\n\n\nThe fan-shape is gone, and these residuals don’t show any obvious signs of model fit issues.\n\nPearson residuals plot for beetles data\nIf we create the same plot for the beetles model, we see some strong evidence of a lack of fit:\n\n\nShow R code\nautoplot(beetles_glm_grouped, which = 1, ncol = 1) |&gt; print()\n\n\n\n\n\n\n\n\n\n\n\nPearson residuals with individual (ungrouped) data\nWhat happens if we try to compute residuals without grouping the data by covariate pattern?\n\n\nShow R code\nlibrary(ggfortify)\n\n\n\n\nShow R code\nautoplot(chd_glm_strat, which = 1, ncol = 1) |&gt; print()\n\n\n\n\n\n\n\n\n\nMeaningless.\n\n\nResiduals plot by hand (optional section)\nIf you want to check your understanding of what these residual plots are, try building them yourself:\n\n\nShow R code\n\nwcgs_grouped = \n  wcgs_grouped |&gt; \n  mutate(\n    fitted = chd_glm_strat_grouped |&gt; fitted(),\n    fitted_logit = fitted |&gt; logit(),\n    resids = chd_glm_strat_grouped |&gt; resid(type = \"pearson\")\n  )\n\nwcgs_resid_plot1 = \n  wcgs_grouped |&gt; \n  ggplot(\n    mapping = aes(\n      x = fitted_logit,\n      y = resids\n      \n    ) \n    \n  ) + \n  geom_point(\n    aes(col = dibpat)\n  ) +\n  geom_hline(yintercept = 0) + \n  geom_smooth(se = FALSE, \n              method.args = list(\n                span=2/3,\n                degree=1,\n                family=\"symmetric\",\n                iterations=3,\n                surface=\"direct\"\n                # span = 2/3, \n                # iterations = 3\n              ),\n              method = stats::loess)\n# plot.glm and autoplot use stats::lowess, which is hard to use with \n# geom_smooth and hard to match exactly; \n# see https://support.bioconductor.org/p/2323/\n\n\n\n\nShow R code\nwcgs_resid_plot1 |&gt; ggplotly()\n\n\n\n\n\n\n\n\n\n3.10.4 Pearson chi-squared goodness of fit test\nThe Pearson chi-squared goodness of fit statistic is: \\[\nX^2 = \\sum_{k=1}^m X_k^2\n\\] Under the null hypothesis that the model in question is correct (i.e., sufficiently complex), \\(X^2\\ \\dot \\sim\\ \\chi^2(N-p)\\).\n\n\nShow R code\n\nX = chd_glm_strat_grouped |&gt; \n  resid(type = \"pearson\")\n\nchisq_stat = sum(X^2)\n\npval = pchisq(\n  chisq_stat, \n  lower = FALSE, \n  df = length(X) - length(coef(chd_glm_strat_grouped)))\n\n\nFor our CHD model, the p-value for this test is 0.26523556; no significant evidence of a lack of fit at the 0.05 level.\n\nStandardized Pearson residuals\nEspecially for small data sets, we might want to adjust our residuals for leverage (since outliers in \\(X\\) add extra variance to the residuals):\n\\[r_{P_k} = \\frac{X_k}{\\sqrt{1-h_k}}\\]\nwhere \\(h_k\\) is the leverage of \\(X_k\\). The functions autoplot() and plot.lm() use these for some of their graphs.\n\n\n\n3.10.5 Deviance residuals\nFor large sample sizes, the Pearson and deviance residuals will be approximately the same. For small sample sizes, the deviance residuals from covariate patterns with small sample sizes can be unreliable (high variance).\n\\[d_k = \\text{sign}(y_k - n_k \\hat \\pi_k)\\left\\{\\sqrt{2[\\ell_{\\text{full}}(x_k) - \\ell(\\hat\\beta; x_k)]}\\right\\}\\]\n\nStandardized deviance residuals\n\\[r_{D_k} = \\frac{d_k}{\\sqrt{1-h_k}}\\]\n\n\n\n3.10.6 Diagnostic plots\nLet’s take a look at the full set of autoplot() diagnostics now for our CHD model:\n\n\nShow R code\nchd_glm_strat_grouped |&gt; autoplot(which = 1:6) |&gt; print()\n\n\n\n\n\nFigure 3.14: Diagnostics for CHD model\n\n\n\n\n\n\n\n\n:::\nThings look pretty good here. The QQ plot is still usable; with large samples; the residuals should be approximately Gaussian.\n\nBeetles\nLet’s look at the beetles model diagnostic plots for comparison:\n\n\nShow R code\nbeetles_glm_grouped |&gt; autoplot(which = 1:6) |&gt; print()\n\n\n\n\n\nFigure 3.15: Diagnostics for logistic model of BeetleMortality data\n\n\n\n\n\n\n\n\nHard to tell much from so little data, but there might be some issues here.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models for Binary Outcomes</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#other-link-functions-for-bernoulli-outcomes",
    "href": "logistic-regression.html#other-link-functions-for-bernoulli-outcomes",
    "title": "3  Models for Binary Outcomes",
    "section": "3.11 Other link functions for Bernoulli outcomes",
    "text": "3.11 Other link functions for Bernoulli outcomes\nIf you want risk ratios, you can sometimes get them by changing the link function:\n\n\nShow R code\n\ndata(anthers, package = \"dobson\")\nanthers.sum&lt;-aggregate(\n  anthers[c(\"n\",\"y\")], \n  by=anthers[c(\"storage\")],FUN=sum) \n\nanthers_glm_log = glm(\n  formula = cbind(y,n-y)~storage,\n  data=anthers.sum, \n  family=binomial(link=\"log\"))\n\nanthers_glm_log |&gt; parameters() |&gt; print_md()\n\n\n\n\n\nParameter\nLog-Risk\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n-0.80\n0.12\n(-1.04, -0.58)\n-6.81\n&lt; .001\n\n\nstorage\n0.17\n0.07\n(0.02, 0.31)\n2.31\n0.021\n\n\n\n\n\nNow \\(\\text{exp}\\left\\{\\beta\\right\\}\\) gives us risk ratios instead of odds ratios:\n\n\nShow R code\nanthers_glm_log |&gt; parameters(exponentiate = TRUE) |&gt; print_md()\n\n\n\n\n\nParameter\nRisk Ratio\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n0.45\n0.05\n(0.35, 0.56)\n-6.81\n&lt; .001\n\n\nstorage\n1.18\n0.09\n(1.03, 1.36)\n2.31\n0.021\n\n\n\n\n\nLet’s compare this model with a logistic model:\n\n\nShow R code\n\nanthers_glm_logit = glm(\n  formula = cbind(y, n - y) ~ storage,\n  data = anthers.sum,\n  family = binomial(link = \"logit\"))\n\nanthers_glm_logit |&gt; parameters(exponentiate = TRUE) |&gt; print_md()\n\n\n\n\n\nParameter\nOdds Ratio\nSE\n95% CI\nz\np\n\n\n\n\n(Intercept)\n0.76\n0.20\n(0.45, 1.27)\n-1.05\n0.296\n\n\nstorage\n1.49\n0.26\n(1.06, 2.10)\n2.29\n0.022\n\n\n\n\n\n[to add: fitted plots on each outcome scale]\nWhen I try to use link =\"log\" in practice, I often get errors about not finding good starting values for the estimation procedure. This is likely because the model is producing fitted probabilities greater than 1.\nWhen this happens, you can try to fit Poisson regression models instead (we will see those soon!). But then the outcome distribution isn’t quite right, and you won’t get warnings about fitted probabilities greater than 1. In my opinion, the Poisson model for binary outcomes is confusing and not very appealing.\n\n3.11.1 WCGS: link functions\n\n\nShow R code\nwcgs_glm_logit_link = \n  chd_grouped_data |&gt; \n  mutate(type = dibpat |&gt; relevel(ref = \"Type B\")) |&gt; \n  glm(\n    \"formula\" = cbind(x, `n - x`) ~ dibpat * age, \n    \"data\" = _,\n    \"family\" = binomial(link = \"logit\")\n  )\n\nwcgs_glm_identity_link = \n  chd_grouped_data |&gt; \n  mutate(type = dibpat |&gt; relevel(ref = \"Type B\")) |&gt; \n  glm(\n    \"formula\" = cbind(x, `n - x`) ~ dibpat * age, \n    \"data\" = _,\n    \"family\" = binomial(link = \"identity\")\n  )\nwcgs_glm_identity_link |&gt; coef() |&gt; pander()\n\n\n\n\n\n\n\n\n\n\n\n(Intercept)\ndibpatType A\nage\ndibpatType A:age\n\n\n\n\n-0.08257\n-0.1374\n0.002906\n0.004194\n\n\n\n\n\n\nShow R code\nlibrary(ggfortify)\nwcgs_glm_logit_link |&gt; autoplot(which = c(1), ncol = 1) + facet_wrap(~dibpat)\nwcgs_glm_identity_link |&gt; autoplot(which = c(1), ncol = 1) + facet_wrap(~dibpat)\n\n\n\n\nFigure 3.16: Residuals vs Fitted plot for wcgs models\n\n\n\n\n\n\n\n(a) Logistic link\n\n\n\n\n\n\n\n\n\n\n\n(b) Identity link\n\n\n\n\n\n\n\n\n\n\n\n\nShow R code\nbeetles_lm = \n  beetles_long |&gt; \n  lm(formula = died ~ dose)\n\nbeetles = \n  beetles |&gt; mutate(\n    resid_logit = beetles_glm_grouped |&gt; resid(type = \"response\"))\nbeetles_glm_grouped |&gt; autoplot(which = c(1), ncol = 1)\nbeetles_lm |&gt; autoplot(which = c(1), ncol = 1)\n\n\n\n\nFigure 3.17: Residuals vs Fitted plot for BeetleMortality models\n\n\n\n\n\n\n\n(a) Logistic link\n\n\n\n\n\n\n\n\n\n\n\n(b) Identity link",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models for Binary Outcomes</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#quasibinomial",
    "href": "logistic-regression.html#quasibinomial",
    "title": "3  Models for Binary Outcomes",
    "section": "3.12 Quasibinomial",
    "text": "3.12 Quasibinomial\nSee Hua Zhou’s lecture notes",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models for Binary Outcomes</span>"
    ]
  },
  {
    "objectID": "logistic-regression.html#further-reading",
    "href": "logistic-regression.html#further-reading",
    "title": "3  Models for Binary Outcomes",
    "section": "3.13 Further reading",
    "text": "3.13 Further reading\n\nHosmer, Lemeshow, and Sturdivant (2013) is a classic textbook on logistic regression\n\n\n\n\n\n\n\nDobson, Annette J, and Adrian G Barnett. 2018. An Introduction to Generalized Linear Models. 4th ed. CRC press. https://doi.org/10.1201/9781315182780.\n\n\nHosmer, David W, Stanley Lemeshow, and Rodney X Sturdivant. 2013. Applied Logistic Regression. John Wiley & Sons. https://onlinelibrary.wiley.com/doi/book/10.1002/9781118548387.\n\n\nNahhas, Ramzi W. 2024. Introduction to Regression Methods for Public Health Using r. CRC Press. https://www.bookdown.org/rwnahhas/RMPH/.\n\n\nRosenman, Ray H, Richard J Brand, C David Jenkins, Meyer Friedman, Reuben Straus, and Moses Wurm. 1975. “Coronary Heart Disease in the Western Collaborative Group Study: Final Follow-up Experience of 8 1/2 Years.” JAMA 233 (8): 872–77. https://doi.org/10.1001/jama.1975.03260080034016.\n\n\nVittinghoff, Eric, David V Glidden, Stephen C Shiboski, and Charles E McCulloch. 2012. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. 2nd ed. Springer. https://doi.org/10.1007/978-1-4614-1353-0.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Models for Binary Outcomes</span>"
    ]
  },
  {
    "objectID": "count-regression.html",
    "href": "count-regression.html",
    "title": "\n4  Models for Count Outcomes\n",
    "section": "",
    "text": "Acknowledgements\nThis content is adapted from:",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models for Count Outcomes</span>"
    ]
  },
  {
    "objectID": "count-regression.html#acknowledgements",
    "href": "count-regression.html#acknowledgements",
    "title": "\n4  Models for Count Outcomes\n",
    "section": "",
    "text": "Dobson and Barnett (2018), Chapter 9\n\nVittinghoff et al. (2012), Chapter 8\n\n\n\nConfiguring R\nFunctions from these packages will be used throughout this document:\n\nShow R codelibrary(conflicted) # check for conflicting function definitions\n# library(printr) # inserts help-file output into markdown output\nlibrary(rmarkdown) # Convert R Markdown documents into a variety of formats.\nlibrary(pander) # format tables for markdown\nlibrary(ggplot2) # graphics\nlibrary(ggeasy) # help with graphics\nlibrary(ggfortify) # help with graphics\nlibrary(dplyr) # manipulate data\nlibrary(tibble) # `tibble`s extend `data.frame`s\nlibrary(magrittr) # `%&gt;%` and other additional piping tools\nlibrary(haven) # import Stata files\nlibrary(knitr) # format R output for markdown\nlibrary(tidyr) # Tools to help to create tidy data\nlibrary(plotly) # interactive graphics\nlibrary(dobson) # datasets from Dobson and Barnett 2018\nlibrary(parameters) # format model output tables for markdown\nlibrary(haven) # import Stata files\nlibrary(latex2exp) # use LaTeX in R code (for figures and tables)\nlibrary(fs) # filesystem path manipulations\nlibrary(survival) # survival analysis\nlibrary(survminer) # survival analysis graphics\nlibrary(KMsurv) # datasets from Klein and Moeschberger\nlibrary(parameters) # format model output tables for\nlibrary(webshot2) # convert interactive content to static for pdf\nlibrary(forcats) # functions for categorical variables (\"factors\")\nlibrary(stringr) # functions for dealing with strings\nlibrary(lubridate) # functions for dealing with dates and times\n\n\nHere are some R settings I use in this document:\n\nShow R coderm(list = ls()) # delete any data that's already loaded into R\n\nconflicts_prefer(dplyr::filter)\nggplot2::theme_set(\n  ggplot2::theme_bw() + \n        # ggplot2::labs(col = \"\") +\n    ggplot2::theme(\n      legend.position = \"bottom\",\n      text = ggplot2::element_text(size = 12, family = \"serif\")))\n\nknitr::opts_chunk$set(message = FALSE)\noptions('digits' = 4)\n\npanderOptions(\"big.mark\", \",\")\npander::panderOptions(\"table.emphasize.rownames\", FALSE)\npander::panderOptions(\"table.split.table\", Inf)\nconflicts_prefer(dplyr::filter) # use the `filter()` function from dplyr() by default\nlegend_text_size = 9",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models for Count Outcomes</span>"
    ]
  },
  {
    "objectID": "count-regression.html#introduction",
    "href": "count-regression.html#introduction",
    "title": "\n4  Models for Count Outcomes\n",
    "section": "\n4.1 Introduction",
    "text": "4.1 Introduction\n\nThis chapter presents models for count data outcomes. With covariates, the event rate \\(\\lambda\\) becomes a function of the covariates \\(\\tilde{X}= (X_1, \\dots,X_n)\\). Typically, count data models use a \\(\\text{log}\\left\\{\\right\\}\\) link function, and thus an \\(\\text{exp}\\left\\{\\right\\}\\) inverse-link function. That is:\n\n\\[\n\\begin{aligned}\n\\mathbb{E}[Y | \\tilde{X}= \\tilde{x}, T = t] &= \\mu(\\tilde{x},t)\n\\\\ \\mu(\\tilde{x},t) &= \\lambda(\\tilde{x})\\cdot t\n\\\\ \\lambda(\\tilde{x}) &= \\text{exp}\\left\\{\\eta(\\tilde{x})\\right\\}\n\\\\ \\eta(\\tilde{x}) &= \\tilde{x}'\\tilde \\beta = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p\n\\end{aligned}\n\\]\nTherefore, \\[\n\\begin{aligned}\n\\text{log}\\left\\{\\mathbb{E}[Y | \\tilde{X}= \\tilde{x},T=t]\\right\\}\n&= \\text{log}\\left\\{\\mu(\\tilde{x})\\right\\}\\\\\n&=\\text{log}\\left\\{\\lambda(\\tilde{x}) \\cdot t\\right\\}\\\\\n&=\\text{log}\\left\\{\\lambda(\\tilde{x})\\right\\} + \\text{log}\\left\\{t\\right\\}\\\\\n&=\\text{log}\\left\\{\\text{exp}\\left\\{\\eta(\\tilde{x})\\right\\}\\right\\} + \\text{log}\\left\\{t\\right\\}\\\\\n&=\\eta(\\tilde{x}) + \\text{log}\\left\\{t\\right\\}\\\\\n&=\\tilde{x}'\\tilde\\beta + \\text{log}\\left\\{t\\right\\}\\\\\n&=(\\beta_0 +\\beta_1 x_1+\\dots + \\beta_p x_p) + \\text{log}\\left\\{t\\right\\}\\\\\n\\end{aligned}\n\\]\nIn contrast with the other covariates (represented by \\(\\tilde{X}\\)), \\(T\\) enters this expression with a \\(\\text{log}\\left\\{\\right\\}\\) transformation and without a corresponding \\(\\beta\\) coefficient.\n\n\n\n\n\n\nNote\n\n\n\nTerms that enter the linear component of a model without a coefficient, such as \\(\\text{log}\\left\\{t\\right\\}\\) here, are called offsets.\n\n\n\n4.1.1 Rate ratios\n\nDifferences on the log-rate scale become ratios on the rate scale, because\n\n\\[\\text{exp}\\left\\{a-b\\right\\} = \\frac{\\text{exp}\\left\\{a\\right\\}}{\\text{exp}\\left\\{b\\right\\}}\\]\n(recall from Algebra 2)\nTherefore, according to this model, differences of \\(\\delta\\) in covariate \\(x_j\\) correspond to rate ratios of \\(\\text{exp}\\left\\{\\beta_j \\cdot \\delta\\right\\}\\).\nThat is, letting \\(\\tilde{X}_{-j}\\) denote vector \\(\\tilde{X}\\) with element \\(j\\) removed:\n\\[\n\\begin{aligned}\n&{\n\\left\\{\n    \\text{log}\\left\\{\\mathbb{E}[Y |{\\color{red}{X_j = a}}, \\tilde{X}_{-j}=\\tilde{x}_{-j},T=t]\\right\\}\n    \\atop\n    {-\\text{log}\\left\\{\\mathbb{E}[Y |{\\color{red}{X_j = b}}, \\tilde{X}_{-j}=\\tilde{x}_{-j},T=t]\\right\\}}\n    \\right\\}\n}\\\\\n&=\n{\\left\\{\n\\text{log}\\left\\{t\\right\\} + \\beta_0 + \\beta_1 x_1 + ... + {\\color{red}{\\beta_j (a)}} + ...+\\beta_p x_p\n\\atop\n{-\\text{log}\\left\\{t\\right\\} + \\beta_0 + \\beta_1 x_1 + ... + {\\color{red}{\\beta_j (b)}} + ...+\\beta_p x_p}\n\\right\\}}\\\\\n&= \\color{red}{\\beta_j(a-b)}\n\\end{aligned}\n\\]\nAnd accordingly,\n\\[\n\\begin{aligned}\n\\frac\n{\\mathbb{E}[Y |{\\color{red}{X_j = a}}, \\tilde{X}_{-j} = \\tilde{x}_{-j}, T = t]\n}\n{\n\\mathbb{E}[Y |{\\color{red}{X_j = b}}, \\tilde{X}_{-j}=\\tilde{x}_{-j},T=t]\n}\n=\n\\text{exp}\\left\\{{\\color{red}{\\beta_j(a-b)}}\\right\\}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models for Count Outcomes</span>"
    ]
  },
  {
    "objectID": "count-regression.html#inference-for-count-regression-models",
    "href": "count-regression.html#inference-for-count-regression-models",
    "title": "\n4  Models for Count Outcomes\n",
    "section": "\n4.2 Inference for count regression models",
    "text": "4.2 Inference for count regression models\n\n4.2.1 Confidence intervals for regression coefficients and rate ratios\nAs usual:\n\\[\n\\beta \\in \\left[\\hat \\beta{\\color{red}\\pm} z_{1 - \\frac{\\alpha}{2}}\\cdot \\hat{\\text{se}}\\left(\\hat \\beta\\right)\\right]\n\\]\nRate ratios: exponentiate CI endpoints\n\\[\n\\text{exp}\\left\\{\\beta\\right\\} \\in \\left[\\text{exp}\\left\\{\\hat \\beta{\\color{red}\\pm} z_{1 - \\frac{\\alpha}{2}}\\cdot \\hat{\\text{se}}\\left(\\hat \\beta\\right)\\right\\} \\right]\n\\]\n\n4.2.2 Hypothesis tests for regression coefficients\n\\[\nt = \\frac{\\hat \\beta - \\beta_0}{\\hat{\\text{se}}\\left(\\hat \\beta\\right)}\n\\]\nCompare \\(t\\) or \\(|t|\\) to the tails of the standard Gaussian distribution, according to the null hypothesis.\n\n4.2.3 Comparing nested models\nlog(likelihood ratio) tests, as usual.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models for Count Outcomes</span>"
    ]
  },
  {
    "objectID": "count-regression.html#prediction",
    "href": "count-regression.html#prediction",
    "title": "\n4  Models for Count Outcomes\n",
    "section": "\n4.3 Prediction",
    "text": "4.3 Prediction\n\\[\n\\begin{aligned}\n\\hat y\n&\\stackrel{\\text{def}}{=}\\hat{\\mathbb{E}}[Y|\\tilde{X}= \\tilde{x},T=t]\\\\\n&=\\hat\\mu(\\tilde{x}, t)\\\\\n&=\\hat\\lambda(\\tilde{x}) \\cdot t\\\\\n&=\\text{exp}\\left\\{\\hat\\eta(\\tilde{x})\\right\\} \\cdot t\\\\\n&=\\text{exp}\\left\\{\\tilde{x}'\\hat{\\boldsymbol{\\beta}}\\right\\} \\cdot t\n\\end{aligned}\n\\]",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models for Count Outcomes</span>"
    ]
  },
  {
    "objectID": "count-regression.html#diagnostics",
    "href": "count-regression.html#diagnostics",
    "title": "\n4  Models for Count Outcomes\n",
    "section": "\n4.4 Diagnostics",
    "text": "4.4 Diagnostics\n\n4.4.1 Residuals\nObservation residuals\n\\[e \\stackrel{\\text{def}}{=}y - \\hat y\\]\nPearson residuals\n\\[r = \\frac{e}{\\hat{\\text{se}}\\left(e\\right)} \\approx \\frac{e}{\\sqrt{\\hat y}}\\]\nStandardized Pearson residuals\n\\[r_p = \\frac{r}{\\sqrt{1-h}}\\] where \\(h\\) is the “leverage” (which we will continue to leave undefined).\nDeviance residuals\n\\[\nd_k = \\text{sign}(y - \\hat y)\\left\\{\\sqrt{2[\\ell_{\\text{full}}(y) - \\ell(\\hat\\beta; y)]}\\right\\}\n\\]\n\n\n\n\n\n\nNote\n\n\n\n\\[\\text{sign}(x) \\stackrel{\\text{def}}{=}\\frac{x}{|x|}\\] In other words:\n\n\n\\(\\text{sign}(x) = -1\\) if \\(x &lt; 0\\)\n\n\n\\(\\text{sign}(x) = 0\\) if \\(x = 0\\)\n\n\n\\(\\text{sign}(x) = 1\\) if \\(x &gt; 0\\)",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models for Count Outcomes</span>"
    ]
  },
  {
    "objectID": "count-regression.html#zero-inflation",
    "href": "count-regression.html#zero-inflation",
    "title": "\n4  Models for Count Outcomes\n",
    "section": "\n4.5 Zero-inflation",
    "text": "4.5 Zero-inflation\n\n4.5.1 Models for zero-inflated counts\nWe assume a latent (unobserved) binary variable, \\(Z\\), which we model using logistic regression:\n\\[P(Z=1|X=x) = \\pi(x) = \\text{expit}(\\gamma_0 + \\gamma_1 x_1 +...)\\]\nAccording to this model, if \\(Z=1\\), then \\(Y\\) will always be zero, regardless of \\(X\\) and \\(T\\):\n\\[P(Y=0|Z=1,X=x,T=t) = 1\\]\nOtherwise (if \\(Z=0\\)), \\(Y\\) will have a Poisson distribution, conditional on \\(X\\) and \\(T\\), as above.\nEven though we never observe \\(Z\\), we can estimate the parameters \\(\\gamma_0\\)-\\(\\gamma_p\\), via maximum likelihood:\n\\[\n\\begin{aligned}\nP(Y=y|X=x,T=t) &= P(Y=y,Z=1|...) + P(Y=y,Z=0|...)\n\\end{aligned}\n\\] (by the Law of Total Probability)\nwhere \\[\n\\begin{aligned}\nP(Y=y,Z=z|...)\n&= P(Y=y|Z=z,...)P(Z=z|...)\n\\end{aligned}\n\\]\n\n\nExercise 4.1 Expand \\(P(Y=0|X=x,T=t)\\), \\(P(Y=1|X=x,T=t)\\) and \\(P(Y=y|X=x,T=t)\\) into expressions involving \\(P(Z=1|X=x,T=t)\\) and \\(P(Y=y|Z=0,X=x,T=t)\\).\n\n\n\nExercise 4.2 Derive the expected value and variance of \\(Y\\), conditional on \\(X\\) and \\(T\\), as functions of \\(P(Z=1|X=x,T=t)\\) and \\(\\mathbb{E}[Y|Z=0,X=x,T=t]\\).",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models for Count Outcomes</span>"
    ]
  },
  {
    "objectID": "count-regression.html#over-dispersion",
    "href": "count-regression.html#over-dispersion",
    "title": "\n4  Models for Count Outcomes\n",
    "section": "\n4.6 Over-dispersion",
    "text": "4.6 Over-dispersion\n\n\nThe Poisson distribution model forces the variance to equal the mean. In practice, many count distributions will have a variance substantially larger than the mean (or occasionally, smaller).\n\n\nDefinition 4.1 (Overdispersion) A random variable \\(X\\) is overdispersed relative to a model \\(\\text{p}(X=x)\\) if if its empirical variance in a dataset is larger than the value is predicted by the fitted model \\(\\hat{\\text{p}}(X=x)\\).\n\n\nc.f. Dobson and Barnett (2018) §3.2.1, 7.7, 9.8; Vittinghoff et al. (2012) §8.1.5; and https://en.wikipedia.org/wiki/Overdispersion.\nWhen we encounter overdispersion, we can try to reduce the residual variance by adding more covariates.\n\n\n4.6.1 Negative binomial models\nThere are alternatives to the Poisson model. Most notably, the negative binomial model.\nWe can still model \\(\\mu\\) as a function of \\(X\\) and \\(T\\) as before, and we can combine this model with zero-inflation (as the conditional distribution for the non-zero component).\n\n4.6.2 Quasipoisson\nAn alternative to Negative binomial is the “quasipoisson” distribution. I’ve never used it, but it seems to be a method-of-moments type approach rather than maximum likelihood. It models the variance as \\(\\text{Var}\\left(Y\\right) = \\mu\\theta\\), and estimates \\(\\theta\\) accordingly.\nSee ?quasipoisson in R for more.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models for Count Outcomes</span>"
    ]
  },
  {
    "objectID": "count-regression.html#example-needle-sharing",
    "href": "count-regression.html#example-needle-sharing",
    "title": "\n4  Models for Count Outcomes\n",
    "section": "\n4.7 Example: needle-sharing",
    "text": "4.7 Example: needle-sharing\n(adapted from Vittinghoff et al. (2012), §8)\n\nShow R codelibrary(tidyverse)\nlibrary(haven)\nneedles = \n  \"inst/extdata/needle_sharing.dta\" |&gt; \n  read_dta() |&gt;\n  as_tibble() |&gt;\n  mutate(\n    hivstat =\n      hivstat |&gt;\n      case_match(\n        1 ~ \"HIV+\",\n        0 ~ \"HIV-\") |&gt; \n      factor() |&gt; \n      relevel(ref = \"HIV-\"),\n    polydrug =\n      polydrug |&gt;\n      case_match(\n        1 ~ \"multiple drugs used\",\n        0 ~ \"one drug used\") |&gt;\n      factor() |&gt;\n      relevel(ref = \"one drug used\"),\n    homeless = \n      homeless |&gt; \n      case_match(\n        1 ~ \"homeless\", \n        0 ~ \"not homeless\") |&gt;\n      factor() |&gt;\n      relevel(ref = \"not homeless\"),\n    sex = sex |&gt; factor() |&gt; relevel(ref = \"M\"))\nneedles\n\n\nTable 4.1: Needle-sharing data\n\n\n\n  \n\n\n\n\n\n\n\n\nShow R codelibrary(ggplot2)\n\nneedles |&gt; \n  ggplot(\n    aes(\n      x = age,\n      y = shared_syr,\n      shape = sex,\n      col = ethn\n    )\n  ) + \n  geom_point(\n    size = 3, \n    alpha = .5) +\n  facet_grid(\n    cols = vars(sex, polydrug), \n    rows = vars(homeless)) +\n  theme(legend.position = \"bottom\")\n\n\n\nFigure 4.1: Rates of needle sharing\n\n\n\n\n\n\n\nCovariate counts\n\n\nTable 4.2: Counts of observations in needles dataset by sex, unhoused status, and multiple drug use\n\nShow R codeneedles |&gt; \n  dplyr::select(sex, homeless, polydrug) |&gt; \n  summary()\n#&gt;     sex             homeless                 polydrug  \n#&gt;  M    :97   not homeless:63   one drug used      :109  \n#&gt;  F    :30   homeless    :61   multiple drugs used: 19  \n#&gt;  Trans: 1   NA's        : 4\n\n\n\n\n\nThere’s only one individual with sex = Trans, which unfortunately isn’t enough data to analyze. We will remove that individual:\n\n{.r .cell-code  code-summary=\"remove singleton observation with `sex == \"Trans\"`\"} needles = needles |&gt; filter(sex != \"Trans\")\n\n\n4.7.1 models\n\nShow R codeglm1 = glm(\n  data = needles,\n  family = stats::poisson,\n  shared_syr ~ age + sex + homeless*polydrug\n)\n\nlibrary(parameters)\nglm1 |&gt; parameters(exponentiate = TRUE) |&gt; \n  print_md()\n\n\nTable 4.3: Poisson model for needle-sharing data\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nIRR\nSE\n95% CI\nz\np\n\n\n\n(Intercept)\n4.52\n1.15\n(2.74, 7.45)\n5.90\n&lt; .001\n\n\nage\n0.97\n5.58e-03\n(0.96, 0.98)\n-5.41\n&lt; .001\n\n\nsex (F)\n1.98\n0.23\n(1.58, 2.49)\n5.88\n&lt; .001\n\n\nhomeless (homeless)\n3.58\n0.45\n(2.79, 4.59)\n10.06\n&lt; .001\n\n\npolydrug (multiple drugs used)\n1.45e-07\n5.82e-05\n(0.00, Inf)\n-0.04\n0.969\n\n\nhomeless (homeless) × polydrug (multiple drugs used)\n1.27e+06\n5.12e+08\n(0.00, Inf)\n0.03\n0.972\n\n\n\n\n\n\n\n\n\nShow R codelibrary(ggfortify)\nautoplot(glm1)\n\n\nTable 4.4: Diagnostics for Poisson model\n\n\n\n\n\n\n\n\n\n\n\n–\n\n\nTable 4.5: Negative binomial model for needle-sharing data\n\nShow R codelibrary(MASS) #need this for glm.nb()\nglm1.nb = glm.nb(\n  data = needles,\n  shared_syr ~ age + sex + homeless*polydrug\n)\nsummary(glm1.nb)\n#&gt; \n#&gt; Call:\n#&gt; glm.nb(formula = shared_syr ~ age + sex + homeless * polydrug, \n#&gt;     data = needles, init.theta = 0.08436295825, link = log)\n#&gt; \n#&gt; Coefficients:\n#&gt;                                               Estimate Std. Error z value\n#&gt; (Intercept)                                   9.91e-01   1.71e+00    0.58\n#&gt; age                                          -2.76e-02   3.82e-02   -0.72\n#&gt; sexF                                          1.06e+00   8.07e-01    1.32\n#&gt; homelesshomeless                              1.65e+00   7.22e-01    2.29\n#&gt; polydrugmultiple drugs used                  -2.46e+01   3.61e+04    0.00\n#&gt; homelesshomeless:polydrugmultiple drugs used  2.32e+01   3.61e+04    0.00\n#&gt;                                              Pr(&gt;|z|)  \n#&gt; (Intercept)                                     0.563  \n#&gt; age                                             0.469  \n#&gt; sexF                                            0.187  \n#&gt; homelesshomeless                                0.022 *\n#&gt; polydrugmultiple drugs used                     0.999  \n#&gt; homelesshomeless:polydrugmultiple drugs used    0.999  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for Negative Binomial(0.0844) family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 69.193  on 119  degrees of freedom\n#&gt; Residual deviance: 57.782  on 114  degrees of freedom\n#&gt;   (7 observations deleted due to missingness)\n#&gt; AIC: 315.5\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 1\n#&gt; \n#&gt; \n#&gt;               Theta:  0.0844 \n#&gt;           Std. Err.:  0.0197 \n#&gt; \n#&gt;  2 x log-likelihood:  -301.5060\n\n\n\n\n\nShow R codetibble(name = names(coef(glm1)), poisson = coef(glm1), nb = coef(glm1.nb))\n\n\nTable 4.6: Poisson versus Negative Binomial Regression coefficient estimates\n\n\n\n  \n\n\n\n\n\n\nzero-inflation\n\nShow R codelibrary(glmmTMB)\nzinf_fit1 = glmmTMB(\n  family = \"poisson\",\n  data  = needles,\n  formula = shared_syr ~ age + sex + homeless*polydrug,\n  ziformula = ~ age + sex + homeless + polydrug # fit won't converge with interaction\n)\n\nzinf_fit1 |&gt; \n  parameters(exponentiate = TRUE) |&gt; \n  print_md()\n\n\nTable 4.7: Zero-inflated poisson model\n\n\n\n# Fixed Effects\n\n\n\n\n\n\n\n\n\nParameter\nIRR\nSE\n95% CI\nz\np\n\n\n\n(Intercept)\n3.16\n0.82\n(1.90, 5.25)\n4.44\n&lt; .001\n\n\nage\n1.01\n5.88e-03\n(1.00, 1.02)\n1.74\n0.081\n\n\nsex [F]\n3.43\n0.44\n(2.67, 4.40)\n9.68\n&lt; .001\n\n\nhomeless [homeless]\n3.44\n0.47\n(2.63, 4.50)\n9.03\n&lt; .001\n\n\npolydrug [multiple drugs used]\n1.85e-09\n1.21e-05\n(0.00, Inf)\n-3.08e-03\n0.998\n\n\nhomeless [homeless] × polydrug [multiple drugs used]\n1.38e+08\n9.04e+11\n(0.00, Inf)\n2.87e-03\n0.998\n\n\n\n\n# Zero-Inflation\n\n\n\n\n\n\n\n\n\nParameter\nOdds Ratio\nSE\n95% CI\nz\np\n\n\n\n(Intercept)\n0.49\n0.54\n(0.06, 4.25)\n-0.65\n0.514\n\n\nage\n1.05\n0.03\n(1.00, 1.10)\n1.95\n0.051\n\n\nsex [F]\n1.44\n0.84\n(0.46, 4.50)\n0.62\n0.533\n\n\nhomeless [homeless]\n0.68\n0.34\n(0.26, 1.80)\n-0.78\n0.436\n\n\npolydrug [multiple drugs used]\n1.15\n0.91\n(0.24, 5.43)\n0.18\n0.858\n\n\n\n\n\n\n\n\n\nAnother R package for zero-inflated models is pscl (Zeileis, Kleiber, and Jackman (2008)).\n\nzero-inflated negative binomial model\n\nShow R codelibrary(glmmTMB)\nzinf_fit1 = glmmTMB(\n  family = nbinom2,\n  data  = needles,\n  formula = shared_syr ~ age + sex + homeless*polydrug,\n  ziformula = ~ age + sex + homeless + polydrug # fit won't converge with interaction\n)\n\nzinf_fit1 |&gt; \n  parameters(exponentiate = TRUE) |&gt; \n  print_md()\n\n\nTable 4.8: Zero-inflated negative binomial model\n\n\n\n# Fixed Effects\n\n\n\n\n\n\n\n\n\nParameter\nIRR\nSE\n95% CI\nz\np\n\n\n\n(Intercept)\n1.06\n1.48\n(0.07, 16.52)\n0.04\n0.969\n\n\nage\n1.02\n0.03\n(0.96, 1.08)\n0.53\n0.599\n\n\nsex [F]\n6.86\n6.36\n(1.12, 42.16)\n2.08\n0.038\n\n\nhomeless [homeless]\n6.44\n4.59\n(1.60, 26.01)\n2.62\n0.009\n\n\npolydrug [multiple drugs used]\n8.25e-10\n7.07e-06\n(0.00, Inf)\n-2.44e-03\n0.998\n\n\nhomeless [homeless] × polydrug [multiple drugs used]\n2.36e+08\n2.02e+12\n(0.00, Inf)\n2.25e-03\n0.998\n\n\n\n\n# Zero-Inflation\n\n\n\n\n\n\n\n\n\nParameter\nOdds Ratio\nSE\n95% CI\nz\np\n\n\n\n(Intercept)\n0.10\n0.20\n(1.47e-03, 6.14)\n-1.11\n0.269\n\n\nage\n1.07\n0.04\n(0.99, 1.15)\n1.78\n0.075\n\n\nsex [F]\n2.72\n2.40\n(0.48, 15.33)\n1.13\n0.258\n\n\nhomeless [homeless]\n1.15\n0.86\n(0.27, 4.96)\n0.19\n0.853\n\n\npolydrug [multiple drugs used]\n0.75\n0.86\n(0.08, 7.12)\n-0.25\n0.799\n\n\n\n\n# Dispersion\n\nParameter\nCoefficient\n95% CI\n\n\n(Intercept)\n0.44\n(0.11, 1.71)\n\n\n\n\n\n\n\n\n\n\n\n\n\nDobson, Annette J, and Adrian G Barnett. 2018. An Introduction to Generalized Linear Models. 4th ed. CRC press. https://doi.org/10.1201/9781315182780.\n\n\nVittinghoff, Eric, David V Glidden, Stephen C Shiboski, and Charles E McCulloch. 2012. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. 2nd ed. Springer. https://doi.org/10.1007/978-1-4614-1353-0.\n\n\nZeileis, Achim, Christian Kleiber, and Simon Jackman. 2008. “Regression Models for Count Data in R.” Journal of Statistical Software 27 (8). https://www.jstatsoft.org/v27/i08/.",
    "crumbs": [
      "Generalized Linear Models",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Models for Count Outcomes</span>"
    ]
  },
  {
    "objectID": "time-to-event-models.html#footnotes",
    "href": "time-to-event-models.html#footnotes",
    "title": "Time to Event Models",
    "section": "",
    "text": "Binary outcomes are typically defined for a specific time-point. It is important to clearly define whether we are interested in outcome status at end of study, at end of life, or at some other time.↩︎",
    "crumbs": [
      "Time to Event Models"
    ]
  },
  {
    "objectID": "intro-to-survival-analysis.html",
    "href": "intro-to-survival-analysis.html",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "",
    "text": "Configuring R\nFunctions from these packages will be used throughout this document:\nShow R codelibrary(conflicted) # check for conflicting function definitions\n# library(printr) # inserts help-file output into markdown output\nlibrary(rmarkdown) # Convert R Markdown documents into a variety of formats.\nlibrary(pander) # format tables for markdown\nlibrary(ggplot2) # graphics\nlibrary(ggeasy) # help with graphics\nlibrary(ggfortify) # help with graphics\nlibrary(dplyr) # manipulate data\nlibrary(tibble) # `tibble`s extend `data.frame`s\nlibrary(magrittr) # `%&gt;%` and other additional piping tools\nlibrary(haven) # import Stata files\nlibrary(knitr) # format R output for markdown\nlibrary(tidyr) # Tools to help to create tidy data\nlibrary(plotly) # interactive graphics\nlibrary(dobson) # datasets from Dobson and Barnett 2018\nlibrary(parameters) # format model output tables for markdown\nlibrary(haven) # import Stata files\nlibrary(latex2exp) # use LaTeX in R code (for figures and tables)\nlibrary(fs) # filesystem path manipulations\nlibrary(survival) # survival analysis\nlibrary(survminer) # survival analysis graphics\nlibrary(KMsurv) # datasets from Klein and Moeschberger\nlibrary(parameters) # format model output tables for\nlibrary(webshot2) # convert interactive content to static for pdf\nlibrary(forcats) # functions for categorical variables (\"factors\")\nlibrary(stringr) # functions for dealing with strings\nlibrary(lubridate) # functions for dealing with dates and times\nHere are some R settings I use in this document:\nShow R coderm(list = ls()) # delete any data that's already loaded into R\n\nconflicts_prefer(dplyr::filter)\nggplot2::theme_set(\n  ggplot2::theme_bw() + \n        # ggplot2::labs(col = \"\") +\n    ggplot2::theme(\n      legend.position = \"bottom\",\n      text = ggplot2::element_text(size = 12, family = \"serif\")))\n\nknitr::opts_chunk$set(message = FALSE)\noptions('digits' = 4)\n\npanderOptions(\"big.mark\", \",\")\npander::panderOptions(\"table.emphasize.rownames\", FALSE)\npander::panderOptions(\"table.split.table\", Inf)\nconflicts_prefer(dplyr::filter) # use the `filter()` function from dplyr() by default\nlegend_text_size = 9",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Survival Analysis</span>"
    ]
  },
  {
    "objectID": "intro-to-survival-analysis.html#overview",
    "href": "intro-to-survival-analysis.html#overview",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.1 Overview",
    "text": "5.1 Overview\n\n5.1.1 Time-to-event outcomes\nSurvival analysis is a framework for modeling time-to-event outcomes. It is used in:\n\nclinical trials, where the event is often death or recurrence of disease.\nengineering reliability analysis, where the event is failure of a device or system.\ninsurance, particularly life insurance, where the event is death.\n\n\n\n\n\n\n\nNote\n\n\n\nThe term survival analysis is a bit misleading. Survival outcomes can sometimes be analyzed using binomial models (logistic regression). Time-to-event models or survival time analysis might be a better name.",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Survival Analysis</span>"
    ]
  },
  {
    "objectID": "intro-to-survival-analysis.html#time-to-event-outcome-distributions",
    "href": "intro-to-survival-analysis.html#time-to-event-outcome-distributions",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.2 Time-to-event outcome distributions",
    "text": "5.2 Time-to-event outcome distributions\n\n5.2.1 Distributions of Time-to-Event Data\n\nThe distribution of event times is asymmetric and can be long-tailed, and starts at 0 (that is, \\(P(T&lt;0) = 0\\)).\nThe base distribution is not normal, but exponential.\nThere are usually censored observations, which are ones in which the failure time is not observed.\nOften, these are right-censored, meaning that we know that the event occurred after some known time \\(t\\), but we don’t know the actual event time, as when a patient is still alive at the end of the study.\nObservations can also be left-censored, meaning we know the event has already happened at time \\(t\\), or interval-censored, meaning that we only know that the event happened between times \\(t_1\\) and \\(t_2\\).\nAnalysis is difficult if censoring is associated with treatment.\n\n5.2.2 Right Censoring\n\nPatients are in a clinical trial for cancer, some on a new treatment and some on standard of care.\nSome patients in each group have died by the end of the study. We know the survival time (measured for example from time of diagnosis—each person on their own clock).\nPatients still alive at the end of the study are right censored.\nPatients who are lost to follow-up or withdraw from the study may be right-censored.\n\n5.2.3 Left and Interval Censoring\n\nAn individual tests positive for HIV.\nIf the event is infection with HIV, then we only know that it has occurred before the testing time \\(t\\), so this is left censored.\nIf an individual has a negative HIV test at time \\(t_1\\) and a positive HIV test at time \\(t_2\\), then the infection event is interval censored.",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Survival Analysis</span>"
    ]
  },
  {
    "objectID": "intro-to-survival-analysis.html#distribution-functions-for-time-to-event-variables",
    "href": "intro-to-survival-analysis.html#distribution-functions-for-time-to-event-variables",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.3 Distribution functions for time-to-event variables",
    "text": "5.3 Distribution functions for time-to-event variables\n\n5.3.1 The Probability Density Function (PDF)\nFor a time-to-event variable \\(T\\) with a continuous distribution, the probability density function is defined as usual (see Section B.4.1).\n\nIn most time-to-event models, this density is assumed to be 0 for all \\(t&lt;0\\); that is, \\(f(t) = 0, \\forall t&lt;0\\). In other words, the support of \\(T\\) is typically \\([0,\\infty)\\).\n\n\n\nExample 5.1 (exponential distribution) Recall from Epi 202: the pdf of the exponential distribution family of models is:\n\\[p(T=t) = \\mathbb{1}_{t \\ge 0} \\cdot \\lambda \\text{e}^{-\\lambda t}\\]\nwhere \\(\\lambda &gt; 0\\).\n\nHere are some examples of exponential pdfs:\n\n\n\n\n\n\n\n\n\n\n5.3.2 The Cumulative Distribution Function (CDF)\nThe cumulative distribution function is defined as:\n\\[\n\\begin{aligned}\nF(t) &\\stackrel{\\text{def}}{=}\\Pr(T \\le t)\\\\\n&=\\int_{u=-\\infty}^t f(u) du\n\\end{aligned}\n\\]\n\nExample 5.2 (exponential distribution) Recall from Epi 202: the cdf of the exponential distribution family of models is:\n\\[\nP(T\\le t) = \\mathbb{1}_{t \\ge 0} \\cdot (1- \\text{e}^{-\\lambda t})\n\\] where \\(\\lambda &gt; 0\\).\n\nHere are some examples of exponential cdfs:\n\n\n\n\n\n\n\n\n\n5.3.3 The Survival Function\nFor survival data, a more important quantity is the survival function:\n\\[\n\\begin{aligned}\nS(t) &\\stackrel{\\text{def}}{=}\\Pr(T &gt; t)\\\\\n&=\\int_{u=t}^\\infty p(u) du\\\\\n&=1-F(t)\\\\\n\\end{aligned}\n\\]\n\n\nDefinition 5.1 (Survival function)  \n\nGiven a random time-to-event variable \\(T\\), the survival function or survivor function, denoted \\(S(t)\\), is the probability that the event time is later than \\(t\\). If the event in a clinical trial is death, then \\(S(t)\\) is the expected fraction of the original population at time 0 who have survived up to time \\(t\\) and are still alive at time \\(t\\); that is:\n\n\\[S(t) \\stackrel{\\text{def}}{=}\\Pr(T &gt; t) \\tag{5.1}\\]\n\n\n\nExample 5.3 (exponential distribution) Since \\(S(t) = 1 - F(t)\\), the survival function of the exponential distribution family of models is:\n\\[\nP(T&gt; t) = \\left\\{ {{\\text{e}^{-\\lambda t}, t\\ge0} \\atop {1, t \\le 0}}\\right.\n\\] where \\(\\lambda &gt; 0\\).\nFigure 5.1 shows some examples of exponential survival functions.\n\n\n\n\n\n\nFigure 5.1: Exponential Survival Functions\n\n\n\n\n\n\n\n\n\nTheorem 5.1 If \\(A_t\\) represents survival status at time \\(t\\), with \\(A_t = 1\\) denoting alive at time \\(t\\) and \\(A_t = 0\\) denoting deceased at time \\(t\\), then:\n\\[S(t) = \\Pr(A_t=1) = \\mathbb{E}[A_t]\\]\n\n\n\nTheorem 5.2 If \\(T\\) is a nonnegative random variable, then:\n\\[\\mathbb{E}[T] = \\int_{t=0}^{\\infty} S(t)dt\\]\n\n\n\nProof. See https://statproofbook.github.io/P/mean-nnrvar.html or\n\n\n5.3.4 The Hazard Function\nAnother important quantity is the hazard function:\n\nDefinition 5.2 (Hazard function, hazard rate, hazard rate function)  \n\nThe hazard function, hazard rate, hazard rate function, for a random variable \\(T\\) at value \\(t\\), typically denoted as \\(h(t)\\) or \\(\\lambda(t)\\), is the conditional density of \\(T\\) at \\(t\\), given \\(T \\ge t\\). That is:\n\n\\[h(t) \\stackrel{\\text{def}}{=}\\text{p}(T=t|T\\ge t)\\]\n\nIf \\(T\\) represents the time at which an event occurs, then \\(h(t)\\) is the probability that the event occurs at time \\(t\\), given that it has not occurred prior to time \\(t\\).\n\n\n\nNew content for 2025:\n\nDefinition 5.3 (Incidence rate) Given a population of individuals indexed by \\(i\\), each with their own hazard rate \\(h_i(t)\\), the incidence rate for that population is the mean hazard rate:\n\\[\\bar{h}(t) \\stackrel{\\text{def}}{=}\\frac{1}{n}\\sum_{i=1}^nh_i(t)\\]\n\n\nNew content for 2025:\n\nTheorem 5.3 (Incidence rate in a homogenous population) If a population of individuals indexed by \\(i\\) all have identical hazard rates \\(h_i(t) = h(t)\\), then the incidence rate for that population is equal to the hazard rate:\n\\[\\bar{h}(t) = h(t)\\]\n\n(End of new content for 2025)\n\n\nThe hazard function has an important relationship to the density and survival functions, which we can use to derive the hazard function for a given probability distribution (Theorem 5.4).\n\n\nLemma 5.1 (Joint probability of a variable with itself) \\[p(T=t, T\\ge t) = p(T=t)\\]\n\nProof. Recall from Epi 202: if \\(A\\) and \\(B\\) are statistical events and \\(A\\subseteq B\\), then \\(p(A, B) = p(A)\\). In particular, \\(\\{T=t\\} \\subseteq \\{T\\geq t\\}\\), so \\(p(T=t, T\\ge t) = p(T=t)\\).\n\n\n\n\nTheorem 5.4 (Hazard equals density over survival) \\[h(t)=\\frac{f(t)}{S(t)}\\]\n\n\n\nProof. \\[\n\\begin{aligned}\nh(t) &=p(T=t|T\\ge t)\\\\\n&=\\frac{p(T=t, T\\ge t)}{p(T \\ge t)}\\\\\n&=\\frac{p(T=t)}{p(T \\ge t)}\\\\\n&=\\frac{f(t)}{S(t)}\n\\end{aligned}\n\\]\n\n\n\nExample 5.4 (exponential distribution) The hazard function of the exponential distribution family of models is:\n\\[\n\\begin{aligned}\nP(T=t|T \\ge t)\n&= \\frac{f(t)}{S(t)}\\\\\n&= \\frac{\\mathbb{1}_{t \\ge 0}\\cdot \\lambda  \\text{e}^{-\\lambda t}}{\\text{e}^{-\\lambda t}}\\\\\n&=\\mathbb{1}_{t \\ge 0}\\cdot \\lambda\n\\end{aligned}\n\\] Figure 5.2 shows some examples of exponential hazard functions.\n\n\n\n\n\n\nFigure 5.2: Examples of hazard functions for exponential distributions\n\n\n\n\n\n\n\n\nWe can also view the hazard function as the derivative of the negative of the logarithm of the survival function:\n\nTheorem 5.5 (transform survival to hazard) \\[h(t) = \\frac{\\partial}{\\partial t}\\left\\{-\\text{log}\\left\\{S(t)\\right\\}\\right\\}\\]\n\n\n\nProof. \\[\n\\begin{aligned}\nh(t)\n&= \\frac{f(t)}{S(t)}\\\\\n&= \\frac{-S'(t)}{S(t)}\\\\\n&= -\\frac{S'(t)}{S(t)}\\\\\n&=-\\frac{\\partial}{\\partial t}\\text{log}\\left\\{S(t)\\right\\}\\\\\n&=\\frac{\\partial}{\\partial t}\\left\\{-\\text{log}\\left\\{S(t)\\right\\}\\right\\}\n\\end{aligned}\n\\]\n\n\n5.3.5 The Cumulative Hazard Function\nSince \\(h(t) = \\frac{\\partial}{\\partial t}\\left\\{-\\text{log}\\left\\{S(t)\\right\\}\\right\\}\\) (see Theorem 5.5), we also have:\n\nCorollary 5.1 \\[S(t) = \\text{exp}\\left\\{-\\int_{u=0}^t h(u)du\\right\\} \\tag{5.2}\\]\n\n\n\nThe integral in Equation 5.2 is important enough to have its own name: cumulative hazard.\n\n\nDefinition 5.4 (cumulative hazard) The cumulative hazard function \\(H(t)\\) is defined as:\n\\[H(t) \\stackrel{\\text{def}}{=}\\int_{u=0}^t h(u) du\\]\n\nAs we will see below, \\(H(t)\\) is tractable to estimate, and we can then derive an estimate of the hazard function using an approximate derivative of the estimated cumulative hazard.\n\n\nExample 5.5 The cumulative hazard function of the exponential distribution family of models is:\n\\[\nH(t) = \\mathbb{1}_{t \\ge 0}\\cdot \\lambda t\n\\]\nFigure 5.3 shows some examples of exponential cumulative hazard functions.\n\n\n\n\n\n\nFigure 5.3: Examples of exponential cumulative hazard functions\n\n\n\n\n\n\n\n\n5.3.6 Some Key Mathematical Relationships among Survival Concepts\nDiagram:\n\\[\nh(t) \\xrightarrow[]{\\int_{u=0}^t h(u)du} H(t)\n\\xrightarrow[]{\\text{exp}\\left\\{-H(t)\\right\\}} S(t)\n\\xrightarrow[]{1-S(t)} F(t)\n\\]\n\\[\nh(t) \\xleftarrow[\\frac{\\partial}{\\partial t}H(t)]{} H(t)\n\\xleftarrow[-\\text{log}\\left\\{S(t)\\right\\}]{} S(t)\n\\xleftarrow[1-F(t)]{} F(t)\n\\]\nIdentities:\n\\[\n\\begin{aligned}\nS(t) &= 1 - F(t)\\\\\n&= \\text{exp}\\left\\{-H(t)\\right\\}\\\\\nS'(t) &= -f(t)\\\\\nH(t) &= -\\text{log}\\left\\{S(t)\\right\\}\\\\\nH'(t) &= h(t)\\\\\nh(t) &= \\frac{f(t)}{S(t)}\\\\\n&= -\\frac{\\partial}{\\partial t}\\text{log}\\left\\{S(t)\\right\\} \\\\\nf(t) &= h(t)\\cdot S(t)\\\\\n\\end{aligned}\n\\]\n\nSome proofs (others left as exercises):\n\\[\n\\begin{aligned}\nS'(t) &= \\frac{\\partial}{\\partial t}(1-F(t))\\\\\n&= -F'(t)\\\\\n&= -f(t)\\\\\n\\end{aligned}\n\\]\n\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial t}\\text{log}\\left\\{S(t)\\right\\}\n&= \\frac{S'(t)}{S(t)}\\\\\n&= -\\frac{f(t)}{S(t)}\\\\\n&= -h(t)\\\\\n\\end{aligned}\n\\]\n\n\\[\n\\begin{aligned}\nH(t)\n&\\stackrel{\\text{def}}{=}\\int_{u=0}^t h(u) du\\\\\n&= \\int_0^t -\\frac{\\partial}{\\partial u}\\text{log}\\left\\{S(u)\\right\\} du\\\\\n&= \\left[-\\text{log}\\left\\{S(u)\\right\\}\\right]_{u=0}^{u=t}\\\\\n&= \\left[\\text{log}\\left\\{S(u)\\right\\}\\right]_{u=t}^{u=0}\\\\\n&= \\text{log}\\left\\{S(0)\\right\\} - \\text{log}\\left\\{S(t)\\right\\}\\\\\n&= \\text{log}\\left\\{1\\right\\} - \\text{log}\\left\\{S(t)\\right\\}\\\\\n&= 0 - \\text{log}\\left\\{S(t)\\right\\}\\\\\n&=-\\text{log}\\left\\{S(t)\\right\\}\n\\end{aligned}\n\\]\n\nCorollary:\n\\[S(t) = \\text{exp}\\left\\{-H(t)\\right\\}\\]\nExample: Time to death the US in 2004\nThe first day is the most dangerous:\n\nShow R code# download `survexp.rda` from: \n# paste0(\n# \"https://github.com/therneau/survival/raw/\",\n# \"f3ac93704949ff26e07720b56f2b18ffa8066470/\",\n# \"data/survexp.rda\")\n\n#(newer versions of `survival` don't have the first-year breakdown; see:\n# https://cran.r-project.org/web/packages/survival/news.html)\n\nfs::path(\n  here::here(),\n  \"data\",\n  \"survexp.rda\") |&gt;\n  load()\ns1 &lt;- survexp.us[,\"female\",\"2004\"]\nage1 &lt;- c(\n  0.5/365.25,\n  4/365.25,\n  17.5/365.25,\n  196.6/365.25,\n  1:109+0.5)\ns2 &lt;- 365.25*s1[5:113]\ns2 &lt;- c(s1[1], 6*s1[2], 22*s1[3], 337.25*s1[4], s2)\ncols &lt;- rep(1,113)\ncols[1] &lt;- 2\ncols[2] &lt;- 3\ncols[3] &lt;- 4\n\nplot(age1,s1,type=\"b\",lwd=2,xlab=\"Age\",ylab=\"Daily Hazard Rate\",col=cols)\n\ntext(10,.003,\"First Day\",col=2)\ntext(18,.00030,\"Rest of First Week\",col=3)\ntext(18,.00015,\"Rest of First month\",col=4)\n\n\n\nFigure 5.4: Daily Hazard Rates in 2004 for US Females\n\n\n\n\n\n\n\n\n\nExercise 5.1 Hypothesize why the male and female hazard functions in Figure 5.5 differ where they do?\n\n\nShow R codeyrs=1:40\ns1 &lt;- survexp.us[5:113,\"male\",\"2004\"]\ns2 &lt;- survexp.us[5:113,\"female\",\"2004\"]\n\nage1 &lt;- 1:109\n\nplot(age1[yrs],s1[yrs],type=\"l\",lwd=2,xlab=\"Age\",ylab=\"Daily Hazard Rate\")\nlines(age1[yrs],s2[yrs],col=2,lwd=2)\nlegend(5,5e-6,c(\"Males\",\"Females\"),col=1:2,lwd=2)\n\n\n\nFigure 5.5: Daily Hazard Rates in 2004 for US Males and Females 1-40\n\n\n\n\n\n\n\n\n\nExercise 5.2 Compare and contrast Figure 5.6 with Figure 5.4.\n\n\n\nShow R codes1 &lt;- survexp.us[,\"female\",\"2004\"]\n\ns2 &lt;- 365.25*s1[5:113]\ns2 &lt;- c(s1[1], 6*s1[2], 21*s1[3], 337.25*s1[4], s2)\ncs2 &lt;- cumsum(s2)\nage2 &lt;- c(1/365.25, 7/365.25, 28/365.25, 1:110)\nplot(age2,exp(-cs2),type=\"l\",lwd=2,xlab=\"Age\",ylab=\"Survival\")\n\n\n\nFigure 5.6: Survival Curve in 2004 for US Females\n\n\n\n\n\n\n\n\n5.3.7 Likelihood with censoring\nIf an event time \\(T\\) is observed exactly as \\(T=t\\), then the likelihood of that observation is just its probability density function:\n\\[\n\\begin{aligned}\n\\mathcal L(t)\n&= p(T=t)\\\\\n&\\stackrel{\\text{def}}{=}f_T(t)\\\\\n&= h_T(t)S_T(t)\\\\\n\\ell(t)\n&\\stackrel{\\text{def}}{=}\\text{log}\\left\\{\\mathcal L(t)\\right\\}\\\\\n&= \\text{log}\\left\\{h_T(t)S_T(t)\\right\\}\\\\\n&= \\text{log}\\left\\{h_T(t)\\right\\} + \\text{log}\\left\\{S_T(t)\\right\\}\\\\\n&= \\text{log}\\left\\{h_T(t)\\right\\} - H_T(t)\\\\\n\\end{aligned}\n\\]\n\nIf instead the event time \\(T\\) is censored and only known to be after time \\(y\\), then the likelihood of that censored observation is instead the survival function evaluated at the censoring time:\n\\[\n\\begin{aligned}\n\\mathcal L(y)\n&=p_T(T&gt;y)\\\\\n&\\stackrel{\\text{def}}{=}S_T(y)\\\\\n\\ell(y)\n&\\stackrel{\\text{def}}{=}\\text{log}\\left\\{\\mathcal L(y)\\right\\}\\\\\n&=\\text{log}\\left\\{S(y)\\right\\}\\\\\n&=-H(y)\\\\\n\\end{aligned}\n\\]\n\n\nWhat’s written above is incomplete. We also observed whether or not the observation was censored. Let \\(C\\) denote the time when censoring would occur (if the event did not occur first); let \\(f_C(y)\\) and \\(S_C(y)\\) be the corresponding density and survival functions for the censoring event.\nLet \\(Y\\) denote the time when observation ended (either by censoring or by the event of interest occurring), and let \\(D\\) be an indicator variable for the event occurring at \\(Y\\) (so \\(D=0\\) represents a censored observation and \\(D=1\\) represents an uncensored observation). In other words, let \\(Y \\stackrel{\\text{def}}{=}\\min(T,C)\\) and \\(D \\stackrel{\\text{def}}{=}\\mathbb 1{\\{T&lt;=C\\}}\\).\nThen the complete likelihood of the observed data \\((Y,D)\\) is:\n\n\\[\n\\begin{aligned}\n\\mathcal L(y,d)\n&= p(Y=y, D=d)\\\\\n&= \\left[p(T=y,C&gt; y)\\right]^d \\cdot\n\\left[p(T&gt;y,C=y)\\right]^{1-d}\\\\\n\\end{aligned}\n\\]\n\n\nTypically, survival analyses assume that \\(C\\) and \\(T\\) are mutually independent; this assumption is called “non-informative” censoring.\nThen the joint likelihood \\(p(Y,D)\\) factors into the product \\(p(Y), p(D)\\), and the likelihood reduces to:\n\n\\[\n\\begin{aligned}\n\\mathcal L(y,d)\n&= \\left[p(T=y,C&gt; y)\\right]^d\\cdot\n\\left[p(T&gt;y,C=y)\\right]^{1-d}\\\\\n&= \\left[p(T=y)p(C&gt; y)\\right]^d\\cdot\n\\left[p(T&gt;y)p(C=y)\\right]^{1-d}\\\\\n&= \\left[f_T(y)S_C(y)\\right]^d\\cdot\n\\left[S(y)f_C(y)\\right]^{1-d}\\\\\n&= \\left[f_T(y)^d S_C(y)^d\\right]\\cdot\n\\left[S_T(y)^{1-d}f_C(y)^{1-d}\\right]\\\\\n&= \\left(f_T(y)^d \\cdot S_T(y)^{1-d}\\right)\\cdot\n\\left(f_C(y)^{1-d} \\cdot S_C(y)^{d}\\right)\n\\end{aligned}\n\\]\n\n\nThe corresponding log-likelihood is:\n\n\\[\n\\begin{aligned}\n\\ell(y,d)\n&= \\text{log}\\left\\{\\mathcal L(y,d) \\right\\}\\\\\n&= \\text{log}\\left\\{\n\\left(f_T(y)^d \\cdot S_T(y)^{1-d}\\right)\\cdot\n\\left(f_C(y)^{1-d} \\cdot S_C(y)^{d}\\right)\n\\right\\}\\\\\n&= \\text{log}\\left\\{\nf_T(y)^d \\cdot S_T(y)^{1-d}\n\\right\\}\n+\n\\text{log}\\left\\{\nf_C(y)^{1-d} \\cdot S_C(y)^{d}\n\\right\\}\\\\\n\\end{aligned}\n\\] Let\n\n\n\\(\\theta_T\\) represent the parameters of \\(p_T(t)\\),\n\n\\(\\theta_C\\) represent the parameters of \\(p_C(c)\\),\n\n\\(\\theta = (\\theta_T, \\theta_C)\\) be the combined vector of all parameters.\n\n\n\nThe corresponding score function is:\n\n\\[\n\\begin{aligned}\n\\ell'(y,d)\n&= \\frac{\\partial}{\\partial \\theta}\n\\left[\n\\text{log}\\left\\{\nf_T(y)^d \\cdot S_T(y)^{1-d}\n\\right\\}\n+\n\\text{log}\\left\\{\nf_C(y)^{1-d} \\cdot S_C(y)^{d}\n\\right\\}\n\\right]\\\\\n&=\n\\left(\n\\frac{\\partial}{\\partial \\theta}\n\\text{log}\\left\\{\nf_T(y)^d \\cdot S_T(y)^{1-d}\n\\right\\}\n\\right)\n+\n\\left(\n\\frac{\\partial}{\\partial \\theta}\n\\text{log}\\left\\{\nf_C(y)^{1-d} \\cdot S_C(y)^{d}\n\\right\\}\n\\right)\\\\\n\\end{aligned}\n\\]\n\n\nAs long as \\(\\theta_C\\) and \\(\\theta_T\\) don’t share any parameters, then if censoring is non-informative, the partial derivative with respect to \\(\\theta_T\\) is:\n\n\\[\n\\begin{aligned}\n\\ell'_{\\theta_T}(y,d)\n&\\stackrel{\\text{def}}{=}\\frac{\\partial}{\\partial \\theta_T}\\ell(y,d)\\\\\n&=\n\\left(\n\\frac{\\partial}{\\partial \\theta_T}\n\\text{log}\\left\\{\nf_T(y)^d \\cdot S_T(y)^{1-d}\n\\right\\}\n\\right)\n+\n\\left(\n\\frac{\\partial}{\\partial \\theta_T}\n\\text{log}\\left\\{\nf_C(y)^{1-d} \\cdot S_C(y)^{d}\n\\right\\}\n\\right)\\\\\n&=\n\\left(\n\\frac{\\partial}{\\partial \\theta_T}\n\\text{log}\\left\\{\nf_T(y)^d \\cdot S_T(y)^{1-d}\n\\right\\}\n\\right) + 0\\\\\n&=\n\\frac{\\partial}{\\partial \\theta_T}\n\\text{log}\\left\\{\nf_T(y)^d \\cdot S_T(y)^{1-d}\n\\right\\}\\\\\n\\end{aligned}\n\\]\n\n\nThus, the MLE for \\(\\theta_T\\) won’t depend on \\(\\theta_C\\), and we can ignore the distribution of \\(C\\) when estimating the parameters of \\(f_T(t)=p(T=t)\\).\n\nThen:\n\\[\n\\begin{aligned}\n\\mathcal L(y,d)\n&= f_T(y)^d \\cdot S_T(y)^{1-d}\\\\\n&= \\left(h_T(y)^d  S_T(y)^d\\right) \\cdot S_T(y)^{1-d}\\\\\n&= h_T(y)^d  \\cdot S_T(y)^d \\cdot S_T(y)^{1-d}\\\\\n&= h_T(y)^d \\cdot S_T(y)\\\\\n&= S_T(y) \\cdot h_T(y)^d \\\\\n\\end{aligned}\n\\]\n\nThat is, if the event occurred at time \\(y\\) (i.e., if \\(d=1\\)), then the likelihood of \\((Y,D) = (y,d)\\) is equal to the hazard function at \\(y\\) times the survival function at \\(y\\). Otherwise, the likelihood is equal to just the survival function at \\(y\\).\n\n\n\nThe corresponding log-likelihood is:\n\n\\[\n\\begin{aligned}\n\\ell(y,d)\n&=\\text{log}\\left\\{\\mathcal L(y,d)\\right\\}\\\\\n&= \\text{log}\\left\\{S_T(y) \\cdot h_T(y)^d\\right\\}\\\\\n&= \\text{log}\\left\\{S_T(y)\\right\\} + \\text{log}\\left\\{h_T(y)^d\\right\\}\\\\\n&= \\text{log}\\left\\{S_T(y)\\right\\} + d\\cdot \\text{log}\\left\\{h_T(y)\\right\\}\\\\\n&= -H_T(y) + d\\cdot \\text{log}\\left\\{h_T(y)\\right\\}\\\\\n\\end{aligned}\n\\]\n\nIn other words, the log-likelihood contribution from a single observation \\((Y,D) = (y,d)\\) is equal to the negative cumulative hazard at \\(y\\), plus the log of the hazard at \\(y\\) if the event occurred at time \\(y\\).",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Survival Analysis</span>"
    ]
  },
  {
    "objectID": "intro-to-survival-analysis.html#parametric-models-for-time-to-event-outcomes",
    "href": "intro-to-survival-analysis.html#parametric-models-for-time-to-event-outcomes",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.4 Parametric Models for Time-to-Event Outcomes",
    "text": "5.4 Parametric Models for Time-to-Event Outcomes\n\n5.4.1 Exponential Distribution\n\nThe exponential distribution is the base distribution for survival analysis.\nThe distribution has a constant hazard \\(\\lambda\\)\n\nThe mean survival time is \\(\\lambda^{-1}\\)\n\n\n\nMathematical details of exponential distribution\n\\[\n\\begin{aligned}\nf(t) &= \\lambda \\text{e}^{-\\lambda t}\\\\\nE(t) &= \\lambda^{-1}\\\\\nVar(t) &= \\lambda^{-2}\\\\\nF(t) &= 1-\\text{e}^{-\\lambda x}\\\\\nS(t)&= \\text{e}^{-\\lambda x}\\\\\n\\ln(S(t))&=-\\lambda x\\\\\nh(t) &= -\\frac{f(t)}{S(t)} = -\\frac{\\lambda \\text{e}^{-\\lambda t}}{\\text{e}^{-\\lambda t}}=\\lambda\n\\end{aligned}\n\\]\nEstimating \\(\\lambda\\)\n\n\nSuppose we have \\(m\\) exponential survival times of \\(t_1, t_2,\\ldots,t_m\\) and \\(k\\) right-censored values at \\(u_1,u_2,\\ldots,u_k\\).\nA survival time of \\(t_i=10\\) means that subject \\(i\\) died at time 10. A right-censored time \\(u_i=10\\) means that at time 10, subject \\(i\\) was still alive and that we have no further follow-up.\nFor the moment we will assume that the survival distribution is exponential and that all the subjects have the same parameter \\(\\lambda\\).\n\nWe have \\(m\\) exponential survival times of \\(t_1, t_2,\\ldots,t_m\\) and \\(k\\) right-censored values at \\(u_1,u_2,\\ldots,u_k\\). The log-likelihood of an observed survival time \\(t_i\\) is \\[\n\\text{log}\\left\\{\\lambda \\text{e}^{-\\lambda t_i}\\right\\} =\n\\text{log}\\left\\{\\lambda\\right\\}-\\lambda t_i\n\\] and the likelihood of a censored value is the probability of that outcome (survival greater than \\(u_j\\)) so the log-likelihood is\n\\[\n\\begin{aligned}\n\\ell_j(\\lambda) &= \\text{log}\\left\\{\\lambda \\text{e}^{u_j}\\right\\}\n\\\\ &= -\\lambda u_j\n\\end{aligned}\n\\]\n\n\nTheorem 5.6 Let \\(T=\\sum t_i\\) and \\(U=\\sum u_j\\). Then:\n\\[\n\\hat{\\lambda}_{ML} = \\frac{m}{T+U}\n\\tag{5.3}\\]\n\n\n\nProof. \\[\n\\begin{aligned}\n\\ell(\\lambda) &= \\sum_{i=1}^m( \\ln \\lambda-\\lambda t_i) + \\sum_{j=1}^k (-\\lambda u_j)\\\\\n&= m \\ln \\lambda -(T+U)\\lambda\\\\\n\\ell'(\\lambda)\n&=m\\lambda^{-1} -(T+U)\\\\\n\\hat{\\lambda} &= \\frac{m}{T+U}\n\\end{aligned}\n\\]\n\n\n\\[\n\\begin{aligned}\n\\ell''&=-m/\\lambda^2\\\\\n&&lt; 0\\\\\n\\hat E[T] &= \\hat\\lambda^{-1}\\\\\n&= \\frac{T+U}{m}\n\\end{aligned}\n\\]\nFisher Information and Standard Error\n\\[\n\\begin{aligned}\nE[-\\ell'']\n& = m/\\lambda^2\\\\\n\\text{Var}\\left(\\hat\\lambda\\right)\n&\\approx \\left(E[-\\ell'']\\right)^{-1}\\\\\n&=\\lambda^2/m\\\\\n\\text{SE}\\left(\\hat\\lambda\\right)\n&= \\sqrt{\\text{Var}\\left(\\hat\\lambda\\right)}\\\\\n&\\approx \\lambda/\\sqrt{m}\n\\end{aligned}\n\\]\n\n\\(\\hat\\lambda\\) depends on the censoring times of the censored observations, but \\(\\text{Var}\\left(\\hat\\lambda\\right)\\) only depends on the number of uncensored observations, \\(m\\), and not on the number of censored observations (\\(k\\)).\n\n\n5.4.2 Other Parametric Survival Distributions\n\nAny density on \\([0,\\infty)\\) can be a survival distribution, but the most useful ones are all skew right.\nThe most frequently used generalization of the exponential is the Weibull.\nOther common choices are the gamma, log-normal, log-logistic, Gompertz, inverse Gaussian, and Pareto.\nMost of what we do going forward is non-parametric or semi-parametric, but sometimes these parametric distributions provide a useful approach.",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Survival Analysis</span>"
    ]
  },
  {
    "objectID": "intro-to-survival-analysis.html#nonparametric-survival-analysis",
    "href": "intro-to-survival-analysis.html#nonparametric-survival-analysis",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.5 Nonparametric Survival Analysis",
    "text": "5.5 Nonparametric Survival Analysis\n\n5.5.1 Basic ideas\n\nMostly, we work without a parametric model.\nThe first task is to estimate a survival function from data listing survival times, and censoring times for censored data.\nFor example one patient may have relapsed at 10 months. Another might have been followed for 32 months without a relapse having occurred (censored).\nThe minimum information we need for each patient is a time and a censoring variable which is 1 if the event occurred at the indicated time and 0 if this is a censoring time.",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Survival Analysis</span>"
    ]
  },
  {
    "objectID": "intro-to-survival-analysis.html#example-clinical-trial-for-pediatric-acute-leukemia",
    "href": "intro-to-survival-analysis.html#example-clinical-trial-for-pediatric-acute-leukemia",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.6 Example: clinical trial for pediatric acute leukemia",
    "text": "5.6 Example: clinical trial for pediatric acute leukemia\n\n5.6.1 Overview of study\nThis is from a clinical trial in 1963 for 6-MP treatment vs. placebo for Acute Leukemia in 42 children.\n\n\nPairs of children:\n\nmatched by remission status at the time of treatment (remstat: 1 = partial, 2 = complete)\nrandomized to 6-MP (exit times in t2) or placebo (exit times in t1)\n\n\nFollowed until relapse or end of study.\nAll of the placebo group relapsed, but some of the 6-MP group were censored (which means they were still in remission); indicated by relapse variable (0 = censored, 1 = relapse).\n6-MP = 6-Mercaptopurine (Purinethol) is an anti-cancer (“antineoplastic” or “cytotoxic”) chemotherapy drug used currently for Acute lymphoblastic leukemia (ALL). It is classified as an antimetabolite.\n\n5.6.2 Study design\n\nClinical trial in 1963 for 6-MP treatment vs. placebo for Acute Leukemia in 42 children.\nPairs of children:\n\nmatched by remission status at the time of treatment (remstat)\n\n\nremstat = 1: partial\n\nremstat = 2: complete\n\n\nrandomized to 6-MP (exit time: t2) or placebo (t1).\n\n\nFollowed until relapse or end of study.\n\nAll of the placebo group relapsed,\nSome of the 6-MP group were censored.\n\n\n\n\n\n\nTable 5.1: drug6mp pediatric acute leukemia data\n\nShow R codelibrary(KMsurv)\ndata(drug6mp)\ndrug6mp = drug6mp |&gt; as_tibble() |&gt; print()\n#&gt; # A tibble: 21 × 5\n#&gt;     pair remstat    t1    t2 relapse\n#&gt;    &lt;int&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;\n#&gt;  1     1       1     1    10       1\n#&gt;  2     2       2    22     7       1\n#&gt;  3     3       2     3    32       0\n#&gt;  4     4       2    12    23       1\n#&gt;  5     5       2     8    22       1\n#&gt;  6     6       1    17     6       1\n#&gt;  7     7       2     2    16       1\n#&gt;  8     8       2    11    34       0\n#&gt;  9     9       2     8    32       0\n#&gt; 10    10       2    12    25       0\n#&gt; # ℹ 11 more rows\n\n\n\n\n\n5.6.3 Data documentation for drug6mp\n\n\nShow R code# library(printr) # inserts help-file output into markdown output\nlibrary(KMsurv)\n?drug6mp\n\n\n\n5.6.4 Descriptive Statistics\n\n\nTable 5.2: Summary statistics for drug6mp data\n\nShow R codesummary(drug6mp)\n#&gt;       pair       remstat           t1              t2          relapse     \n#&gt;  Min.   : 1   Min.   :1.00   Min.   : 1.00   Min.   : 6.0   Min.   :0.000  \n#&gt;  1st Qu.: 6   1st Qu.:2.00   1st Qu.: 4.00   1st Qu.: 9.0   1st Qu.:0.000  \n#&gt;  Median :11   Median :2.00   Median : 8.00   Median :16.0   Median :0.000  \n#&gt;  Mean   :11   Mean   :1.76   Mean   : 8.67   Mean   :17.1   Mean   :0.429  \n#&gt;  3rd Qu.:16   3rd Qu.:2.00   3rd Qu.:12.00   3rd Qu.:23.0   3rd Qu.:1.000  \n#&gt;  Max.   :21   Max.   :2.00   Max.   :23.00   Max.   :35.0   Max.   :1.000\n\n\n\n\n\n\nThe average time in each group is not useful. Some of the 6-MP patients have not relapsed at the time recorded, while all of the placebo patients have relapsed.\nThe median time is not really useful either because so many of the 6-MP patients have not relapsed (12/21).\nBoth are biased down in the 6-MP group. Remember that lower times are worse since they indicate sooner recurrence.\n\n\n\n5.6.5 Exponential model\n\n\nWe can compute the hazard rate, assuming an exponential model: number of relapses divided by the sum of the exit times (Equation 5.3).\n\n\n\\[\\hat\\lambda = \\frac{\\sum_{i=1}^nD_i}{\\sum_{i=1}^nY_i}\\]\n\n\nFor the placebo, that is just the reciprocal of the mean time:\n\n\n\\[\n\\begin{aligned}\n\\hat \\lambda_{\\text{placebo}}\n&= \\frac{\\sum_{i=1}^nD_i}{\\sum_{i=1}^nY_i}\n\\\\ &= \\frac{\\sum_{i=1}^n1}{\\sum_{i=1}^nY_i}\n\\\\ &= \\frac{n}{\\sum_{i=1}^nY_i}\n\\\\ &= \\frac{1}{\\bar{Y}}\n\\\\ &= \\frac{1}{8.6667}\n\\\\ &= 0.1154\n\\end{aligned}\n\\]\n\n\nFor the 6-MP group, \\(\\hat\\lambda = 9/359 = 0.025\\)\n\n\n\\[\n\\begin{aligned}\n\\hat \\lambda_{\\text{6-MP}}\n&= \\frac{\\sum_{i=1}^nD_i}{\\sum_{i=1}^nY_i}\n\\\\ &= \\frac{9}{359}\n\\\\ &= 0.0251\n\\end{aligned}\n\\]\n\nThe estimated hazard in the placebo group is 4.6 times as large as in the 6-MP group (assuming the hazard is constant over time).",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Survival Analysis</span>"
    ]
  },
  {
    "objectID": "intro-to-survival-analysis.html#the-kaplan-meier-product-limit-estimator",
    "href": "intro-to-survival-analysis.html#the-kaplan-meier-product-limit-estimator",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.7 The Kaplan-Meier Product Limit Estimator",
    "text": "5.7 The Kaplan-Meier Product Limit Estimator\n\n5.7.1 Estimating survival in datasets without censoring\n\nIn the drug6mp dataset, the estimated survival function for the placebo patients is easy to compute. For any time \\(t\\) in months, \\(S(t)\\) is the fraction of patients with times greater than \\(t\\):\n\n\n5.7.2 Estimating survival in datasets with censoring\n\nFor the 6-MP patients, we cannot ignore the censored data because we know that the time to relapse is greater than the censoring time.\nFor any time \\(t\\) in months, we know that 6-MP patients with times greater than \\(t\\) have not relapsed, and those with relapse time less than \\(t\\) have relapsed, but we don’t know if patients with censored time less than \\(t\\) have relapsed or not.\nThe procedure we usually use is the Kaplan-Meier product-limit estimator of the survival function.\nThe Kaplan-Meier estimator is a step function (like the empirical cdf), which changes value only at the event times, not at the censoring times.\nAt each event time \\(t\\), we compute the at-risk group size \\(Y\\), which is all those observations whose event time or censoring time is at least \\(t\\).\nIf \\(d\\) of the observations have an event time (not a censoring time) of \\(t\\), then the group of survivors immediately following time \\(t\\) is reduced by the fraction \\[\\frac{Y-d}{Y}=1-\\frac{d}{Y}\\]\n\n\n\nDefinition 5.5 (Kaplan-Meier Product-Limit Estimator of Survival Function) If the event times are \\(t_i\\) with events per time of \\(d_i\\) (\\(1\\le i \\le k\\)), then the Kaplan-Meier Product-Limit Estimator of the survival function is:\n\\[\\hat S(t) = \\prod_{t_i &lt; t} \\left[\\frac{1-d_i}{Y_i}\\right] \\tag{5.4}\\]\nwhere \\(Y_i\\) is the set of observations whose time (event or censored) is \\(\\ge t_i\\), the group at risk at time \\(t_i\\).\n\n\n\nTheorem 5.7 (Kaplan-Meier Estimate with No Censored Observations) If there are no censored data, and there are \\(n\\) data points, then just after (say) the third event time\n\\[\n\\begin{aligned}\n\\hat S(t)\n&= \\prod_{t_i &lt; t}\\left[1-\\frac{d_i}{Y_i}\\right]\n\\\\ &= \\left[\\frac{n-d_1}{n}\\right] \\left[\\frac{n-d_1-d_2}{n-d_1}\\right] \\left[\\frac{n-d_1-d_2-d_3}{n-d_1-d_2}\\right]\n\\\\ &= \\frac{n-d_1-d_2-d_3}{n}\n\\\\ &=1-\\frac{d_1+d_2+d_3}{n}\n\\\\ &=1-\\hat F(t)\n\\end{aligned}\n\\]\nwhere \\(\\hat F(t)\\) is the usual empirical CDF estimate.\n\n\n5.7.3 Kaplan-Meier curve for drug6mp data\nHere is the Kaplan-Meier estimated survival curve for the patients who received 6-MP in the drug6mp dataset (we will see code to produce figures like this one shortly):\n\nShow R code# | echo: false\n\nrequire(KMsurv)\ndata(drug6mp)\nlibrary(dplyr)\nlibrary(survival)\n\ndrug6mp_km_model1 = \n  drug6mp |&gt; \n  mutate(surv = Surv(t2, relapse))  |&gt; \n  survfit(formula = surv ~ 1, data = _)\n\nlibrary(ggfortify)\ndrug6mp_km_model1 |&gt;  \n  autoplot(\n    mark.time = TRUE,\n    conf.int = FALSE) +\n  expand_limits(y = 0) +\n  xlab('Time since diagnosis (months)') +\n  ylab(\"KM Survival Curve\")\n\n\n\nFigure 5.7: Kaplan-Meier Survival Curve for 6-MP Patients\n\n\n\n\n\n\n\n\n5.7.4 Kaplan-Meier calculations\nLet’s compute these estimates and build the chart by hand:\n\nShow R codelibrary(KMsurv)\nlibrary(dplyr)\ndata(drug6mp)\n\ndrug6mp.v2 = \n  drug6mp |&gt; \n  as_tibble() |&gt; \n  mutate(\n    remstat = remstat |&gt; \n      case_match(\n        1 ~ \"partial\",\n        2 ~ \"complete\"\n      ),\n    # renaming to \"outcome\" while relabeling is just a style choice:\n    outcome = relapse |&gt; \n      case_match(\n        0 ~ \"censored\",\n        1 ~ \"relapsed\"\n      )\n  )\n\nkm.6mp =\n  drug6mp.v2 |&gt; \n  summarize(\n    .by = t2,\n    Relapses = sum(outcome == \"relapsed\"),\n    Censored = sum(outcome == \"censored\")) |&gt;\n  # here we add a start time row, so the graph starts at time 0:\n  bind_rows(\n    tibble(\n      t2 = 0, \n      Relapses = 0, \n      Censored = 0)\n  ) |&gt; \n  # sort in time order:\n  arrange(t2) |&gt;\n  mutate(\n    Exiting = Relapses + Censored,\n    `Study Size` = sum(Exiting),\n    Exited = cumsum(Exiting) |&gt; dplyr::lag(default = 0),\n    `At Risk` = `Study Size` - Exited,\n    Hazard = Relapses / `At Risk`,\n    `KM Factor` = 1 - Hazard,\n    `Cumulative Hazard` = cumsum(`Hazard`),\n    `KM Survival Curve` = cumprod(`KM Factor`)\n  )\n\nlibrary(pander)  \npander(km.6mp)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nt2\nRelapses\nCensored\nExiting\nStudy Size\nExited\nAt Risk\nHazard\nKM Factor\nCumulative Hazard\nKM Survival Curve\n\n\n\n0\n0\n0\n0\n21\n0\n21\n0\n1\n0\n1\n\n\n6\n3\n1\n4\n21\n0\n21\n0.1429\n0.8571\n0.1429\n0.8571\n\n\n7\n1\n0\n1\n21\n4\n17\n0.05882\n0.9412\n0.2017\n0.8067\n\n\n9\n0\n1\n1\n21\n5\n16\n0\n1\n0.2017\n0.8067\n\n\n10\n1\n1\n2\n21\n6\n15\n0.06667\n0.9333\n0.2683\n0.7529\n\n\n11\n0\n1\n1\n21\n8\n13\n0\n1\n0.2683\n0.7529\n\n\n13\n1\n0\n1\n21\n9\n12\n0.08333\n0.9167\n0.3517\n0.6902\n\n\n16\n1\n0\n1\n21\n10\n11\n0.09091\n0.9091\n0.4426\n0.6275\n\n\n17\n0\n1\n1\n21\n11\n10\n0\n1\n0.4426\n0.6275\n\n\n19\n0\n1\n1\n21\n12\n9\n0\n1\n0.4426\n0.6275\n\n\n20\n0\n1\n1\n21\n13\n8\n0\n1\n0.4426\n0.6275\n\n\n22\n1\n0\n1\n21\n14\n7\n0.1429\n0.8571\n0.5854\n0.5378\n\n\n23\n1\n0\n1\n21\n15\n6\n0.1667\n0.8333\n0.7521\n0.4482\n\n\n25\n0\n1\n1\n21\n16\n5\n0\n1\n0.7521\n0.4482\n\n\n32\n0\n2\n2\n21\n17\n4\n0\n1\n0.7521\n0.4482\n\n\n34\n0\n1\n1\n21\n19\n2\n0\n1\n0.7521\n0.4482\n\n\n35\n0\n1\n1\n21\n20\n1\n0\n1\n0.7521\n0.4482\n\n\n\n\n\n\nSummary\nFor the 6-MP patients at time 6 months, there are 21 patients at risk. At \\(t=6\\) there are 3 relapses and 1 censored observations.\nThe Kaplan-Meier factor is \\((21-3)/21 = 0.857\\). The number at risk for the next time (\\(t=7\\)) is \\(21-3-1=17\\).\nAt time 7 months, there are 17 patients at risk. At \\(t=7\\) there is 1 relapse and 0 censored observations. The Kaplan-Meier factor is \\((17-1)/17 = 0.941\\). The Kaplan Meier estimate is \\(0.857\\times0.941=0.807\\). The number at risk for the next time (\\(t=9\\)) is \\(17-1=16\\).\n\nNow, let’s graph this estimated survival curve using ggplot():\n\nShow R codelibrary(ggplot2)\nconflicts_prefer(dplyr::filter)\nkm.6mp |&gt; \n  ggplot(aes(x = t2, y = `KM Survival Curve`)) +\n  geom_step() +\n  geom_point(data = km.6mp |&gt; filter(Censored &gt; 0), shape = 3) +\n  expand_limits(y = c(0,1), x = 0) +\n  xlab('Time since diagnosis (months)') +\n  ylab(\"KM Survival Curve\") +\n  scale_y_continuous(labels = scales::percent)\n\n\n\nFigure 5.8: KM curve for 6MP patients, calculated by hand",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Survival Analysis</span>"
    ]
  },
  {
    "objectID": "intro-to-survival-analysis.html#using-the-survival-package-in-r",
    "href": "intro-to-survival-analysis.html#using-the-survival-package-in-r",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.8 Using the survival package in R",
    "text": "5.8 Using the survival package in R\nWe don’t have to do these calculations by hand every time; the survival package and several others have functions available to automate many of these tasks (full list: https://cran.r-project.org/web/views/Survival.html).\n\n5.8.1 The Surv function\nTo use the survival package, the first step is telling R how to combine the exit time and exit reason (censoring versus event) columns. The Surv() function accomplishes this task.\nExample: Surv() with drug6mp data\n\nShow R codelibrary(survival)\ndrug6mp.v3 = \n  drug6mp.v2 |&gt; \n  mutate(\n    surv2 = Surv(\n      time = t2,\n      event = (outcome == \"relapsed\")))\n\nprint(drug6mp.v3)\n#&gt; # A tibble: 21 × 7\n#&gt;     pair remstat     t1    t2 relapse outcome   surv2\n#&gt;    &lt;int&gt; &lt;chr&gt;    &lt;int&gt; &lt;int&gt;   &lt;int&gt; &lt;chr&gt;    &lt;Surv&gt;\n#&gt;  1     1 partial      1    10       1 relapsed    10 \n#&gt;  2     2 complete    22     7       1 relapsed     7 \n#&gt;  3     3 complete     3    32       0 censored    32+\n#&gt;  4     4 complete    12    23       1 relapsed    23 \n#&gt;  5     5 complete     8    22       1 relapsed    22 \n#&gt;  6     6 partial     17     6       1 relapsed     6 \n#&gt;  7     7 complete     2    16       1 relapsed    16 \n#&gt;  8     8 complete    11    34       0 censored    34+\n#&gt;  9     9 complete     8    32       0 censored    32+\n#&gt; 10    10 complete    12    25       0 censored    25+\n#&gt; # ℹ 11 more rows\n\n\nThe output of Surv() is a vector of objects with class Surv. When we print this vector:\n\nobservations where the event was observed are printed as the event time (for example, surv2 = 10 on line 1)\nobservations where the event was right-censored are printed as the censoring time with a plus sign (+; for example, surv2 = 32+ on line 3).\n\n5.8.2 The survfit function\nOnce we have constructed our Surv variable, we can calculate the Kaplan-Meier estimate of the survival curve using the survfit() function.\n\n\n\n\n\n\nNote\n\n\n\nThe documentation for ?survfit isn’t too helpful; the survfit.formula documentation is better.\n\n\n\nExample: survfit() with drug6mp data\nHere we use survfit() to create a survfit object, which contains the Kaplan-Meier estimate:\n\nShow R codedrug6mp.km_model = survfit(\n  formula = surv2 ~ 1, \n  data = drug6mp.v3)\n\n\nprint.survfit() just gives some summary statistics:\n\nShow R codeprint(drug6mp.km_model)\n#&gt; Call: survfit(formula = surv2 ~ 1, data = drug6mp.v3)\n#&gt; \n#&gt;       n events median 0.95LCL 0.95UCL\n#&gt; [1,] 21      9     23      16      NA\n\n\nsummary.survfit() shows us the underlying Kaplan-Meier table:\n\nShow R codesummary(drug6mp.km_model)\n#&gt; Call: survfit(formula = surv2 ~ 1, data = drug6mp.v3)\n#&gt; \n#&gt;  time n.risk n.event survival std.err lower 95% CI upper 95% CI\n#&gt;     6     21       3    0.857  0.0764        0.720        1.000\n#&gt;     7     17       1    0.807  0.0869        0.653        0.996\n#&gt;    10     15       1    0.753  0.0963        0.586        0.968\n#&gt;    13     12       1    0.690  0.1068        0.510        0.935\n#&gt;    16     11       1    0.627  0.1141        0.439        0.896\n#&gt;    22      7       1    0.538  0.1282        0.337        0.858\n#&gt;    23      6       1    0.448  0.1346        0.249        0.807\n\n\n\nWe can specify which time points we want using the times argument:\n\nShow R codesummary(\n  drug6mp.km_model, \n  times = c(0, drug6mp.v3$t2))\n#&gt; Call: survfit(formula = surv2 ~ 1, data = drug6mp.v3)\n#&gt; \n#&gt;  time n.risk n.event survival std.err lower 95% CI upper 95% CI\n#&gt;     0     21       0    1.000  0.0000        1.000        1.000\n#&gt;     6     21       3    0.857  0.0764        0.720        1.000\n#&gt;     6     21       0    0.857  0.0764        0.720        1.000\n#&gt;     6     21       0    0.857  0.0764        0.720        1.000\n#&gt;     6     21       0    0.857  0.0764        0.720        1.000\n#&gt;     7     17       1    0.807  0.0869        0.653        0.996\n#&gt;     9     16       0    0.807  0.0869        0.653        0.996\n#&gt;    10     15       1    0.753  0.0963        0.586        0.968\n#&gt;    10     15       0    0.753  0.0963        0.586        0.968\n#&gt;    11     13       0    0.753  0.0963        0.586        0.968\n#&gt;    13     12       1    0.690  0.1068        0.510        0.935\n#&gt;    16     11       1    0.627  0.1141        0.439        0.896\n#&gt;    17     10       0    0.627  0.1141        0.439        0.896\n#&gt;    19      9       0    0.627  0.1141        0.439        0.896\n#&gt;    20      8       0    0.627  0.1141        0.439        0.896\n#&gt;    22      7       1    0.538  0.1282        0.337        0.858\n#&gt;    23      6       1    0.448  0.1346        0.249        0.807\n#&gt;    25      5       0    0.448  0.1346        0.249        0.807\n#&gt;    32      4       0    0.448  0.1346        0.249        0.807\n#&gt;    32      4       0    0.448  0.1346        0.249        0.807\n#&gt;    34      2       0    0.448  0.1346        0.249        0.807\n#&gt;    35      1       0    0.448  0.1346        0.249        0.807\n\n\n\n\nShow R code?summary.survfit\n\n\n\n5.8.3 Plotting estimated survival functions\nWe can plot survfit objects with plot(), autoplot(), or ggsurvplot():\n\nShow R codelibrary(ggfortify)\nautoplot(drug6mp.km_model)\n\n\nKaplan-Meier Survival Curve for 6-MP Patients\n\n\n\nShow R code\n# not shown:\n# plot(drug6mp.km_model)\n\n# library(survminer)\n# ggsurvplot(drug6mp.km_model)\n\n\n\nquantiles of survival curve\nWe can extract quantiles with quantile():\n\nShow R codedrug6mp.km_model |&gt; \n  quantile(p = c(.25, .5)) |&gt; \n  as_tibble() |&gt; \n  mutate(p = c(.25, .5)) |&gt; \n  relocate(p, .before = everything())",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Survival Analysis</span>"
    ]
  },
  {
    "objectID": "intro-to-survival-analysis.html#two-sample-tests",
    "href": "intro-to-survival-analysis.html#two-sample-tests",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.9 Two-sample tests",
    "text": "5.9 Two-sample tests\n\n5.9.1 The survdiff function\n\nShow R code?survdiff\n\n\n\n5.9.2 Example: survdiff() with drug6mp data\nNow we are going to compare the placebo and 6-MP data. We need to reshape the data to make it usable with the standard survival workflow:\n\nShow R codelibrary(survival)\nlibrary(tidyr)\ndrug6mp.v4 = \n  drug6mp.v3 |&gt;\n  select(pair, remstat, t1, t2, outcome) |&gt; \n  # here we are going to change the data from a wide format to long:\n  pivot_longer(\n    cols = c(t1, t2),\n    names_to = \"treatment\",\n    values_to = \"exit_time\") |&gt; \n  mutate(\n    treatment = treatment |&gt; \n      case_match(\n        \"t1\" ~ \"placebo\",\n        \"t2\" ~ \"6-MP\"\n      ),\n    outcome = if_else(\n      treatment == \"placebo\",\n      \"relapsed\",\n      outcome\n    ),\n    surv = Surv(\n      time = exit_time,\n      event = (outcome == \"relapsed\"))\n  )\n\n\n\nUsing this long data format, we can fit a Kaplan-Meier curve for each treatment group simultaneously:\n\nShow R codedrug6mp.km_model2 = \n   survfit(\n  formula = surv ~ treatment, \n  data = drug6mp.v4)\n\n\n\nWe can plot the curves in the same graph:\n\nShow R codedrug6mp.km_model2 |&gt; autoplot()\n\n\n\n\n\n\n\n\nWe can also perform something like a t-test, where the null hypothesis is that the curves are the same:\n\nShow R codesurvdiff(\n  formula = surv ~ treatment,\n  data = drug6mp.v4)\n#&gt; Call:\n#&gt; survdiff(formula = surv ~ treatment, data = drug6mp.v4)\n#&gt; \n#&gt;                    N Observed Expected (O-E)^2/E (O-E)^2/V\n#&gt; treatment=6-MP    21        9     19.3      5.46      16.8\n#&gt; treatment=placebo 21       21     10.7      9.77      16.8\n#&gt; \n#&gt;  Chisq= 16.8  on 1 degrees of freedom, p= 4e-05\n\n\nBy default, survdiff() ignores any pairing, but we can use strata() to perform something similar to a paired t-test:\n\nShow R codesurvdiff(\n  formula = surv ~ treatment + strata(pair),\n  data = drug6mp.v4)\n#&gt; Call:\n#&gt; survdiff(formula = surv ~ treatment + strata(pair), data = drug6mp.v4)\n#&gt; \n#&gt;                    N Observed Expected (O-E)^2/E (O-E)^2/V\n#&gt; treatment=6-MP    21        9     16.5      3.41      10.7\n#&gt; treatment=placebo 21       21     13.5      4.17      10.7\n#&gt; \n#&gt;  Chisq= 10.7  on 1 degrees of freedom, p= 0.001\n\n\nInterestingly, accounting for pairing reduces the significant of the difference.",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Survival Analysis</span>"
    ]
  },
  {
    "objectID": "intro-to-survival-analysis.html#example-bone-marrow-transplant-data",
    "href": "intro-to-survival-analysis.html#example-bone-marrow-transplant-data",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.10 Example: Bone Marrow Transplant Data",
    "text": "5.10 Example: Bone Marrow Transplant Data\nData from Copelan et al. (1991)\n\n\n\nFigure 5.9: Recovery process from a bone marrow transplant (Fig. 1.1 from Klein and Moeschberger (2003))\n\n\n\n\n\n\n\n5.10.1 Study design\nTreatment\n\n\nallogeneic (from a donor) bone marrow transplant therapy\n\nInclusion criteria\n\nacute myeloid leukemia (AML)\nacute lymphoblastic leukemia (ALL).\nPossible intermediate events\n\n\ngraft vs. host disease (GVHD): an immunological rejection response to the transplant\n\nplatelet recovery: a return of platelet count to normal levels.\n\nOne or the other, both in either order, or neither may occur.\nEnd point events\n\nrelapse of the disease\ndeath\n\nAny or all of these events may be censored.\n\n5.10.2 KMsurv::bmt data in R\n\nShow R codelibrary(KMsurv)\n?bmt\n\n\n\n5.10.3 Analysis plan\n\nWe concentrate for now on disease-free survival (t2 and d3) for the three risk groups, ALL, AML Low Risk, and AML High Risk.\nWe will construct the Kaplan-Meier survival curves, compare them, and test for differences.\nWe will construct the cumulative hazard curves and compare them.\nWe will estimate the hazard functions, interpret, and compare them.\n\n5.10.4 Survival Function Estimate and Variance\n\\[\\hat S(t) = \\prod_{t_i &lt; t}\\left[1-\\frac{d_i}{Y_i}\\right]\\] where \\(Y_i\\) is the group at risk at time \\(t_i\\).\nThe estimated variance of \\(\\hat S(t)\\) is:\n\nTheorem 5.8 (Greenwood’s estimator for variance of Kaplan-Meier survival estimator) \\[\n\\widehat{\\text{Var}}\\left(\\hat S(t)\\right) = \\hat S(t)^2\\sum_{t_i &lt;t}\\frac{d_i}{Y_i(Y_i-d_i)}\n\\tag{5.5}\\]\n\nWe can use Equation 5.5 for confidence intervals for a survival function or a difference of survival functions.\n\nKaplan-Meier survival curves\n\ncode to preprocess and model bmt datalibrary(KMsurv)\nlibrary(survival)\ndata(bmt)\n\nbmt = \n  bmt |&gt; \n  as_tibble() |&gt; \n  mutate(\n    group = \n      group |&gt; \n      factor(\n        labels = c(\"ALL\",\"Low Risk AML\",\"High Risk AML\")),\n    surv = Surv(t2,d3))\n\nkm_model1 = survfit(\n  formula = surv ~ group, \n  data = bmt)\n\n\n\nShow R codelibrary(ggfortify)\nautoplot(\n  km_model1, \n  conf.int = TRUE,\n  ylab = \"Pr(disease-free survival)\",\n  xlab = \"Time since transplant (days)\") + \n  theme_bw() +\n  theme(legend.position=\"bottom\")\n\n\nDisease-Free Survival by Disease Group\n\n\n\n\n\n5.10.5 Understanding Greenwood’s formula (optional)\n\nTo see where Greenwood’s formula comes from, let \\(x_i = Y_i - d_i\\). We approximate the solution treating each time as independent, with \\(Y_i\\) fixed and ignore randomness in times of failure and we treat \\(x_i\\) as independent binomials \\(\\text{Bin}(Y_i,p_i)\\). Letting \\(S(t)\\) be the “true” survival function\n\n\\[\n\\begin{aligned}\n\\hat S(t) &=\\prod_{t_i&lt;t}x_i/Y_i\\\\\nS(t)&=\\prod_{t_i&lt;t}p_i\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\frac{\\hat S(t)}{S(t)}\n   &= \\prod_{t_i&lt;t} \\frac{x_i}.    {p_iY_i}\n\\\\ &= \\prod_{t_i&lt;t} \\frac{\\hat p_i}{p_i}\n\\\\ &= \\prod_{t_i&lt;t} \\left(1+\\frac{\\hat p_i-p_i}{p_i}\\right)\n\\\\ &\\approx 1+\\sum_{t_i&lt;t} \\frac{\\hat p_i-p_i}{p_i}\n\\end{aligned}\n\\]\n\n\\[\n\\begin{aligned}\n\\text{Var}\\left(\\frac{\\hat S(t)}{S(t)}\\right)\n&\\approx \\text{Var}\\left(1+\\sum_{t_i&lt;t} \\frac{\\hat p_i-p_i}{p_i}\\right)\n\\\\ &=\\sum_{t_i&lt;t} \\frac{1}{p_i^2}\\frac{p_i(1-p_i)}{Y_i}\n\\\\ &= \\sum_{t_i&lt;t} \\frac{(1-p_i)}{p_iY_i}\n\\\\ &\\approx\\sum_{t_i&lt;t} \\frac{(1-x_i/Y_i)}{x_i}\n\\\\ &=\\sum_{t_i&lt;t} \\frac{Y_i-x_i}{x_iY_i}\n\\\\ &=\\sum_{t_i&lt;t} \\frac{d_i}{Y_i(Y_i-d_i)}\n\\\\ \\therefore\\text{Var}\\left(\\hat S(t)\\right)\n&\\approx \\hat S(t)^2\\sum_{t_i&lt;t} \\frac{d_i}{Y_i(Y_i-d_i)}\n\\end{aligned}\n\\]\n\n5.10.6 Test for differences among the disease groups\nHere we compute a chi-square test for assocation between disease group (group) and disease-free survival:\n\nShow R codesurvdiff(surv ~ group, data = bmt)\n#&gt; Call:\n#&gt; survdiff(formula = surv ~ group, data = bmt)\n#&gt; \n#&gt;                      N Observed Expected (O-E)^2/E (O-E)^2/V\n#&gt; group=ALL           38       24     21.9     0.211     0.289\n#&gt; group=Low Risk AML  54       25     40.0     5.604    11.012\n#&gt; group=High Risk AML 45       34     21.2     7.756    10.529\n#&gt; \n#&gt;  Chisq= 13.8  on 2 degrees of freedom, p= 0.001\n\n\n\n5.10.7 Cumulative Hazard\n\\[\n\\begin{aligned}\nh(t)\n&\\stackrel{\\text{def}}{=}P(T=t|T\\ge t)\\\\\n&= \\frac{p(T=t)}{P(T\\ge t)}\\\\\n&= -\\frac{\\partial}{\\partial t}\\text{log}\\left\\{S(t)\\right\\}\n\\end{aligned}\n\\]\nThe cumulative hazard (or integrated hazard) function is\n\\[H(t)\\stackrel{\\text{def}}{=}\\int_0^t h(t) dt\\] Since \\(h(t) = -\\frac{\\partial}{\\partial t}\\text{log}\\left\\{S(t)\\right\\}\\) as shown above, we have:\n\\[\nH(t)=-\\text{log}\\left\\{S\\right\\}(t)\n\\]\n\nSo we can estimate \\(H(t)\\) as:\n\\[\n\\begin{aligned}\n\\hat H(t)\n&= -\\text{log}\\left\\{\\hat S(t)\\right\\}\\\\\n&= -\\text{log}\\left\\{\\prod_{t_i &lt; t}\\left[1-\\frac{d_i}{Y_i}\\right]\\right\\}\\\\\n&= -\\sum_{t_i &lt; t}\\text{log}\\left\\{1-\\frac{d_i}{Y_i}\\right\\}\\\\\n\\end{aligned}\n\\]\nThis is the Kaplan-Meier (product-limit) estimate of cumulative hazard.\n\nExample: Cumulative Hazard Curves for Bone-Marrow Transplant (bmt) data\n\nShow R codeautoplot(\n  fun = \"cumhaz\",\n  km_model1, \n  conf.int = FALSE,\n  ylab = \"Cumulative hazard (disease-free survival)\",\n  xlab = \"Time since transplant (days)\") + \n  theme_bw() +\n  theme(legend.position=\"bottom\")\n\n\n\nFigure 5.10: Disease-Free Cumulative Hazard by Disease Group",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Survival Analysis</span>"
    ]
  },
  {
    "objectID": "intro-to-survival-analysis.html#nelson-aalen-estimates-of-cumulative-hazard-and-survival",
    "href": "intro-to-survival-analysis.html#nelson-aalen-estimates-of-cumulative-hazard-and-survival",
    "title": "\n5  Introduction to Survival Analysis\n",
    "section": "\n5.11 Nelson-Aalen Estimates of Cumulative Hazard and Survival",
    "text": "5.11 Nelson-Aalen Estimates of Cumulative Hazard and Survival\n\n\nDefinition 5.6 (Nelson-Aalen Cumulative Hazard Estimator)  \n\nThe point hazard at time \\(t_i\\) can be estimated by \\(d_i/Y_i\\), which leads to the Nelson-Aalen estimator of the cumulative hazard:\n\n\\[\\hat H_{NA}(t) \\stackrel{\\text{def}}{=}\\sum_{t_i &lt; t}\\frac{d_i}{Y_i} \\tag{5.6}\\]\n\n\n\nTheorem 5.9 (Variance of Nelson-Aalen estimator)  \n\nThe variance of this estimator is approximately:\n\n\\[\n\\begin{aligned}\n\\hat{\\text{Var}}\\left(\\hat H_{NA} (t)\\right)\n&= \\sum_{t_i &lt;t}\\frac{(d_i/Y_i)(1-d_i/Y_i)}{Y_i}\\\\\n&\\approx \\sum_{t_i &lt;t}\\frac{d_i}{Y_i^2}\n\\end{aligned}\n\\tag{5.7}\\]\n\n\nSince \\(S(t)=\\text{exp}\\left\\{-H(t)\\right\\}\\), the Nelson-Aalen cumulative hazard estimate can be converted into an alternate estimate of the survival function:\n\\[\n\\begin{aligned}\n\\hat S_{NA}(t)\n&= \\text{exp}\\left\\{-\\hat H_{NA}(t)\\right\\}\\\\\n&= \\text{exp}\\left\\{-\\sum_{t_i &lt; t}\\frac{d_i}{Y_i}\\right\\}\\\\\n&= \\prod_{t_i &lt; t}\\text{exp}\\left\\{-\\frac{d_i}{Y_i}\\right\\}\\\\\n\\end{aligned}\n\\]\n\nCompare these with the corresponding Kaplan-Meier estimates:\n\\[\n\\begin{aligned}\n\\hat H_{KM}(t) &= -\\sum_{t_i &lt; t}\\text{log}\\left\\{1-\\frac{d_i}{Y_i}\\right\\}\\\\\n\\hat S_{KM}(t) &= \\prod_{t_i &lt; t}\\left[1-\\frac{d_i}{Y_i}\\right]\n\\end{aligned}\n\\]\n\nThe product limit estimate and the Nelson-Aalen estimate often do not differ by much. The latter is considered more accurate in small samples and also directly estimates the cumulative hazard. The \"fleming-harrington\" method for survfit() reduces to Nelson-Aalen when the data are unweighted. We can also estimate the cumulative hazard as the negative log of the KM survival function estimate.\n\n\n5.11.1 Application to bmt dataset\n\nShow R code\nna_fit = survfit(\n  formula = surv ~ group,\n  type = \"fleming-harrington\",\n  data = bmt)\n\nkm_fit = survfit(\n  formula = surv ~ group,\n  type = \"kaplan-meier\",\n  data = bmt)\n\nkm_and_na = \n  bind_rows(\n    .id = \"model\",\n    \"Kaplan-Meier\" = km_fit |&gt; fortify(surv.connect = TRUE),\n    \"Nelson-Aalen\" = na_fit |&gt; fortify(surv.connect = TRUE)\n  ) |&gt; \n  as_tibble()\n\n\n\nShow R codekm_and_na |&gt; \n  ggplot(aes(x = time, y = surv, col = model)) +\n  geom_step() +\n  facet_grid(. ~ strata) +\n  theme_bw() + \n  ylab(\"S(t) = P(T&gt;=t)\") +\n  xlab(\"Survival time (t, days)\") +\n  theme(legend.position = \"bottom\")\n\n\nKaplan-Meier and Nelson-Aalen Survival Function Estimates, stratified by disease group\n\n\n\n\nThe Kaplan-Meier and Nelson-Aalen survival estimates are very similar for this dataset.\n\n\n\n\n\n\nCopelan, Edward A, James C Biggs, James M Thompson, Pamela Crilley, Jeff Szer, John P Klein, Neena Kapoor, Belinda R Avalos, Isabel Cunningham, and Kerry Atkinson. 1991. “Treatment for Acute Myelocytic Leukemia with Allogeneic Bone Marrow Transplantation Following Preparation with BuCy2.” https://doi.org/10.1182/blood.V78.3.838.838.\n\n\nKlein, John P, and Melvin L Moeschberger. 2003. Survival Analysis: Techniques for Censored and Truncated Data. Vol. 1230. Springer. https://link.springer.com/book/10.1007/b97377.",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Survival Analysis</span>"
    ]
  },
  {
    "objectID": "proportional-hazards-models.html",
    "href": "proportional-hazards-models.html",
    "title": "\n6  Proportional Hazards Models\n",
    "section": "",
    "text": "Configuring R\nFunctions from these packages will be used throughout this document:\nShow R codelibrary(conflicted) # check for conflicting function definitions\n# library(printr) # inserts help-file output into markdown output\nlibrary(rmarkdown) # Convert R Markdown documents into a variety of formats.\nlibrary(pander) # format tables for markdown\nlibrary(ggplot2) # graphics\nlibrary(ggeasy) # help with graphics\nlibrary(ggfortify) # help with graphics\nlibrary(dplyr) # manipulate data\nlibrary(tibble) # `tibble`s extend `data.frame`s\nlibrary(magrittr) # `%&gt;%` and other additional piping tools\nlibrary(haven) # import Stata files\nlibrary(knitr) # format R output for markdown\nlibrary(tidyr) # Tools to help to create tidy data\nlibrary(plotly) # interactive graphics\nlibrary(dobson) # datasets from Dobson and Barnett 2018\nlibrary(parameters) # format model output tables for markdown\nlibrary(haven) # import Stata files\nlibrary(latex2exp) # use LaTeX in R code (for figures and tables)\nlibrary(fs) # filesystem path manipulations\nlibrary(survival) # survival analysis\nlibrary(survminer) # survival analysis graphics\nlibrary(KMsurv) # datasets from Klein and Moeschberger\nlibrary(parameters) # format model output tables for\nlibrary(webshot2) # convert interactive content to static for pdf\nlibrary(forcats) # functions for categorical variables (\"factors\")\nlibrary(stringr) # functions for dealing with strings\nlibrary(lubridate) # functions for dealing with dates and times\nHere are some R settings I use in this document:\nShow R coderm(list = ls()) # delete any data that's already loaded into R\n\nconflicts_prefer(dplyr::filter)\nggplot2::theme_set(\n  ggplot2::theme_bw() + \n        # ggplot2::labs(col = \"\") +\n    ggplot2::theme(\n      legend.position = \"bottom\",\n      text = ggplot2::element_text(size = 12, family = \"serif\")))\n\nknitr::opts_chunk$set(message = FALSE)\noptions('digits' = 4)\n\npanderOptions(\"big.mark\", \",\")\npander::panderOptions(\"table.emphasize.rownames\", FALSE)\npander::panderOptions(\"table.split.table\", Inf)\nconflicts_prefer(dplyr::filter) # use the `filter()` function from dplyr() by default\nlegend_text_size = 9",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "proportional-hazards-models.html#introduction",
    "href": "proportional-hazards-models.html#introduction",
    "title": "\n6  Proportional Hazards Models\n",
    "section": "\n6.1 Introduction",
    "text": "6.1 Introduction\n\n\nRecall: the exponential distribution has constant hazard:\n\n\\[\n\\begin{aligned}\nf(t) &= \\lambda e^{-\\lambda t}\\\\\nS(t) &= e^{-\\lambda t}\\\\\nh(t) &= \\lambda\n\\end{aligned}\n\\]\n\n\nLet’s make two generalizations. First, we let the hazard depend on some covariates \\(x_1,x_2, \\dots, x_p\\); we will indicate this dependence by extending our notation for hazard:\n\n\\[h(t|\\tilde{x}) \\stackrel{\\text{def}}{=}\\text{p}(T=t|T\\ge t, \\tilde{X}= \\tilde{x})\\]\n\n\nDefinition 6.1 (baseline hazard)  \n\nThe baseline hazard, base hazard, or reference hazard, denoted \\(h_0(t)\\) or \\(\\lambda_0(t)\\), is the hazard function for the subpopulation of individuals whose covariates are all equal to their reference levels:\n\n\\[h_0(t) \\stackrel{\\text{def}}{=}h(t | \\tilde{X}= \\tilde{0})\\]\n\n\nSimilarly:\n\nDefinition 6.2 (baseline cumulative hazard)  \n\nThe baseline cumulative hazard, base cumulative hazard, or reference cumulative hazard, denoted \\(H_0(t)\\) or \\(\\Lambda_0(t)\\), is the cumulative hazard function for the subpopulation of individuals whose covariates are all equal to their reference levels:\n\n\\[H_0(t) \\stackrel{\\text{def}}{=}H(t | \\tilde{X}= \\tilde{0})\\]\n\n\nAlso:\n\nDefinition 6.3 (Baseline survival function) The baseline survival function is the survival function for an individual whose covariates are all equal to their default values.\n\\[S_0(t) \\stackrel{\\text{def}}{=}S(t | \\tilde{X}= \\tilde{0})\\]\n\n\n\nAs the second generalization, we let the base hazard, cumulative hazard, and survival functions depend on \\(t\\), but not on the covariates (for now). We can do this using either parametric or semi-parametric approaches.",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "proportional-hazards-models.html#coxs-proportional-hazards-model",
    "href": "proportional-hazards-models.html#coxs-proportional-hazards-model",
    "title": "\n6  Proportional Hazards Models\n",
    "section": "\n6.2 Cox’s Proportional Hazards Model",
    "text": "6.2 Cox’s Proportional Hazards Model\n\n\nThe generalization is that the hazard function is:\n\n\\[\n\\begin{aligned}\nh(t|x)&= h_0(t)\\theta(x)\\\\\n\\theta(x) &= \\text{exp}\\left\\{\\eta(x)\\right\\}\\\\\n\\eta(x) &= \\tilde{x}^{\\top}\\tilde{\\beta}\\\\\n&\\stackrel{\\text{def}}{=}\\beta_1x_1+\\cdots+\\beta_px_p\n\\end{aligned}\n\\]\n\nThe relationship between \\(h(t|x)\\) and \\(\\eta(x)\\) is typically modeled using a log link, as in a generalized linear model; that is:\n\n\\[\\text{log}\\left\\{h(t|x)\\right\\} = \\text{log}\\left\\{h_0(t)\\right\\} + \\eta(x)\\]\n\n\nThis model is semi-parametric, because the linear predictor depends on estimated parameters but the base hazard function is unspecified. There is no constant term in \\(\\eta(x)\\), because it is absorbed in the base hazard.\n\n\nAlternatively, we could define \\(\\beta_0(t) = \\text{log}\\left\\{h_0(t)\\right\\}\\), and then:\n\\[\\eta(x,t) = \\beta_0(t) + \\beta_1x_1+\\cdots+\\beta_px_p\\]\n\n\nFor two different individuals with covariate patterns \\(\\boldsymbol x_1\\) and \\(\\boldsymbol x_2\\), the ratio of the hazard functions (a.k.a. hazard ratio, a.k.a. relative hazard) is:\n\n\\[\n\\begin{aligned}\n\\frac{h(t|\\boldsymbol x_1)}{h(t|\\boldsymbol x_2)}\n&=\\frac{h_0(t)\\theta(\\boldsymbol x_1)}{h_0(t)\\theta(\\boldsymbol x_2)}\\\\\n&=\\frac{\\theta(\\boldsymbol x_1)}{\\theta(\\boldsymbol x_2)}\\\\\n\\end{aligned}\n\\]\n\nUnder the proportional hazards model, this ratio (a.k.a. proportion) does not depend on \\(t\\). This property is a structural limitation of the model; it is called the proportional hazards assumption.\n\n\n\nDefinition 6.4 (proportional hazards) A conditional probability distribution \\(p(T|X)\\) has proportional hazards if the hazard ratio \\(h(t|\\boldsymbol x_1)/h(t|\\boldsymbol x_2)\\) does not depend on \\(t\\). Mathematically, it can be written as:\n\\[\n\\frac{h(t|\\boldsymbol x_1)}{h(t|\\boldsymbol x_2)}\n= \\theta(\\boldsymbol x_1,\\boldsymbol x_2)\n\\]\n\n\nAs we saw above, Cox’s proportional hazards model has this property, with \\(\\theta(\\boldsymbol x_1,\\boldsymbol x_2) = \\frac{\\theta(\\boldsymbol x_1)}{\\theta(\\boldsymbol x_2)}\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe are using two similar notations, \\(\\theta(\\boldsymbol x_1,\\boldsymbol x_2)\\) and \\(\\theta(\\boldsymbol x)\\). We can link these notations if we define \\(\\theta(\\boldsymbol x) \\stackrel{\\text{def}}{=}\\theta(\\boldsymbol x, \\boldsymbol 0)\\) and \\(\\theta(\\boldsymbol 0) = 1\\).\n\n\n\nThe proportional hazards model also has additional notable properties:\n\\[\n\\begin{aligned}\n\\frac{h(t|\\boldsymbol x_1)}{h(t|\\boldsymbol x_2)}\n&=\\frac{\\theta(\\boldsymbol x_1)}{\\theta(\\boldsymbol x_2)}\\\\\n&=\\frac{\\text{exp}\\left\\{\\eta(\\boldsymbol x_1)\\right\\}}{\\text{exp}\\left\\{\\eta(\\boldsymbol x_2)\\right\\}}\\\\\n&=\\text{exp}\\left\\{\\eta(\\boldsymbol x_1)-\\eta(\\boldsymbol x_2)\\right\\}\\\\\n&=\\text{exp}\\left\\{\\boldsymbol x_1'\\beta-\\boldsymbol x_2'\\beta\\right\\}\\\\\n&=\\text{exp}\\left\\{(\\boldsymbol x_1 - \\boldsymbol x_2)'\\beta\\right\\}\\\\\n\\end{aligned}\n\\]\n\nHence on the log scale, we have:\n\\[\n\\begin{aligned}\n\\text{log}\\left\\{\\frac{h(t|\\boldsymbol x_1)}{h(t|\\boldsymbol x_2)}\\right\\}\n&=\\eta(\\boldsymbol x_1)-\\eta(\\boldsymbol x_2)\\\\\n&= \\boldsymbol x_1'\\beta-\\boldsymbol x_2'\\beta\\\\\n&= (\\boldsymbol x_1 - \\boldsymbol x_2)'\\beta\n\\end{aligned}\n\\]\n\nIf only one covariate \\(x_j\\) is changing, then:\n\\[\n\\begin{aligned}\n\\text{log}\\left\\{\\frac{h(t|\\boldsymbol x_1)}{h(t|\\boldsymbol x_2)}\\right\\}\n&=  (x_{1j} - x_{2j}) \\cdot \\beta_j\\\\\n&\\propto (x_{1j} - x_{2j})\n\\end{aligned}\n\\]\n\nThat is, under Cox’s model \\(h(t|\\boldsymbol x) = h_0(t)\\text{exp}\\left\\{\\boldsymbol x'\\beta\\right\\}\\), the log of the hazard ratio is proportional to the difference in \\(x_j\\), with the proportionality coefficient equal to \\(\\beta_j\\).\n\n\nFurther,\n\\[\n\\begin{aligned}\n\\text{log}\\left\\{h(t|\\boldsymbol x)\\right\\}\n&=\\text{log}\\left\\{h_0(t)\\right\\}  + x'\\beta\n\\end{aligned}\n\\]\n\nThat is, the covariate effects are additive on the log-hazard scale; hazard functions for different covariate patterns should be vertical shifts of each other.\nSee also:\nhttps://en.wikipedia.org/wiki/Proportional_hazards_model#Why_it_is_called_%22proportional%22\n\n\n6.2.1 Additional properties of the proportional hazards model\nIf \\(h(t|x)= h_0(t)\\theta(x)\\), then:\n\nTheorem 6.1 (Cumulative hazards are also proportional to \\(H_0(t)\\)) \\[\n\\begin{aligned}\nH(t|x)\n&\\stackrel{\\text{def}}{=}\\int_{u=0}^t h(u)du\\\\\n&= \\int_{u=0}^t h_0(u)\\theta(x)du\\\\\n&= \\theta(x)\\int_{u=0}^t h_0(u)du\\\\\n&= \\theta(x)H_0(t)\n\\end{aligned}\n\\]\nwhere \\(H_0(t) \\stackrel{\\text{def}}{=}H(t|0) = \\int_{u=0}^t h_0(u)du\\).\n\n\n\nTheorem 6.2 (The logarithms of cumulative hazard should be parallel) \\[\n\\text{log}\\left\\{H(t|\\tilde{x})\\right\\} =\\text{log}\\left\\{H_0(t)\\right\\}  + \\tilde{x}'\\tilde{\\beta}\n\\]\n\n\n\nTheorem 6.3 (Survival functions are exponential multiples of \\(S_0(t)\\)) \\[\n\\begin{aligned}\nS(t|x)\n&= \\text{exp}\\left\\{-H(t|x)\\right\\}\\\\\n&= \\text{exp}\\left\\{-\\theta(x)\\cdot H_0(t)\\right\\}\\\\\n&= \\left(\\text{exp}\\left\\{- H_0(t)\\right\\}\\right)^{\\theta(x)}\\\\\n&= \\left[S_0(t)\\right]^{\\theta(x)}\\\\\n\\end{aligned}\n\\]\n\n\n6.2.2 Testing the proportional hazards assumption\n\nThe Nelson-Aalen estimate of the cumulative hazard is usually used for estimates of the hazard and often the cumulative hazard.\nIf the hazards of the three groups are proportional, that means that the ratio of the hazards is constant over \\(t\\). We can test this using the ratios of the estimated cumulative hazards, which also would be proportional, as shown above.\n\n\nShow R code\nlibrary(KMsurv)\nlibrary(survival)\nlibrary(dplyr)\ndata(bmt)\n\nbmt = \n  bmt |&gt; \n  as_tibble() |&gt; \n  mutate(\n    group = \n      group |&gt; \n      factor(\n        labels = c(\"ALL\",\"Low Risk AML\",\"High Risk AML\")))\n\nnafit = survfit(\n  formula = Surv(t2,d3) ~ group,\n  type = \"fleming-harrington\",\n  data = bmt)\n\nbmt_curves = tibble(timevec = 1:1000)\nsf1 &lt;- with(nafit[1], stepfun(time,c(1,surv)))\nsf2 &lt;- with(nafit[2], stepfun(time,c(1,surv)))\nsf3 &lt;- with(nafit[3], stepfun(time,c(1,surv)))\n\nbmt_curves = \n  bmt_curves |&gt; \n  mutate(\n    cumhaz1 = -log(sf1(timevec)),\n    cumhaz2 = -log(sf2(timevec)),\n    cumhaz3 = -log(sf3(timevec)))\n\n\n\nShow R codelibrary(ggplot2)\nbmt_rel_hazard_plot = \n  bmt_curves |&gt; \n  ggplot(\n    aes(\n      x = timevec,\n      y = cumhaz1/cumhaz2)\n  ) +\n  geom_line(aes(col = \"ALL/Low Risk AML\")) + \n  ylab(\"Hazard Ratio\") +\n  xlab(\"Time\") + \n  ylim(0,6) +\n  geom_line(aes(y = cumhaz3/cumhaz1, col = \"High Risk AML/ALL\")) +\n  geom_line(aes(y = cumhaz3/cumhaz2, col = \"High Risk AML/Low Risk AML\")) +\n  theme_bw() +\n  labs(colour = \"Comparison\") +\n  theme(legend.position=\"bottom\")\n\nprint(bmt_rel_hazard_plot)\n\n\n\nFigure 6.1: Hazard Ratios by Disease Group for bmt data\n\n\n\n\n\n\n\n\nWe can zoom in on 30-300 days to take a closer look:\n\nShow R codebmt_rel_hazard_plot + xlim(c(30,300))\n\n\n\nFigure 6.2: Hazard Ratios by Disease Group (30-300 Days)\n\n\n\n\n\n\n\n\nThe cumulative hazard curves should also be proportional\n\nShow R codelibrary(ggfortify)\nplot_cuhaz_bmt =\n  bmt |&gt;\n  survfit(formula = Surv(t2, d3) ~ group) |&gt;\n  autoplot(fun = \"cumhaz\",\n           mark.time = TRUE) + \n  ylab(\"Cumulative hazard\")\n  \nplot_cuhaz_bmt |&gt; print()\n\n\n\nFigure 6.3: Disease-Free Cumulative Hazard by Disease Group\n\n\n\n\n\n\n\n\n\nShow R codeplot_cuhaz_bmt + \n  scale_y_log10() +\n  scale_x_log10()\n\n\n\nFigure 6.4: Disease-Free Cumulative Hazard by Disease Group (log-scale)\n\n\n\n\n\n\n\n\n6.2.3 Smoothed hazard functions\n\nThe Nelson-Aalen estimate of the cumulative hazard is usually used for estimates of the hazard. Since the hazard is the derivative of the cumulative hazard, we need a smooth estimate of the cumulative hazard, which is provided by smoothing the step-function cumulative hazard.\nThe R package muhaz handles this for us. What we are looking for is whether the hazard function is more or less the same shape, increasing, decreasing, constant, etc. Are the hazards “proportional”?\n\n\nShow R codelibrary(muhaz)\n\nmuhaz(bmt$t2,bmt$d3,bmt$group==\"High Risk AML\") |&gt; plot(lwd=2,col=3)\nmuhaz(bmt$t2,bmt$d3,bmt$group==\"ALL\") |&gt; lines(lwd=2,col=1)\nmuhaz(bmt$t2,bmt$d3,bmt$group==\"Low Risk AML\") |&gt; lines(lwd=2,col=2)\nlegend(\"topright\",c(\"ALL\",\"Low Risk AML\",\"High Risk AML\"),col=1:3,lwd=2)\n\n\nSmoothed Hazard Rate Estimates by Disease Group\n\n\n\n\n\nGroup 3 was plotted first because it has the highest hazard.\nExcept for an initial blip in the high risk AML group, the hazards look roughly proportional. They are all strongly decreasing.\n\n\n6.2.4 Fitting the Proportional Hazards Model\n\nHow do we fit a proportional hazards regression model? We need to estimate the coefficients of the covariates, and we need to estimate the base hazard \\(h_0(t)\\). For the covariates, supposing for simplicity that there are no tied event times, let the event times for the whole data set be \\(t_1, t_2,\\ldots,t_D\\). Let the risk set at time \\(t_i\\) be \\(R(t_i)\\) and\n\n\\[\n\\begin{aligned}\n\\eta(\\boldsymbol{x}) &= \\beta_1x_{1}+\\cdots+\\beta_p x_{p}\\\\\n\\theta(\\boldsymbol{x}) &= e^{\\eta(\\boldsymbol{x})}\\\\\nh(t|X=x)&= h_0(t)e^{\\eta(\\boldsymbol{x})}=\\theta(\\boldsymbol{x}) h_0(t)\n\\end{aligned}\n\\]\n\nConditional on a single failure at time \\(t\\), the probability that the event is due to subject \\(f\\in R(t)\\) is approximately\n\\[\n\\begin{aligned}\n\\Pr(f \\text{ fails}|\\text{1 failure at } t)\n&= \\frac{h_0(t)e^{\\eta(\\boldsymbol{x}_f)}}{\\sum_{k \\in R(t)}h_0(t)e^{\\eta(\\boldsymbol{x}_f)}}\\\\\n&=\\frac{\\theta(\\boldsymbol{x}_f)}{\\sum_{k \\in R(t)} \\theta(\\boldsymbol{x}_k)}\n\\end{aligned}\n\\]\nThe logic behind this has several steps. We first fix (ex post) the failure times and note that in this discrete context, the probability \\(p_j\\) that a subject \\(j\\) in the risk set fails at time \\(t\\) is just the hazard of that subject at that time.\nIf all of the \\(p_j\\) are small, the chance that exactly one subject fails is\n\\[\n\\sum_{k\\in R(t)}p_k\\left[\\prod_{m\\in R(t), m\\ne k} (1-p_m)\\right]\\approx\\sum_{k\\in R(t)}p_k\n\\]\nIf subject \\(i\\) is the one who experiences the event of interest at time \\(t_i\\), then the partial likelihood is\n\\[\n\\mathcal L^*(\\beta|T)=\n\\prod_i \\frac{\\theta(x_i)}{\\sum_{k \\in R(t_i)} \\theta(\\boldsymbol{x}_k)}\n\\]\nand we can numerically maximize this with respect to the coefficients \\(\\boldsymbol{\\beta}\\) that specify \\(\\eta(\\boldsymbol{x}) = \\boldsymbol{x}'\\boldsymbol{\\beta}\\). When there are tied event times adjustments need to be made, but the likelihood is still similar. Note that we don’t need to know the base hazard to solve for the coefficients.\nOnce we have coefficient estimates \\(\\hat{\\boldsymbol{\\beta}} =(\\hat \\beta_1,\\ldots,\\hat\\beta_p)\\), this also defines \\(\\hat\\eta(x)\\) and \\(\\hat\\theta(x)\\) and then the estimated base cumulative hazard function is \\[\\hat H(t)=\n\\sum_{t_i &lt; t} \\frac{d_i}{\\sum_{k\\in R(t_i)} \\theta(x_k)}\\] which reduces to the Nelson-Aalen estimate when there are no covariates. There are numerous other estimates that have been proposed as well.",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "proportional-hazards-models.html#cox-model-for-the-bmt-data",
    "href": "proportional-hazards-models.html#cox-model-for-the-bmt-data",
    "title": "\n6  Proportional Hazards Models\n",
    "section": "\n6.3 Cox Model for the bmt data",
    "text": "6.3 Cox Model for the bmt data\n\n6.3.1 Fit the model\n\nShow R code\nbmt.cox &lt;- coxph(Surv(t2, d3) ~ group, data = bmt)\nsummary(bmt.cox)\n#&gt; Call:\n#&gt; coxph(formula = Surv(t2, d3) ~ group, data = bmt)\n#&gt; \n#&gt;   n= 137, number of events= 83 \n#&gt; \n#&gt;                      coef exp(coef) se(coef)     z Pr(&gt;|z|)  \n#&gt; groupLow Risk AML  -0.574     0.563    0.287 -2.00    0.046 *\n#&gt; groupHigh Risk AML  0.383     1.467    0.267  1.43    0.152  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;                    exp(coef) exp(-coef) lower .95 upper .95\n#&gt; groupLow Risk AML      0.563      1.776     0.321     0.989\n#&gt; groupHigh Risk AML     1.467      0.682     0.869     2.478\n#&gt; \n#&gt; Concordance= 0.625  (se = 0.03 )\n#&gt; Likelihood ratio test= 13.4  on 2 df,   p=0.001\n#&gt; Wald test            = 13  on 2 df,   p=0.001\n#&gt; Score (logrank) test = 13.8  on 2 df,   p=0.001\n\n\nThe table provides hypothesis tests comparing groups 2 and 3 to group 1. Group 3 has the highest hazard, so the most significant comparison is not directly shown.\nThe coefficient 0.3834 is on the log-hazard-ratio scale, as in log-risk-ratio. The next column gives the hazard ratio 1.4673, and a hypothesis (Wald) test.\nThe (not shown) group 3 vs. group 2 log hazard ratio is 0.3834 + 0.5742 = 0.9576. The hazard ratio is then exp(0.9576) or 2.605.\nInference on all coefficients and combinations can be constructed using coef(bmt.cox) and vcov(bmt.cox) as with logistic and poisson regression.\nConcordance is agreement of first failure between pairs of subjects and higher predicted risk between those subjects, omitting non-informative pairs.\nThe Rsquare value is Cox and Snell’s pseudo R-squared and is not very useful.\n\n6.3.2 Tests for nested models\nsummary() prints three tests for whether the model with the group covariate is better than the one without\n\n\nLikelihood ratio test (chi-squared)\n\nWald test (also chi-squared), obtained by adding the squares of the z-scores\n\nScore = log-rank test, as with comparison of survival functions.\n\nThe likelihood ratio test is probably best in smaller samples, followed by the Wald test.\n\n6.3.3 Survival Curves from the Cox Model\nWe can take a look at the resulting group-specific curves:\n\nShow R code\n#| fig-cap: \"Survival Functions for Three Groups by KM and Cox Model\"\n\nkm_fit = survfit(Surv(t2, d3) ~ group, data = as.data.frame(bmt))\n\ncox_fit = survfit(\n  bmt.cox, \n  newdata = \n    data.frame(\n      group = unique(bmt$group), \n      row.names = unique(bmt$group)))\n\nlibrary(survminer)\n\nlist(KM = km_fit, Cox = cox_fit) |&gt; \n  survminer::ggsurvplot(\n    # facet.by = \"group\",\n    legend = \"bottom\", \n    legend.title = \"\",\n    combine = TRUE, \n    fun = 'pct', \n    size = .5,\n    ggtheme = theme_bw(), \n    conf.int = FALSE, \n    censor = FALSE) |&gt; \n  suppressWarnings() # ggsurvplot() throws some warnings that aren't too worrying\n\n\n\n\n\n\n\nWhen we use survfit() with a Cox model, we have to specify the covariate levels we are interested in; the argument newdata should include a data.frame with the same named columns as the predictors in the Cox model and one or more levels of each.\n\nFrom ?survfit.coxph:\n\nIf the newdata argument is missing, a curve is produced for a single “pseudo” subject with covariate values equal to the means component of the fit. The resulting curve(s) almost never make sense, but the default remains due to an unwarranted attachment to the option shown by some users and by other packages. Two particularly egregious examples are factor variables and interactions. Suppose one were studying interspecies transmission of a virus, and the data set has a factor variable with levels (“pig”, “chicken”) and about equal numbers of observations for each. The “mean” covariate level will be 0.5 – is this a flying pig?\n\n\n6.3.4 Examining survfit\n\n\nShow R codesurvfit(Surv(t2, d3) ~ group, data = bmt)\n#&gt; Call: survfit(formula = Surv(t2, d3) ~ group, data = bmt)\n#&gt; \n#&gt;                      n events median 0.95LCL 0.95UCL\n#&gt; group=ALL           38     24    418     194      NA\n#&gt; group=Low Risk AML  54     25   2204     704      NA\n#&gt; group=High Risk AML 45     34    183     115     456\n\n\n\nShow R codesurvfit(Surv(t2, d3) ~ group, data = bmt) |&gt; summary()\n#&gt; Call: survfit(formula = Surv(t2, d3) ~ group, data = bmt)\n#&gt; \n#&gt;                 group=ALL \n#&gt;  time n.risk n.event survival std.err lower 95% CI upper 95% CI\n#&gt;     1     38       1    0.974  0.0260        0.924        1.000\n#&gt;    55     37       1    0.947  0.0362        0.879        1.000\n#&gt;    74     36       1    0.921  0.0437        0.839        1.000\n#&gt;    86     35       1    0.895  0.0498        0.802        0.998\n#&gt;   104     34       1    0.868  0.0548        0.767        0.983\n#&gt;   107     33       1    0.842  0.0592        0.734        0.966\n#&gt;   109     32       1    0.816  0.0629        0.701        0.949\n#&gt;   110     31       1    0.789  0.0661        0.670        0.930\n#&gt;   122     30       2    0.737  0.0714        0.609        0.891\n#&gt;   129     28       1    0.711  0.0736        0.580        0.870\n#&gt;   172     27       1    0.684  0.0754        0.551        0.849\n#&gt;   192     26       1    0.658  0.0770        0.523        0.827\n#&gt;   194     25       1    0.632  0.0783        0.495        0.805\n#&gt;   230     23       1    0.604  0.0795        0.467        0.782\n#&gt;   276     22       1    0.577  0.0805        0.439        0.758\n#&gt;   332     21       1    0.549  0.0812        0.411        0.734\n#&gt;   383     20       1    0.522  0.0817        0.384        0.709\n#&gt;   418     19       1    0.494  0.0819        0.357        0.684\n#&gt;   466     18       1    0.467  0.0818        0.331        0.658\n#&gt;   487     17       1    0.439  0.0815        0.305        0.632\n#&gt;   526     16       1    0.412  0.0809        0.280        0.605\n#&gt;   609     14       1    0.382  0.0803        0.254        0.577\n#&gt;   662     13       1    0.353  0.0793        0.227        0.548\n#&gt; \n#&gt;                 group=Low Risk AML \n#&gt;  time n.risk n.event survival std.err lower 95% CI upper 95% CI\n#&gt;    10     54       1    0.981  0.0183        0.946        1.000\n#&gt;    35     53       1    0.963  0.0257        0.914        1.000\n#&gt;    48     52       1    0.944  0.0312        0.885        1.000\n#&gt;    53     51       1    0.926  0.0356        0.859        0.998\n#&gt;    79     50       1    0.907  0.0394        0.833        0.988\n#&gt;    80     49       1    0.889  0.0428        0.809        0.977\n#&gt;   105     48       1    0.870  0.0457        0.785        0.965\n#&gt;   211     47       1    0.852  0.0483        0.762        0.952\n#&gt;   219     46       1    0.833  0.0507        0.740        0.939\n#&gt;   248     45       1    0.815  0.0529        0.718        0.925\n#&gt;   272     44       1    0.796  0.0548        0.696        0.911\n#&gt;   288     43       1    0.778  0.0566        0.674        0.897\n#&gt;   381     42       1    0.759  0.0582        0.653        0.882\n#&gt;   390     41       1    0.741  0.0596        0.633        0.867\n#&gt;   414     40       1    0.722  0.0610        0.612        0.852\n#&gt;   421     39       1    0.704  0.0621        0.592        0.837\n#&gt;   481     38       1    0.685  0.0632        0.572        0.821\n#&gt;   486     37       1    0.667  0.0642        0.552        0.805\n#&gt;   606     36       1    0.648  0.0650        0.533        0.789\n#&gt;   641     35       1    0.630  0.0657        0.513        0.773\n#&gt;   704     34       1    0.611  0.0663        0.494        0.756\n#&gt;   748     33       1    0.593  0.0669        0.475        0.739\n#&gt;  1063     26       1    0.570  0.0681        0.451        0.720\n#&gt;  1074     25       1    0.547  0.0691        0.427        0.701\n#&gt;  2204      6       1    0.456  0.1012        0.295        0.704\n#&gt; \n#&gt;                 group=High Risk AML \n#&gt;  time n.risk n.event survival std.err lower 95% CI upper 95% CI\n#&gt;     2     45       1    0.978  0.0220        0.936        1.000\n#&gt;    16     44       1    0.956  0.0307        0.897        1.000\n#&gt;    32     43       1    0.933  0.0372        0.863        1.000\n#&gt;    47     42       2    0.889  0.0468        0.802        0.986\n#&gt;    48     40       1    0.867  0.0507        0.773        0.972\n#&gt;    63     39       1    0.844  0.0540        0.745        0.957\n#&gt;    64     38       1    0.822  0.0570        0.718        0.942\n#&gt;    74     37       1    0.800  0.0596        0.691        0.926\n#&gt;    76     36       1    0.778  0.0620        0.665        0.909\n#&gt;    80     35       1    0.756  0.0641        0.640        0.892\n#&gt;    84     34       1    0.733  0.0659        0.615        0.875\n#&gt;    93     33       1    0.711  0.0676        0.590        0.857\n#&gt;   100     32       1    0.689  0.0690        0.566        0.838\n#&gt;   105     31       1    0.667  0.0703        0.542        0.820\n#&gt;   113     30       1    0.644  0.0714        0.519        0.801\n#&gt;   115     29       1    0.622  0.0723        0.496        0.781\n#&gt;   120     28       1    0.600  0.0730        0.473        0.762\n#&gt;   157     27       1    0.578  0.0736        0.450        0.742\n#&gt;   162     26       1    0.556  0.0741        0.428        0.721\n#&gt;   164     25       1    0.533  0.0744        0.406        0.701\n#&gt;   168     24       1    0.511  0.0745        0.384        0.680\n#&gt;   183     23       1    0.489  0.0745        0.363        0.659\n#&gt;   242     22       1    0.467  0.0744        0.341        0.638\n#&gt;   268     21       1    0.444  0.0741        0.321        0.616\n#&gt;   273     20       1    0.422  0.0736        0.300        0.594\n#&gt;   318     19       1    0.400  0.0730        0.280        0.572\n#&gt;   363     18       1    0.378  0.0723        0.260        0.550\n#&gt;   390     17       1    0.356  0.0714        0.240        0.527\n#&gt;   422     16       1    0.333  0.0703        0.221        0.504\n#&gt;   456     15       1    0.311  0.0690        0.201        0.481\n#&gt;   467     14       1    0.289  0.0676        0.183        0.457\n#&gt;   625     13       1    0.267  0.0659        0.164        0.433\n#&gt;   677     12       1    0.244  0.0641        0.146        0.409\n\n\n\nShow R codesurvfit(bmt.cox)\n#&gt; Call: survfit(formula = bmt.cox)\n#&gt; \n#&gt;        n events median 0.95LCL 0.95UCL\n#&gt; [1,] 137     83    422     268      NA\nsurvfit(bmt.cox, newdata = tibble(group = unique(bmt$group)))\n#&gt; Call: survfit(formula = bmt.cox, newdata = tibble(group = unique(bmt$group)))\n#&gt; \n#&gt;     n events median 0.95LCL 0.95UCL\n#&gt; 1 137     83    422     268      NA\n#&gt; 2 137     83     NA     625      NA\n#&gt; 3 137     83    268     162     467\n\n\n\nShow R codebmt.cox |&gt; \n  survfit(newdata = tibble(group = unique(bmt$group))) |&gt; \n  summary()\n#&gt; Call: survfit(formula = bmt.cox, newdata = tibble(group = unique(bmt$group)))\n#&gt; \n#&gt;  time n.risk n.event survival1 survival2 survival3\n#&gt;     1    137       1     0.993     0.996     0.989\n#&gt;     2    136       1     0.985     0.992     0.978\n#&gt;    10    135       1     0.978     0.987     0.968\n#&gt;    16    134       1     0.970     0.983     0.957\n#&gt;    32    133       1     0.963     0.979     0.946\n#&gt;    35    132       1     0.955     0.975     0.935\n#&gt;    47    131       2     0.941     0.966     0.914\n#&gt;    48    129       2     0.926     0.957     0.893\n#&gt;    53    127       1     0.918     0.953     0.882\n#&gt;    55    126       1     0.911     0.949     0.872\n#&gt;    63    125       1     0.903     0.944     0.861\n#&gt;    64    124       1     0.896     0.940     0.851\n#&gt;    74    123       2     0.881     0.931     0.830\n#&gt;    76    121       1     0.873     0.926     0.819\n#&gt;    79    120       1     0.865     0.922     0.809\n#&gt;    80    119       2     0.850     0.913     0.788\n#&gt;    84    117       1     0.843     0.908     0.778\n#&gt;    86    116       1     0.835     0.903     0.768\n#&gt;    93    115       1     0.827     0.899     0.757\n#&gt;   100    114       1     0.820     0.894     0.747\n#&gt;   104    113       1     0.812     0.889     0.737\n#&gt;   105    112       2     0.797     0.880     0.717\n#&gt;   107    110       1     0.789     0.875     0.707\n#&gt;   109    109       1     0.782     0.870     0.697\n#&gt;   110    108       1     0.774     0.866     0.687\n#&gt;   113    107       1     0.766     0.861     0.677\n#&gt;   115    106       1     0.759     0.856     0.667\n#&gt;   120    105       1     0.751     0.851     0.657\n#&gt;   122    104       2     0.735     0.841     0.637\n#&gt;   129    102       1     0.727     0.836     0.627\n#&gt;   157    101       1     0.720     0.831     0.617\n#&gt;   162    100       1     0.712     0.826     0.607\n#&gt;   164     99       1     0.704     0.821     0.598\n#&gt;   168     98       1     0.696     0.815     0.588\n#&gt;   172     97       1     0.688     0.810     0.578\n#&gt;   183     96       1     0.680     0.805     0.568\n#&gt;   192     95       1     0.672     0.800     0.558\n#&gt;   194     94       1     0.664     0.794     0.549\n#&gt;   211     93       1     0.656     0.789     0.539\n#&gt;   219     92       1     0.648     0.783     0.530\n#&gt;   230     90       1     0.640     0.778     0.520\n#&gt;   242     89       1     0.632     0.773     0.511\n#&gt;   248     88       1     0.624     0.767     0.501\n#&gt;   268     87       1     0.616     0.761     0.492\n#&gt;   272     86       1     0.608     0.756     0.482\n#&gt;   273     85       1     0.600     0.750     0.473\n#&gt;   276     84       1     0.592     0.745     0.464\n#&gt;   288     83       1     0.584     0.739     0.454\n#&gt;   318     82       1     0.576     0.733     0.445\n#&gt;   332     81       1     0.568     0.727     0.436\n#&gt;   363     80       1     0.560     0.722     0.427\n#&gt;   381     79       1     0.552     0.716     0.418\n#&gt;   383     78       1     0.544     0.710     0.409\n#&gt;   390     77       2     0.528     0.698     0.392\n#&gt;   414     75       1     0.520     0.692     0.383\n#&gt;   418     74       1     0.512     0.686     0.374\n#&gt;   421     73       1     0.504     0.680     0.366\n#&gt;   422     72       1     0.496     0.674     0.357\n#&gt;   456     71       1     0.488     0.667     0.349\n#&gt;   466     70       1     0.480     0.661     0.340\n#&gt;   467     69       1     0.472     0.655     0.332\n#&gt;   481     68       1     0.464     0.649     0.324\n#&gt;   486     67       1     0.455     0.642     0.315\n#&gt;   487     66       1     0.447     0.636     0.307\n#&gt;   526     65       1     0.439     0.629     0.299\n#&gt;   606     63       1     0.431     0.623     0.291\n#&gt;   609     62       1     0.423     0.616     0.283\n#&gt;   625     61       1     0.415     0.609     0.275\n#&gt;   641     60       1     0.407     0.603     0.267\n#&gt;   662     59       1     0.399     0.596     0.260\n#&gt;   677     58       1     0.391     0.589     0.252\n#&gt;   704     57       1     0.383     0.582     0.244\n#&gt;   748     56       1     0.374     0.575     0.237\n#&gt;  1063     47       1     0.365     0.567     0.228\n#&gt;  1074     46       1     0.356     0.559     0.220\n#&gt;  2204      9       1     0.313     0.520     0.182",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "proportional-hazards-models.html#adjustment-for-ties-optional",
    "href": "proportional-hazards-models.html#adjustment-for-ties-optional",
    "title": "\n6  Proportional Hazards Models\n",
    "section": "\n6.4 Adjustment for Ties (optional)",
    "text": "6.4 Adjustment for Ties (optional)\n\nAt each time \\(t_i\\) at which more than one of the subjects has an event, let \\(d_i\\) be the number of events at that time, \\(D_i\\) the set of subjects with events at that time, and let \\(s_i\\) be a covariate vector for an artificial subject obtained by adding up the covariate values for the subjects with an event at time \\(t_i\\). Let \\[\\bar\\eta_i = \\beta_1s_{i1}+\\cdots+\\beta_ps_{ip}\\] and \\(\\bar\\theta_i = \\text{exp}\\left\\{\\bar\\eta_i\\right\\}\\).\nLet \\(s_i\\) be a covariate vector for an artificial subject obtained by adding up the covariate values for the subjects with an event at time \\(t_i\\). Note that\n\\[\n\\begin{aligned}\n\\bar\\eta_i &=\\sum_{j \\in D_i}\\beta_1x_{j1}+\\cdots+\\beta_px_{jp}\\\\\n&= \\beta_1s_{i1}+\\cdots+\\beta_ps_{ip}\\\\\n\\bar\\theta_i &= \\text{exp}\\left\\{\\bar\\eta_i\\right\\}\\\\\n&= \\prod_{j \\in D_i}\\theta_i\n\\end{aligned}\n\\]\nBreslow’s method for ties\nBreslow’s method estimates the partial likelihood as\n\\[\n\\begin{aligned}\nL(\\beta|T) &=\n\\prod_i \\frac{\\bar\\theta_i}{[\\sum_{k \\in R(t_i)} \\theta_k]^{d_i}}\\\\\n&= \\prod_i \\prod_{j \\in D_i}\\frac{\\theta_j}{\\sum_{k \\in R(t_i)} \\theta_k}\n\\end{aligned}\n\\]\nThis method is equivalent to treating each event as distinct and using the non-ties formula. It works best when the number of ties is small. It is the default in many statistical packages, including PROC PHREG in SAS.\nEfron’s method for ties\nThe other common method is Efron’s, which is the default in R.\n\\[L(\\beta|T)=\n\\prod_i \\frac{\\bar\\theta_i}{\\prod_{j=1}^{d_i}[\\sum_{k \\in R(t_i)} \\theta_k-\\frac{j-1}{d_i}\\sum_{k \\in D_i} \\theta_k]}\\] This is closer to the exact discrete partial likelihood when there are many ties.\nThe third option in R (and an option also in SAS as discrete) is the “exact” method, which is the same one used for matched logistic regression.\nExample: Breslow’s method\nSuppose as an example we have a time \\(t\\) where there are 20 individuals at risk and three failures. Let the three individuals have risk parameters \\(\\theta_1, \\theta_2, \\theta_3\\) and let the sum of the risk parameters of the remaining 17 individuals be \\(\\theta_R\\). Then the factor in the partial likelihood at time \\(t\\) using Breslow’s method is\n\n\\[\n\\left(\\frac{\\theta_1}{\\theta_R+\\theta_1+\\theta_2+\\theta_3}\\right)\n\\left(\\frac{\\theta_2}{\\theta_R+\\theta_1+\\theta_2+\\theta_3}\\right)\n\\left(\\frac{\\theta_3}{\\theta_R+\\theta_1+\\theta_2+\\theta_3}\\right)\n\\]\n\nIf on the other hand, they had died in the order 1,2, 3, then the contribution to the partial likelihood would be:\n\n\\[\n\\left(\\frac{\\theta_1}{\\theta_R+\\theta_1+\\theta_2+\\theta_3}\\right)\n\\left(\\frac{\\theta_2}{\\theta_R+\\theta_2+\\theta_3}\\right)\n\\left(\\frac{\\theta_3}{\\theta_R+\\theta_3}\\right)\n\\]\n\nas the risk set got smaller with each failure. The exact method roughly averages the results for the six possible orderings of the failures.\nExample: Efron’s method\nBut we don’t know the order they failed in, so instead of reducing the denominator by one risk coefficient each time, we reduce it by the same fraction. This is Efron’s method.\n\n\\[\\left(\\frac{\\theta_1}{\\theta_R+\\theta_1+\\theta_2+\\theta_3}\\right)\n\\left(\\frac{\\theta_2}{\\theta_R+2(\\theta_1+\\theta_2+\\theta_3)/3}\\right)\n\\left(\\frac{\\theta_3}{\\theta_R+(\\theta_1+\\theta_2+\\theta_3)/3}\\right)\\]",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "proportional-hazards-models.html#building-cox-proportional-hazards-models",
    "href": "proportional-hazards-models.html#building-cox-proportional-hazards-models",
    "title": "\n6  Proportional Hazards Models\n",
    "section": "\n6.5 Building Cox Proportional Hazards models",
    "text": "6.5 Building Cox Proportional Hazards models\n\n6.5.1 hodg Lymphoma Data Set from KMsurv\n\nParticipants\n43 bone marrow transplant patients at Ohio State University (Avalos 1993)\nVariables\n\n\ndtype: Disease type (Hodgkin’s or non-Hodgkins lymphoma)\n\ngtype: Bone marrow graft type:\nallogeneic: from HLA-matched sibling\nautologous: from self (prior to chemo)\n\ntime: time to study exit\n\ndelta: study exit reason (death/relapse vs censored)\n\nwtime: waiting time to transplant (in months)\n\nscore: Karnofsky score:\n80–100: Able to carry on normal activity and to work; no special care needed.\n50–70: Unable to work; able to live at home and care for most personal needs; varying amount of assistance needed.\n10–60: Unable to care for self; requires equivalent of institutional or hospital care; disease may be progressing rapidly.\nData\n\nShow R code\ndata(hodg, package = \"KMsurv\")\nhodg2 = hodg |&gt; \n  as_tibble() |&gt; \n  mutate(\n    # We add factor labels to the categorical variables:\n    gtype = gtype |&gt; \n      case_match(\n        1 ~ \"Allogenic\",\n        2 ~ \"Autologous\"),\n    dtype = dtype |&gt; \n      case_match(\n        1 ~ \"Non-Hodgkins\",\n        2 ~ \"Hodgkins\") |&gt; \n      factor() |&gt; \n      relevel(ref = \"Non-Hodgkins\"), \n    delta = delta |&gt; \n      case_match(\n        1 ~ \"dead\",\n        0 ~ \"alive\"),\n    surv = Surv(\n      time = time, \n      event = delta == \"dead\")\n  )\nhodg2 |&gt; print()\n#&gt; # A tibble: 43 × 7\n#&gt;    gtype     dtype         time delta score wtime   surv\n#&gt;    &lt;chr&gt;     &lt;fct&gt;        &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;Surv&gt;\n#&gt;  1 Allogenic Non-Hodgkins    28 dead     90    24    28 \n#&gt;  2 Allogenic Non-Hodgkins    32 dead     30     7    32 \n#&gt;  3 Allogenic Non-Hodgkins    49 dead     40     8    49 \n#&gt;  4 Allogenic Non-Hodgkins    84 dead     60    10    84 \n#&gt;  5 Allogenic Non-Hodgkins   357 dead     70    42   357 \n#&gt;  6 Allogenic Non-Hodgkins   933 alive    90     9   933+\n#&gt;  7 Allogenic Non-Hodgkins  1078 alive   100    16  1078+\n#&gt;  8 Allogenic Non-Hodgkins  1183 alive    90    16  1183+\n#&gt;  9 Allogenic Non-Hodgkins  1560 alive    80    20  1560+\n#&gt; 10 Allogenic Non-Hodgkins  2114 alive    80    27  2114+\n#&gt; # ℹ 33 more rows\n\n\n\n6.5.2 Proportional hazards model\n\nShow R code\nhodg.cox1 = coxph(\n  formula = surv ~ gtype * dtype + score + wtime, \n  data = hodg2)\n\nsummary(hodg.cox1)\n#&gt; Call:\n#&gt; coxph(formula = surv ~ gtype * dtype + score + wtime, data = hodg2)\n#&gt; \n#&gt;   n= 43, number of events= 26 \n#&gt; \n#&gt;                                  coef exp(coef) se(coef)     z Pr(&gt;|z|)    \n#&gt; gtypeAutologous                0.6394    1.8953   0.5937  1.08   0.2815    \n#&gt; dtypeHodgkins                  2.7603   15.8050   0.9474  2.91   0.0036 ** \n#&gt; score                         -0.0495    0.9517   0.0124 -3.98  6.8e-05 ***\n#&gt; wtime                         -0.0166    0.9836   0.0102 -1.62   0.1046    \n#&gt; gtypeAutologous:dtypeHodgkins -2.3709    0.0934   1.0355 -2.29   0.0220 *  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;                               exp(coef) exp(-coef) lower .95 upper .95\n#&gt; gtypeAutologous                  1.8953     0.5276    0.5920     6.068\n#&gt; dtypeHodgkins                   15.8050     0.0633    2.4682   101.207\n#&gt; score                            0.9517     1.0507    0.9288     0.975\n#&gt; wtime                            0.9836     1.0167    0.9641     1.003\n#&gt; gtypeAutologous:dtypeHodgkins    0.0934    10.7074    0.0123     0.711\n#&gt; \n#&gt; Concordance= 0.776  (se = 0.059 )\n#&gt; Likelihood ratio test= 32.1  on 5 df,   p=6e-06\n#&gt; Wald test            = 27.2  on 5 df,   p=5e-05\n#&gt; Score (logrank) test = 37.7  on 5 df,   p=4e-07",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "proportional-hazards-models.html#diagnostic-graphs-for-proportional-hazards-assumption",
    "href": "proportional-hazards-models.html#diagnostic-graphs-for-proportional-hazards-assumption",
    "title": "\n6  Proportional Hazards Models\n",
    "section": "\n6.6 Diagnostic graphs for proportional hazards assumption",
    "text": "6.6 Diagnostic graphs for proportional hazards assumption\n\n6.6.1 Analysis plan\n\n\nsurvival function for the four combinations of disease type and graft type.\n\nobserved (nonparametric) vs. expected (semiparametric) survival functions.\n\ncomplementary log-log survival for the four groups.\n\n6.6.2 Kaplan-Meier survival functions\n\nShow R codekm_model = survfit(\n  formula = surv ~ dtype + gtype,\n  data = hodg2)\n\nkm_model |&gt; \n  autoplot(conf.int = FALSE) +\n  theme_bw() +\n  theme(\n    legend.position=\"bottom\",\n    legend.title = element_blank(),\n    legend.text = element_text(size = legend_text_size)\n  ) +\n  guides(col=guide_legend(ncol=2)) +\n  ylab('Survival probability, S(t)') +\n  xlab(\"Time since transplant (days)\")\n\n\nKaplan-Meier Survival Curves for HOD/NHL and Allo/Auto Grafts\n\n\n\n\n\n6.6.3 Observed and expected survival curves\n\nShow R code# we need to create a tibble of covariate patterns;\n# we will set score and wtime to mean values for disease and graft types:\nmeans = hodg2 |&gt; \n  summarize(\n    .by = c(dtype, gtype), \n    score = mean(score), \n    wtime = mean(wtime)) |&gt; \n  arrange(dtype, gtype) |&gt; \n  mutate(strata = paste(dtype, gtype, sep = \",\")) |&gt; \n  as.data.frame() \n\n# survfit.coxph() will use the rownames of its `newdata`\n# argument to label its output:\nrownames(means) = means$strata\n\ncox_model = \n  hodg.cox1 |&gt; \n  survfit(\n    data = hodg2, # ggsurvplot() will need this\n    newdata = means)\n\n\n\nShow R code# I couldn't find a good function to reformat `cox_model` for ggplot, \n# so I made my own:\nstack_surv_ph = function(cox_model)\n{\n  cox_model$surv |&gt; \n    as_tibble() |&gt; \n    mutate(time = cox_model$time) |&gt; \n    pivot_longer(\n      cols = -time,\n      names_to = \"strata\",\n      values_to = \"surv\") |&gt; \n    mutate(\n      cumhaz = -log(surv),\n      model = \"Cox PH\")\n}\n\nkm_and_cph =\n  km_model |&gt; \n  fortify(surv.connect = TRUE) |&gt; \n  mutate(\n    strata = trimws(strata),\n    model = \"Kaplan-Meier\",\n    cumhaz = -log(surv)) |&gt;\n  bind_rows(stack_surv_ph(cox_model))\n\n\n\nShow R codekm_and_cph |&gt; \n  ggplot(aes(x = time, y = surv, col = model)) +\n  geom_step() +\n  facet_wrap(~strata) +\n  theme_bw() + \n  ylab(\"S(t) = P(T&gt;=t)\") +\n  xlab(\"Survival time (t, days)\") +\n  theme(legend.position = \"bottom\")\n\n\nObserved and expected survival curves for bmt data\n\n\n\n\n\n6.6.4 Cumulative hazard (log-scale) curves\nAlso known as “complementary log-log (clog-log) survival curves”.\n\nShow R codena_model = survfit(\n  formula = surv ~ dtype + gtype,\n  data = hodg2,\n  type = \"fleming\")\n\nna_model |&gt; \n  survminer::ggsurvplot(\n  legend = \"bottom\", \n  legend.title = \"\",\n  ylab = \"log(Cumulative Hazard)\",\n  xlab = \"Time since transplant (days, log-scale)\",\n  fun = 'cloglog', \n  size = .5,\n  ggtheme = theme_bw(),\n  conf.int = FALSE, \n  censor = TRUE) |&gt;  \n  magrittr::extract2(\"plot\") +\n  guides(\n    col = \n      guide_legend(\n        ncol = 2,\n        label.theme = \n          element_text(\n            size = legend_text_size)))\n\n\n\nFigure 6.5: Complementary log-log survival curves - Nelson-Aalen estimates\n\n\n\n\n\n\n\nLet’s compare these empirical (i.e., non-parametric) curves with the fitted curves from our coxph() model:\n\nShow R codecox_model |&gt; \n  survminer::ggsurvplot(\n    facet_by = \"\",\n    legend = \"bottom\", \n    legend.title = \"\",\n    ylab = \"log(Cumulative Hazard)\",\n    xlab = \"Time since transplant (days, log-scale)\",\n    fun = 'cloglog', \n    size = .5,\n    ggtheme = theme_bw(),\n    censor = FALSE, # doesn't make sense for cox model\n    conf.int = FALSE) |&gt;  \n  magrittr::extract2(\"plot\") +\n  guides(\n    col = \n      guide_legend(\n        ncol = 2,\n        label.theme = \n          element_text(\n            size = legend_text_size)))\n\n\n\nFigure 6.6: Complementary log-log survival curves - PH estimates\n\n\n\n\n\n\n\nNow let’s overlay these cumulative hazard curves:\n\nShow R codena_and_cph = \n  na_model |&gt; \n  fortify(fun = \"cumhaz\") |&gt; \n  # `fortify.survfit()` doesn't name cumhaz correctly:\n  rename(cumhaz = surv) |&gt;  \n  mutate(\n    surv = exp(-cumhaz),\n    strata = trimws(strata)) |&gt; \n  mutate(model = \"Nelson-Aalen\") |&gt; \n  bind_rows(stack_surv_ph(cox_model))\n\nna_and_cph |&gt; \n  ggplot(\n    aes(\n      x = time, \n      y = cumhaz, \n      col = model)) +\n  geom_step() +\n  facet_wrap(~strata) +\n  theme_bw() + \n  scale_y_continuous(\n    trans = \"log10\",\n    name = \"Cumulative hazard H(t) (log-scale)\") +\n  scale_x_continuous(\n    trans = \"log10\",\n    name = \"Survival time (t, days, log-scale)\") +\n  theme(legend.position = \"bottom\")\n\n\n\nFigure 6.7: Observed and expected cumulative hazard curves for bmt data (cloglog format)",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "proportional-hazards-models.html#predictions-and-residuals",
    "href": "proportional-hazards-models.html#predictions-and-residuals",
    "title": "\n6  Proportional Hazards Models\n",
    "section": "\n6.7 Predictions and Residuals",
    "text": "6.7 Predictions and Residuals\n\n6.7.1 Review: Predictions in Linear Regression\n\nIn linear regression, we have a linear predictor for each data point \\(i\\)\n\n\n\\[\n\\begin{aligned}\n\\eta_i &= \\beta_0+\\beta_1x_{1i}+\\cdots+\\beta_px_{pi}\\\\\n\\hat y_i &=\\hat\\eta_i = \\hat\\beta_0+\\hat\\beta_1x_{1i}+\\cdots+\\hat\\beta_px_{pi}\\\\\ny_i &\\sim N(\\eta_i,\\sigma^2)\n\\end{aligned}\n\\]\n\n\n\\(\\hat y_i\\) estimates the conditional mean of \\(y_i\\) given the covariate values \\(\\tilde{x}_i\\). This together with the prediction error says that we are predicting the distribution of values of \\(y\\).\n\n6.7.2 Review: Residuals in Linear Regression\n\nThe usual residual is \\(r_i=y_i-\\hat y_i\\), the difference between the actual value of \\(y\\) and a prediction of its mean.\nThe residuals are also the quantities the sum of whose squares is being minimized by the least squares/MLE estimation.\n\n6.7.3 Predictions and Residuals in survival models\n\nIn survival analysis, the equivalent of \\(y_i\\) is the event time \\(t_i\\), which is unknown for the censored observations.\nThe expected event time can be tricky to calculate:\n\n\\[\n\\hat{\\text{E}}[T|X=x] = \\int_{t=0}^{\\infty} \\hat S(t)dt\n\\]\n\n6.7.4 Wide prediction intervals\nThe nature of time-to-event data results in very wide prediction intervals:\n\nSuppose a cancer patient is predicted to have a mean lifetime of 5 years after diagnosis and suppose the distribution is exponential.\nIf we want a 95% interval for survival, the lower end is at the 0.025 percentage point of the exponential which is qexp(.025, rate = 1/5) = 0.1266 years, or 1/40 of the mean lifetime.\nThe upper end is at the 0.975 point which is qexp(.975, rate = 1/5) = 18.4444 years, or 3.7 times the mean lifetime.\nSaying that the survival time is somewhere between 6 weeks and 18 years does not seem very useful, but it may be the best we can do.\nFor survival analysis, something is like a residual if it is small when the model is accurate or if the accumulation of them is in some way minimized by the estimation algorithm, but there is no exact equivalence to linear regression residuals.\nAnd if there is, they are mostly quite large!\n\n6.7.5 Types of Residuals in Time-to-Event Models\n\nIt is often hard to make a decision from graph appearances, though the process can reveal much.\nSome diagnostic tests are based on residuals as with other regression methods:\n\n\nSchoenfeld residuals (via cox.zph) for proportionality\n\nCox-Snell residuals for goodness of fit (Section 6.8)\n\nmartingale residuals for non-linearity\n\ndfbeta for influence.\n\n\n\n6.7.6 Schoenfeld residuals\n\nThere is a Schoenfeld residual for each subject \\(i\\) with an event (not censored) and for each predictor \\(x_{k}\\).\nAt the event time \\(t\\) for that subject, there is a risk set \\(R\\), and each subject \\(j\\) in the risk set has a risk coefficient \\(\\theta_j\\) and also a value \\(x_{jk}\\) of the predictor.\nThe Schoenfeld residual is the difference between \\(x_{ik}\\) and the risk-weighted average of all the \\(x_{jk}\\) over the risk set.\n\n\\[\nr^S_{ik} =\nx_{ik}-\\frac{\\sum_{k\\in R}x_{jk}\\theta_k}{\\sum_{k\\in R}\\theta_k}\n\\]\nThis residual measures how typical the individual subject is with respect to the covariate at the time of the event. Since subjects should fail more or less uniformly according to risk, the Schoenfeld residuals should be approximately level over time, not increasing or decreasing.\nWe can test this with the correlation with time on some scale, which could be the time itself, the log time, or the rank in the set of failure times.\nThe default is to use the KM curve as a transform, which is similar to the rank but deals better with censoring.\nThe cox.zph() function implements a score test proposed in Grambsch and Therneau (1994).\n\nShow R codehodg.zph = cox.zph(hodg.cox1)\nprint(hodg.zph)\n#&gt;               chisq df     p\n#&gt; gtype        0.5400  1 0.462\n#&gt; dtype        1.8012  1 0.180\n#&gt; score        3.8805  1 0.049\n#&gt; wtime        0.0173  1 0.895\n#&gt; gtype:dtype  4.0474  1 0.044\n#&gt; GLOBAL      13.7573  5 0.017\n\n\ngtype\n\nShow R codeggcoxzph(hodg.zph, var = \"gtype\")\n\n\n\n\n\n\n\ndtype\n\nShow R codeggcoxzph(hodg.zph, var = \"dtype\")\n\n\n\n\n\n\n\nscore\n\nShow R codeggcoxzph(hodg.zph, var = \"score\")\n\n\n\n\n\n\n\nwtime\n\nShow R codeggcoxzph(hodg.zph, var = \"wtime\")\n\n\n\n\n\n\n\ngtype:dtype\n\nShow R codeggcoxzph(hodg.zph, var = \"gtype:dtype\")\n\n\n\n\n\n\n\nConclusions\n\nFrom the correlation test, the Karnofsky score and the interaction with graft type disease type induce modest but statistically significant non-proportionality.\nThe sample size here is relatively small (26 events in 43 subjects). If the sample size is large, very small amounts of non-proportionality can induce a significant result.\nAs time goes on, autologous grafts are over-represented at their own event times, but those from HOD patients become less represented.\nBoth the statistical tests and the plots are useful.",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "proportional-hazards-models.html#sec-cox-snell",
    "href": "proportional-hazards-models.html#sec-cox-snell",
    "title": "\n6  Proportional Hazards Models\n",
    "section": "\n6.8 Goodness of Fit using the Cox-Snell Residuals",
    "text": "6.8 Goodness of Fit using the Cox-Snell Residuals\n(references: Klein and Moeschberger (2003), §11.2, and Dobson and Barnett (2018), §10.6)\n\n\nSuppose that an individual has a survival time \\(T\\) which has survival function \\(S(t)\\), meaning that \\(\\Pr(T&gt; t) = S(t)\\). Then \\(S(T)\\) has a uniform distribution on \\((0,1)\\):\n\n\\[\n\\begin{aligned}\n\\Pr(S(T_i) \\le u)\n&= \\Pr(T_i &gt; S_i^{-1}(u))\\\\\n&= S_i(S_i^{-1}(u))\\\\\n&= u\n\\end{aligned}\n\\]\n\n\nAlso, if \\(U\\) has a uniform distribution on \\((0,1)\\), then what is the distribution of \\(-\\ln(U)\\)?\n\n\n\\[\n\\begin{aligned}\n\\Pr(-\\ln(U) &lt; x) &= \\Pr(U&gt;\\text{exp}\\left\\{-x\\right\\})\\\\\n&= 1-e^{-x}\n\\end{aligned}\n\\]\n\nwhich is the CDF of an exponential distribution with parameter \\(\\lambda=1\\).\n\n\n\nDefinition 6.5 (Cox-Snell generalized residuals)  \n\nThe Cox-Snell generalized residuals are defined as:\n\n\\[\nr^{CS}_i \\stackrel{\\text{def}}{=}\\hat H(t_i|\\tilde{x}_i)\n\\]\n\n\nIf the estimate \\(\\hat S_i\\) is accurate, \\(r^{CS}_i\\) should have an exponential distribution with constant hazard \\(\\lambda=1\\), which means that these values should look like a censored sample from this exponential distribution.\n\n\n\nShow R codehodg2 = hodg2 |&gt; \n  mutate(cs = predict(hodg.cox1, type = \"expected\"))\n\nsurv.csr = survfit(\n  data = hodg2,\n  formula = Surv(time = cs, event = delta == \"dead\") ~ 1,\n  type = \"fleming-harrington\")\n\nautoplot(surv.csr, fun = \"cumhaz\") + \n  geom_abline(aes(intercept = 0, slope = 1), col = \"red\") +\n  theme_bw()\n\n\nCumulative Hazard of Cox-Snell Residuals\n\n\n\n\nThe line with slope 1 and intercept 0 fits the curve relatively well, so we don’t see lack of fit using this procedure.",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "proportional-hazards-models.html#martingale-residuals",
    "href": "proportional-hazards-models.html#martingale-residuals",
    "title": "\n6  Proportional Hazards Models\n",
    "section": "\n6.9 Martingale Residuals",
    "text": "6.9 Martingale Residuals\nThe martingale residuals are a slight modification of the Cox-Snell residuals. If the censoring indicator is \\(\\delta_i\\), then \\[r^M_i=\\delta_i-r^{CS}_i\\] These residuals can be interpreted as an estimate of the excess number of events seen in the data but not predicted by the model. We will use these to examine the functional forms of continuous covariates.\n\n6.9.1 Using Martingale Residuals\nMartingale residuals can be used to examine the functional form of a numeric variable.\n\nWe fit the model without that variable and compute the martingale residuals.\nWe then plot these martingale residuals against the values of the variable.\nWe can see curvature, or a possible suggestion that the variable can be discretized.\n\nLet’s use this to examine the score and wtime variables in the wtime data set.\nKarnofsky score\n\nShow R codehodg2 = hodg2 |&gt; \n  mutate(\n    mres = \n      hodg.cox1 |&gt; \n      update(. ~ . - score) |&gt; \n      residuals(type=\"martingale\"))\n\nhodg2 |&gt; \n  ggplot(aes(x = score, y = mres)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", aes(col = \"loess\")) +\n  geom_smooth(method = 'lm', aes(col = \"lm\")) +\n  theme_classic() +\n  xlab(\"Karnofsky Score\") +\n  ylab(\"Martingale Residuals\") +\n  guides(col=guide_legend(title = \"\"))\n\n\nMartingale Residuals vs. Karnofsky Score\n\n\n\n\nThe line is almost straight. It could be some modest transformation of the Karnofsky score would help, but it might not make much difference.\nWaiting time\n\nShow R codehodg2$mres = \n  hodg.cox1 |&gt; \n  update(. ~ . - wtime) |&gt; \n  residuals(type=\"martingale\")\n\nhodg2 |&gt; \n  ggplot(aes(x = wtime, y = mres)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", aes(col = \"loess\")) +\n  geom_smooth(method = 'lm', aes(col = \"lm\")) +\n  theme_classic() +\n  xlab(\"Waiting Time\") +\n  ylab(\"Martingale Residuals\") +\n  guides(col=guide_legend(title = \"\"))\n\n\nMartingale Residuals vs. Waiting Time\n\n\n\n\nThe line could suggest a step function. To see where the drop is, we can look at the largest waiting times and the associated martingale residual.\nThe martingale residuals are all negative for wtime &gt;83 and positive for the next smallest value. A reasonable cut-point is 80 days.\nUpdating the model\nLet’s reformulate the model with dichotomized wtime.\n\nShow R codehodg2 = \n  hodg2 |&gt; \n  mutate(\n    wt2 = cut(\n      wtime,c(0, 80, 200),\n      labels=c(\"short\",\"long\")))\n\nhodg.cox2 =\n  coxph(\n    formula = \n      Surv(time, event = delta == \"dead\") ~ \n      gtype*dtype + score + wt2,\n    data = hodg2)\n\n\n\nShow R codehodg.cox1 |&gt; drop1(test=\"Chisq\")\n\n Model summary table with waiting time on continuous scale\n  \n\n\n\n\nShow R codehodg.cox2 |&gt; drop1(test=\"Chisq\")\n\n Model summary table with dichotomized waiting time\n  \n\n\n\nThe new model has better (lower) AIC.",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "proportional-hazards-models.html#checking-for-outliers-and-influential-observations",
    "href": "proportional-hazards-models.html#checking-for-outliers-and-influential-observations",
    "title": "\n6  Proportional Hazards Models\n",
    "section": "\n6.10 Checking for Outliers and Influential Observations",
    "text": "6.10 Checking for Outliers and Influential Observations\nWe will check for outliers using the deviance residuals. The martingale residuals show excess events or the opposite, but highly skewed, with the maximum possible value being 1, but the smallest value can be very large negative. Martingale residuals can detect unexpectedly long-lived patients, but patients who die unexpectedly early show up only in the deviance residual. Influence will be examined using dfbeta in a similar way to linear regression, logistic regression, or Poisson regression.\n\n6.10.1 Deviance Residuals\n\\[\n\\begin{aligned}\nr_i^D &= \\textrm{sign}(r_i^M)\\sqrt{-2\\left[ r_i^M+\\delta_i\\ln(\\delta_i-r_i^M)  \\right]}\\\\\nr_i^D &= \\textrm{sign}(r_i^M)\\sqrt{-2\\left[ r_i^M+\\delta_i\\ln(r_i^{CS})  \\right]}\n\\end{aligned}\n\\]\nRoughly centered on 0 with approximate standard deviation 1.\n\n6.10.2 \n\nShow R codehodg.mart = residuals(hodg.cox2,type=\"martingale\")\nhodg.dev = residuals(hodg.cox2,type=\"deviance\")\nhodg.dfb = residuals(hodg.cox2,type=\"dfbeta\")\nhodg.preds = predict(hodg.cox2)                   #linear predictor\n\n\n\nShow R codeplot(hodg.preds,\n     hodg.mart,\n     xlab=\"Linear Predictor\",\n     ylab=\"Martingale Residual\")\n\n\nMartingale Residuals vs. Linear Predictor\n\n\n\n\nThe smallest three martingale residuals in order are observations 1, 29, and 18.\n\nShow R codeplot(hodg.preds,hodg.dev,xlab=\"Linear Predictor\",ylab=\"Deviance Residual\")\n\n\nDeviance Residuals vs. Linear Predictor\n\n\n\n\nThe two largest deviance residuals are observations 1 and 29. Worth examining.\n\n6.10.3 dfbeta\n\ndfbeta is the approximate change in the coefficient vector if that observation were dropped\ndfbetas is the approximate change in the coefficients, scaled by the standard error for the coefficients.\n\nGraft type\n\nShow R codeplot(hodg.dfb[,1],xlab=\"Observation Order\",ylab=\"dfbeta for Graft Type\")\n\n\ndfbeta Values by Observation Order for Graft Type\n\n\n\n\nThe smallest dfbeta for graft type is observation 1.\nDisease type\n\nShow R codeplot(hodg.dfb[,2],\n     xlab=\"Observation Order\",\n     ylab=\"dfbeta for Disease Type\")\n\n\ndfbeta Values by Observation Order for Disease Type\n\n\n\n\nThe smallest two dfbeta values for disease type are observations 1 and 16.\nKarnofsky score\n\nShow R codeplot(hodg.dfb[,3],\n     xlab=\"Observation Order\",\n     ylab=\"dfbeta for Karnofsky Score\")\n\n\ndfbeta Values by Observation Order for Karnofsky Score\n\n\n\n\nThe two highest dfbeta values for score are observations 1 and 18. The next three are observations 17, 29, and 19. The smallest value is observation 2.\nWaiting time (dichotomized)\n\nShow R codeplot(\n  hodg.dfb[,4],\n  xlab=\"Observation Order\",\n  ylab=\"dfbeta for `Waiting Time &lt; 80`\")\n\n\ndfbeta Values by Observation Order for Waiting Time (dichotomized)\n\n\n\n\nThe two large values of dfbeta for dichotomized waiting time are observations 15 and 16. This may have to do with the discretization of waiting time.\nInteraction: graft type and disease type\n\nShow R codeplot(hodg.dfb[,5],\n     xlab=\"Observation Order\",\n     ylab=\"dfbeta for dtype:gtype\")\n\n\ndfbeta Values by Observation Order for dtype:gtype\n\n\n\n\nThe two largest values are observations 1 and 16. The smallest value is observation 35.\n\n\nTable 6.1: Observations to Examine by Residuals and Influence\n\n\n\nDiagnostic\nObservations to Examine\n\n\n\nMartingale Residuals\n1, 29, 18\n\n\nDeviance Residuals\n1, 29\n\n\nGraft Type Influence\n1\n\n\nDisease Type Influence\n1, 16\n\n\nKarnofsky Score Influence\n1, 18 (17, 29, 19)\n\n\nWaiting Time Influence\n15, 16\n\n\nGraft by Disease Influence\n1, 16, 35\n\n\n\n\n\n\nThe most important observations to examine seem to be 1, 15, 16, 18, and 29.\n\n6.10.4 \n\nShow R codewith(hodg,summary(time[delta==1]))\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;     2.0    41.2    62.5    97.6    83.2   524.0\n\n\n\nShow R codewith(hodg,summary(wtime))\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;     5.0    16.0    24.0    37.7    55.5   171.0\n\n\n\nShow R codewith(hodg,summary(score))\n#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#&gt;    20.0    60.0    80.0    76.3    90.0   100.0\n\n\n\nShow R codehodg.cox2\n#&gt; Call:\n#&gt; coxph(formula = Surv(time, event = delta == \"dead\") ~ gtype * \n#&gt;     dtype + score + wt2, data = hodg2)\n#&gt; \n#&gt;                                coef exp(coef) se(coef)  z     p\n#&gt; gtypeAutologous                0.67      1.94     0.59  1 0.263\n#&gt; dtypeHodgkins                  2.33     10.25     0.73  3 0.002\n#&gt; score                         -0.06      0.95     0.01 -4 8e-06\n#&gt; wt2long                       -2.06      0.13     1.05 -2 0.050\n#&gt; gtypeAutologous:dtypeHodgkins -2.07      0.13     0.93 -2 0.026\n#&gt; \n#&gt; Likelihood ratio test=35  on 5 df, p=1e-06\n#&gt; n= 43, number of events= 26\n\n\n\nShow R codehodg2[c(1,15,16,18,29),] |&gt; \n  select(gtype, dtype, time, delta, score, wtime) |&gt; \n  mutate(\n    comment = \n      c(\n        \"early death, good score, low risk\",\n        \"high risk grp, long wait, poor score\",\n        \"high risk grp, short wait, poor score\",\n        \"early death, good score, med risk grp\",\n        \"early death, good score, med risk grp\"\n      ))\n\n\n  \n\n\n\n\n6.10.5 Action Items\n\nUnusual points may need checking, particularly if the data are not completely cleaned. In this case, observations 15 and 16 may show some trouble with the dichotomization of waiting time, but it still may be useful.\nThe two largest residuals seem to be due to unexpectedly early deaths, but unfortunately this can occur.\nIf hazards don’t look proportional, then we may need to use strata, between which the base hazards are permitted to be different. For this problem, the natural strata are the two diseases, because they could need to be managed differently anyway.\nA main point that we want to be sure of is the relative risk difference by disease type and graft type.\n\n\nShow R codehodg.cox2 |&gt; \n  predict(\n    reference = \"zero\",\n    newdata = means |&gt; \n      mutate(\n        wt2 = \"short\", \n        score = 0), \n    type = \"lp\") |&gt; \n  data.frame('linear predictor' = _) |&gt; \n  pander()\n\n\nLinear Risk Predictors for Lymphoma\n\n\n\n\n\n \nlinear.predictor\n\n\n\nNon-Hodgkins,Allogenic\n0\n\n\nNon-Hodgkins,Autologous\n0.6651\n\n\nHodgkins,Allogenic\n2.327\n\n\nHodgkins,Autologous\n0.9256\n\n\n\n\n\nFor Non-Hodgkin’s, the allogenic graft is better. For Hodgkin’s, the autologous graft is much better.",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "proportional-hazards-models.html#stratified-survival-models",
    "href": "proportional-hazards-models.html#stratified-survival-models",
    "title": "\n6  Proportional Hazards Models\n",
    "section": "\n6.11 Stratified survival models",
    "text": "6.11 Stratified survival models\n\n6.11.1 Revisiting the leukemia dataset (anderson)\nWe will analyze remission survival times on 42 leukemia patients, half on new treatment, half on standard treatment.\nThis is the same data as the drug6mp data from KMsurv, but with two other variables and without the pairing. This version comes from Kleinbaum and Klein (2012) (e.g., p281):\n\nShow R code\nanderson = \n  paste0(\n    \"http://web1.sph.emory.edu/dkleinb/allDatasets/\",\n    \"surv2datasets/anderson.dta\") |&gt; \n  haven::read_dta() |&gt; \n  mutate(\n    status = status |&gt; \n      case_match(\n        1 ~ \"relapse\",\n        0 ~ \"censored\"\n      ),\n    \n    sex = sex |&gt; \n      case_match(\n        0 ~ \"female\",\n        1 ~ \"male\"\n      ) |&gt; \n      factor() |&gt; \n      relevel(ref = \"female\"),\n    \n    rx = rx |&gt; \n      case_match(\n        0 ~ \"new\",\n        1 ~ \"standard\"\n      ) |&gt; \n      factor() |&gt; relevel(ref = \"standard\"),\n    \n    surv = Surv(\n      time = survt, \n      event = (status == \"relapse\"))\n  )\n\nprint(anderson)\n\n\n\n6.11.2 Cox semi-parametric proportional hazards model\n\nShow R code\nanderson.cox1 = coxph(\n  formula = surv ~ rx + sex + logwbc,\n  data = anderson)\n\nsummary(anderson.cox1)\n#&gt; Call:\n#&gt; coxph(formula = surv ~ rx + sex + logwbc, data = anderson)\n#&gt; \n#&gt;   n= 42, number of events= 30 \n#&gt; \n#&gt;           coef exp(coef) se(coef)     z Pr(&gt;|z|)    \n#&gt; rxnew   -1.504     0.222    0.462 -3.26   0.0011 ** \n#&gt; sexmale  0.315     1.370    0.455  0.69   0.4887    \n#&gt; logwbc   1.682     5.376    0.337  5.00  5.8e-07 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;         exp(coef) exp(-coef) lower .95 upper .95\n#&gt; rxnew       0.222      4.498     0.090     0.549\n#&gt; sexmale     1.370      0.730     0.562     3.338\n#&gt; logwbc      5.376      0.186     2.779    10.398\n#&gt; \n#&gt; Concordance= 0.851  (se = 0.041 )\n#&gt; Likelihood ratio test= 47.2  on 3 df,   p=3e-10\n#&gt; Wald test            = 33.5  on 3 df,   p=2e-07\n#&gt; Score (logrank) test = 48  on 3 df,   p=2e-10\n\n\nTest the proportional hazards assumption\n\nShow R codecox.zph(anderson.cox1)\n#&gt;        chisq df    p\n#&gt; rx     0.036  1 0.85\n#&gt; sex    5.420  1 0.02\n#&gt; logwbc 0.142  1 0.71\n#&gt; GLOBAL 5.879  3 0.12\n\n\nGraph the K-M survival curves\n\nShow R code\nanderson_km_model = survfit(\n  formula = surv ~ sex,\n  data = anderson)\n\nanderson_km_model |&gt; \n  autoplot(conf.int = FALSE) +\n  theme_bw() +\n  theme(legend.position=\"bottom\")\n\n\n\n\n\n\n\nThe survival curves cross, which indicates a problem in the proportionality assumption by sex.\n\n6.11.3 Graph the Nelson-Aalen cumulative hazard\nWe can also look at the log-hazard (“cloglog survival”) plots:\n\nShow R codeanderson_na_model = survfit(\n  formula = surv ~ sex,\n  data = anderson,\n  type = \"fleming\")\n\nanderson_na_model |&gt; \n  autoplot(\n    fun = \"cumhaz\",\n    conf.int = FALSE) +\n  theme_classic() +\n  theme(legend.position=\"bottom\") +\n  ylab(\"log(Cumulative Hazard)\") +\n  scale_y_continuous(\n    trans = \"log10\",\n    name = \"Cumulative hazard (H(t), log scale)\") +\n  scale_x_continuous(\n    breaks = c(1,2,5,10,20,50),\n    trans = \"log\"\n  )\n\n\nCumulative hazard (cloglog scale) for anderson data\n\n\n\n\nThis can be fixed by using strata or possibly by other model alterations.\n\n6.11.4 The Stratified Cox Model\n\nIn a stratified Cox model, each stratum, defined by one or more factors, has its own base survival function \\(h_0(t)\\).\nBut the coefficients for each variable not used in the strata definitions are assumed to be the same across strata.\nTo check if this assumption is reasonable one can include interactions with strata and see if they are significant (this may generate a warning and NA lines but these can be ignored).\nSince the sex variable shows possible non-proportionality, we try stratifying on sex.\n\n\nShow R code\nanderson.coxph.strat = \n  coxph(\n    formula = \n      surv ~ rx + logwbc + strata(sex),\n    data = anderson)\n\nsummary(anderson.coxph.strat)\n#&gt; Call:\n#&gt; coxph(formula = surv ~ rx + logwbc + strata(sex), data = anderson)\n#&gt; \n#&gt;   n= 42, number of events= 30 \n#&gt; \n#&gt;          coef exp(coef) se(coef)     z Pr(&gt;|z|)    \n#&gt; rxnew  -0.998     0.369    0.474 -2.11    0.035 *  \n#&gt; logwbc  1.454     4.279    0.344  4.22  2.4e-05 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;        exp(coef) exp(-coef) lower .95 upper .95\n#&gt; rxnew      0.369      2.713     0.146     0.932\n#&gt; logwbc     4.279      0.234     2.180     8.398\n#&gt; \n#&gt; Concordance= 0.812  (se = 0.059 )\n#&gt; Likelihood ratio test= 32.1  on 2 df,   p=1e-07\n#&gt; Wald test            = 22.8  on 2 df,   p=1e-05\n#&gt; Score (logrank) test = 30.8  on 2 df,   p=2e-07\n\n\nLet’s compare this to a model fit only on the subset of males:\n\nShow R code\nanderson.coxph.male = \n  coxph(\n    formula = surv ~ rx + logwbc,\n    subset = sex == \"male\",\n    data = anderson)\n\nsummary(anderson.coxph.male)\n#&gt; Call:\n#&gt; coxph(formula = surv ~ rx + logwbc, data = anderson, subset = sex == \n#&gt;     \"male\")\n#&gt; \n#&gt;   n= 20, number of events= 14 \n#&gt; \n#&gt;          coef exp(coef) se(coef)     z Pr(&gt;|z|)   \n#&gt; rxnew  -1.978     0.138    0.739 -2.68   0.0075 **\n#&gt; logwbc  1.743     5.713    0.536  3.25   0.0011 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;        exp(coef) exp(-coef) lower .95 upper .95\n#&gt; rxnew      0.138      7.227    0.0325     0.589\n#&gt; logwbc     5.713      0.175    1.9991    16.328\n#&gt; \n#&gt; Concordance= 0.905  (se = 0.043 )\n#&gt; Likelihood ratio test= 29.2  on 2 df,   p=5e-07\n#&gt; Wald test            = 15.3  on 2 df,   p=5e-04\n#&gt; Score (logrank) test = 26.4  on 2 df,   p=2e-06\n\n\n\nShow R codeanderson.coxph.female = \n  coxph(\n    formula = \n      surv ~ rx + logwbc,\n    subset = sex == \"female\",\n    data = anderson)\n\nsummary(anderson.coxph.female)\n#&gt; Call:\n#&gt; coxph(formula = surv ~ rx + logwbc, data = anderson, subset = sex == \n#&gt;     \"female\")\n#&gt; \n#&gt;   n= 22, number of events= 16 \n#&gt; \n#&gt;          coef exp(coef) se(coef)     z Pr(&gt;|z|)  \n#&gt; rxnew  -0.311     0.733    0.564 -0.55    0.581  \n#&gt; logwbc  1.206     3.341    0.503  2.40    0.017 *\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;        exp(coef) exp(-coef) lower .95 upper .95\n#&gt; rxnew      0.733      1.365     0.243      2.21\n#&gt; logwbc     3.341      0.299     1.245      8.96\n#&gt; \n#&gt; Concordance= 0.692  (se = 0.085 )\n#&gt; Likelihood ratio test= 6.65  on 2 df,   p=0.04\n#&gt; Wald test            = 6.36  on 2 df,   p=0.04\n#&gt; Score (logrank) test = 6.74  on 2 df,   p=0.03\n\n\nThe coefficients of treatment look different. Are they statistically different?\n\nShow R code\nanderson.coxph.strat.intxn = \n  coxph(\n    formula = surv ~ strata(sex) * (rx + logwbc),\n    data = anderson)\n\nanderson.coxph.strat.intxn |&gt; summary()\n#&gt; Call:\n#&gt; coxph(formula = surv ~ strata(sex) * (rx + logwbc), data = anderson)\n#&gt; \n#&gt;   n= 42, number of events= 30 \n#&gt; \n#&gt;                          coef exp(coef) se(coef)     z Pr(&gt;|z|)  \n#&gt; rxnew                  -0.311     0.733    0.564 -0.55    0.581  \n#&gt; logwbc                  1.206     3.341    0.503  2.40    0.017 *\n#&gt; strata(sex)male:rxnew  -1.667     0.189    0.930 -1.79    0.073 .\n#&gt; strata(sex)male:logwbc  0.537     1.710    0.735  0.73    0.465  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;                        exp(coef) exp(-coef) lower .95 upper .95\n#&gt; rxnew                      0.733      1.365    0.2427      2.21\n#&gt; logwbc                     3.341      0.299    1.2452      8.96\n#&gt; strata(sex)male:rxnew      0.189      5.294    0.0305      1.17\n#&gt; strata(sex)male:logwbc     1.710      0.585    0.4048      7.23\n#&gt; \n#&gt; Concordance= 0.797  (se = 0.058 )\n#&gt; Likelihood ratio test= 35.8  on 4 df,   p=3e-07\n#&gt; Wald test            = 21.7  on 4 df,   p=2e-04\n#&gt; Score (logrank) test = 33.1  on 4 df,   p=1e-06\n\n\n\nShow R codeanova(\n  anderson.coxph.strat.intxn,\n  anderson.coxph.strat)\n\n\n  \n\n\n\nWe don’t have enough evidence to tell the difference between these two models.\n\n6.11.5 Conclusions\n\nWe chose to use a stratified model because of the apparent non-proportionality of the hazard for the sex variable.\nWhen we fit interactions with the strata variable, we did not get an improved model (via the likelihood ratio test).\nSo we use the stratifed model with coefficients that are the same across strata.\n\n6.11.6 Another Modeling Approach\n\nWe used an additive model without interactions and saw that we might need to stratify by sex.\nInstead, we could try to improve the model’s functional form - maybe the interaction of treatment and sex is real, and after fitting that we might not need separate hazard functions.\nEither approach may work.\n\n\nShow R code\nanderson.coxph.intxn = \n  coxph(\n    formula = surv ~ (rx + logwbc) * sex,\n    data = anderson)\n\nanderson.coxph.intxn |&gt; summary()\n#&gt; Call:\n#&gt; coxph(formula = surv ~ (rx + logwbc) * sex, data = anderson)\n#&gt; \n#&gt;   n= 42, number of events= 30 \n#&gt; \n#&gt;                   coef exp(coef) se(coef)     z Pr(&gt;|z|)  \n#&gt; rxnew          -0.3748    0.6874   0.5545 -0.68    0.499  \n#&gt; logwbc          1.0637    2.8971   0.4726  2.25    0.024 *\n#&gt; sexmale        -2.8052    0.0605   2.0323 -1.38    0.167  \n#&gt; rxnew:sexmale  -2.1782    0.1132   0.9109 -2.39    0.017 *\n#&gt; logwbc:sexmale  1.2303    3.4223   0.6301  1.95    0.051 .\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;                exp(coef) exp(-coef) lower .95 upper .95\n#&gt; rxnew             0.6874      1.455   0.23185     2.038\n#&gt; logwbc            2.8971      0.345   1.14730     7.315\n#&gt; sexmale           0.0605     16.531   0.00113     3.248\n#&gt; rxnew:sexmale     0.1132      8.830   0.01899     0.675\n#&gt; logwbc:sexmale    3.4223      0.292   0.99539    11.766\n#&gt; \n#&gt; Concordance= 0.861  (se = 0.036 )\n#&gt; Likelihood ratio test= 57  on 5 df,   p=5e-11\n#&gt; Wald test            = 35.6  on 5 df,   p=1e-06\n#&gt; Score (logrank) test = 57.1  on 5 df,   p=5e-11\n\n\n\nShow R codecox.zph(anderson.coxph.intxn)\n#&gt;            chisq df    p\n#&gt; rx         0.136  1 0.71\n#&gt; logwbc     1.652  1 0.20\n#&gt; sex        1.266  1 0.26\n#&gt; rx:sex     0.149  1 0.70\n#&gt; logwbc:sex 0.102  1 0.75\n#&gt; GLOBAL     3.747  5 0.59",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "proportional-hazards-models.html#time-varying-covariates",
    "href": "proportional-hazards-models.html#time-varying-covariates",
    "title": "\n6  Proportional Hazards Models\n",
    "section": "\n6.12 Time-varying covariates",
    "text": "6.12 Time-varying covariates\n(adapted from Klein and Moeschberger (2003), §9.2)\n\n6.12.1 Motivating example: back to the leukemia dataset\n\n# load the data:\ndata(bmt, package = 'KMsurv')\nbmt |&gt; as_tibble() |&gt; print(n = 5)\n#&gt; # A tibble: 137 × 22\n#&gt;   group    t1    t2    d1    d2    d3    ta    da    tc    dc    tp    dp    z1\n#&gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n#&gt; 1     1  2081  2081     0     0     0    67     1   121     1    13     1    26\n#&gt; 2     1  1602  1602     0     0     0  1602     0   139     1    18     1    21\n#&gt; 3     1  1496  1496     0     0     0  1496     0   307     1    12     1    26\n#&gt; 4     1  1462  1462     0     0     0    70     1    95     1    13     1    17\n#&gt; 5     1  1433  1433     0     0     0  1433     0   236     1    12     1    32\n#&gt; # ℹ 132 more rows\n#&gt; # ℹ 9 more variables: z2 &lt;int&gt;, z3 &lt;int&gt;, z4 &lt;int&gt;, z5 &lt;int&gt;, z6 &lt;int&gt;,\n#&gt; #   z7 &lt;int&gt;, z8 &lt;int&gt;, z9 &lt;int&gt;, z10 &lt;int&gt;\n\nThis dataset comes from the Copelan et al. (1991) study of allogenic bone marrow transplant therapy for acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL).\nOutcomes (endpoints)\n\nThe main endpoint is disease-free survival (t2 and d3) for the three risk groups, “ALL”, “AML Low Risk”, and “AML High Risk”.\nPossible intermediate events\n\ngraft vs. host disease (GVHD), an immunological rejection response to the transplant (bad)\nacute (AGVHD)\nchronic (CGVHD)\nplatelet recovery, a return of platelet count to normal levels (good)\n\nOne or the other, both in either order, or neither may occur.\nCovariates\n\nWe are interested in possibly using the covariates z1-z10 to adjust for other factors.\nIn addition, the time-varying covariates for acute GVHD, chronic GVHD, and platelet recovery may be useful.\nPreprocessing\nWe reformat the data before analysis:\n\nShow R code# reformat the data:\nbmt1 = \n  bmt |&gt; \n  as_tibble() |&gt; \n  mutate(\n    id = 1:n(), # will be used to connect multiple records for the same individual\n    \n    group =  group |&gt; \n      case_match(\n        1 ~ \"ALL\",\n        2 ~ \"Low Risk AML\",\n        3 ~ \"High Risk AML\") |&gt; \n      factor(levels = c(\"ALL\", \"Low Risk AML\", \"High Risk AML\")),\n    \n    `patient age` = z1,\n    \n    `donor age` = z2,\n    \n    `patient sex` = z3 |&gt; \n      case_match(\n        0 ~ \"Female\",\n        1 ~ \"Male\"),\n    \n    `donor sex` = z4 |&gt; \n      case_match(\n        0 ~ \"Female\",\n        1 ~ \"Male\"),\n    \n    `Patient CMV Status` = z5 |&gt; \n      case_match(\n        0 ~ \"CMV Negative\",\n        1 ~ \"CMV Positive\"),\n    \n    `Donor CMV Status` = z6 |&gt; \n      case_match(\n        0 ~ \"CMV Negative\",\n        1 ~ \"CMV Positive\"),\n    \n    `Waiting Time to Transplant` = z7,\n    \n    FAB = z8 |&gt; \n      case_match(\n        1 ~ \"Grade 4 Or 5 (AML only)\",\n        0 ~ \"Other\") |&gt; \n      factor() |&gt; \n      relevel(ref = \"Other\"),\n    \n    hospital = z9 |&gt;  # `z9` is hospital\n      case_match(\n        1 ~ \"Ohio State University\",\n        2 ~ \"Alferd\",\n        3 ~ \"St. Vincent\",\n        4 ~ \"Hahnemann\") |&gt; \n      factor() |&gt; \n      relevel(ref = \"Ohio State University\"),\n    \n    MTX = (z10 == 1) # a prophylatic treatment for GVHD\n    \n  ) |&gt; \n  select(-(z1:z10)) # don't need these anymore\n\nbmt1 |&gt; \n  select(group, id:MTX) |&gt; \n  print(n = 10)\n#&gt; # A tibble: 137 × 12\n#&gt;    group    id `patient age` `donor age` `patient sex` `donor sex`\n#&gt;    &lt;fct&gt; &lt;int&gt;         &lt;int&gt;       &lt;int&gt; &lt;chr&gt;         &lt;chr&gt;      \n#&gt;  1 ALL       1            26          33 Male          Female     \n#&gt;  2 ALL       2            21          37 Male          Male       \n#&gt;  3 ALL       3            26          35 Male          Male       \n#&gt;  4 ALL       4            17          21 Female        Male       \n#&gt;  5 ALL       5            32          36 Male          Male       \n#&gt;  6 ALL       6            22          31 Male          Male       \n#&gt;  7 ALL       7            20          17 Male          Female     \n#&gt;  8 ALL       8            22          24 Male          Female     \n#&gt;  9 ALL       9            18          21 Female        Male       \n#&gt; 10 ALL      10            24          40 Male          Male       \n#&gt; # ℹ 127 more rows\n#&gt; # ℹ 6 more variables: `Patient CMV Status` &lt;chr&gt;, `Donor CMV Status` &lt;chr&gt;,\n#&gt; #   `Waiting Time to Transplant` &lt;int&gt;, FAB &lt;fct&gt;, hospital &lt;fct&gt;, MTX &lt;lgl&gt;\n\n\n\n6.12.2 Time-Dependent Covariates\n\nA time-dependent covariate (“TDC”) is a covariate whose value changes during the course of the study.\nFor variables like age that change in a linear manner with time, we can just use the value at the start.\nBut it may be plausible that when and if GVHD occurs, the risk of relapse or death increases, and when and if platelet recovery occurs, the risk decreases.\n\n6.12.3 Analysis in R\n\nWe form a variable precovery which is = 0 before platelet recovery and is = 1 after platelet recovery, if it occurs.\nFor each subject where platelet recovery occurs, we set up multiple records (lines in the data frame); for example one from t = 0 to the time of platelet recovery, and one from that time to relapse, recovery, or death.\nWe do the same for acute GVHD and chronic GVHD.\nFor each record, the covariates are constant.\n\n\nShow R code\nbmt2 = bmt1 |&gt; \n  #set up new long-format data set:\n  tmerge(bmt1, id = id, tstop = t2) |&gt; \n  \n  # the following three steps can be in any order, \n  # and will still produce the same result:\n  #add aghvd as tdc:\n  tmerge(bmt1, id = id, agvhd = tdc(ta)) |&gt; \n  #add cghvd as tdc:\n  tmerge(bmt1, id = id, cgvhd = tdc(tc)) |&gt; \n  #add platelet recovery as tdc:\n  tmerge(bmt1, id = id, precovery = tdc(tp))   \n\nbmt2 = bmt2 |&gt; \n  as_tibble() |&gt; \n  mutate(status = as.numeric((tstop == t2) & d3))\n# status only = 1 if at end of t2 and not censored\n\n\nLet’s see how we’ve rearranged the first row of the data:\n\nShow R code\nbmt1 |&gt; \n  dplyr::filter(id == 1) |&gt; \n  dplyr::select(id, t1, d1, t2, d2, d3, ta, da, tc, dc, tp, dp)\n\n\n  \n\n\n\nThe event times for this individual are:\n\n\nt = 0 time of transplant\n\ntp = 13 platelet recovery\n\nta = 67 acute GVHD onset\n\ntc = 121 chronic GVHD onset\n\nt2 = 2081 end of study, patient not relapsed or dead\n\nAfter converting the data to long-format, we have:\n\nShow R codebmt2 |&gt; \n  select(\n    id,\n    tstart,\n    tstop,\n    agvhd,\n    cgvhd,\n    precovery,\n    status\n  ) |&gt; \n  dplyr::filter(id == 1)\n\n\n  \n\n\n\nNote that status could have been 1 on the last row, indicating that relapse or death occurred; since it is false, the participant must have exited the study without experiencing relapse or death (i.e., they were censored).\n\n6.12.4 Event sequences\nLet:\n\nA = acute GVHD\nC = chronic GVHD\nP = platelet recovery\n\nEach of the eight possible combinations of A or not-A, with C or not-C, with P or not-P occurs in this data set.\n\nA always occurs before C, and P always occurs before C, if both occur.\nThus there are ten event sequences in the data set: None, A, C, P, AC, AP, PA, PC, APC, and PAC.\nIn general, there could be as many as \\(1+3+(3)(2)+6=16\\) sequences, but our domain knowledge tells us that some are missing: CA, CP, CAP, CPA, PCA, PC, PAC\nDifferent subjects could have 1, 2, 3, or 4 intervals, depending on which of acute GVHD, chronic GVHD, and/or platelet recovery occurred.\nThe final interval for any subject has status = 1 if the subject relapsed or died at that time; otherwise status = 0.\nAny earlier intervals have status = 0.\nEven though there might be multiple lines per ID in the dataset, there is never more than one event, so no alterations need be made in the estimation procedures or in the interpretation of the output.\nThe function tmerge in the survival package eases the process of constructing the new long-format dataset.\n\n6.12.5 Model with Time-Fixed Covariates\n\nShow R codebmt1 = \n  bmt1 |&gt; \n  mutate(surv = Surv(t2,d3))\n\nbmt_coxph_TF = coxph(\n  formula = surv ~ group + `patient age`*`donor age` + FAB,\n  data = bmt1)\nsummary(bmt_coxph_TF)\n#&gt; Call:\n#&gt; coxph(formula = surv ~ group + `patient age` * `donor age` + \n#&gt;     FAB, data = bmt1)\n#&gt; \n#&gt;   n= 137, number of events= 83 \n#&gt; \n#&gt;                                 coef exp(coef)  se(coef)     z Pr(&gt;|z|)    \n#&gt; groupLow Risk AML          -1.090648  0.335999  0.354279 -3.08  0.00208 ** \n#&gt; groupHigh Risk AML         -0.403905  0.667707  0.362777 -1.11  0.26555    \n#&gt; `patient age`              -0.081639  0.921605  0.036107 -2.26  0.02376 *  \n#&gt; `donor age`                -0.084587  0.918892  0.030097 -2.81  0.00495 ** \n#&gt; FABGrade 4 Or 5 (AML only)  0.837416  2.310388  0.278464  3.01  0.00264 ** \n#&gt; `patient age`:`donor age`   0.003159  1.003164  0.000951  3.32  0.00089 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;                            exp(coef) exp(-coef) lower .95 upper .95\n#&gt; groupLow Risk AML              0.336      2.976     0.168     0.673\n#&gt; groupHigh Risk AML             0.668      1.498     0.328     1.360\n#&gt; `patient age`                  0.922      1.085     0.859     0.989\n#&gt; `donor age`                    0.919      1.088     0.866     0.975\n#&gt; FABGrade 4 Or 5 (AML only)     2.310      0.433     1.339     3.988\n#&gt; `patient age`:`donor age`      1.003      0.997     1.001     1.005\n#&gt; \n#&gt; Concordance= 0.665  (se = 0.033 )\n#&gt; Likelihood ratio test= 32.8  on 6 df,   p=1e-05\n#&gt; Wald test            = 33  on 6 df,   p=1e-05\n#&gt; Score (logrank) test = 35.8  on 6 df,   p=3e-06\ndrop1(bmt_coxph_TF, test = \"Chisq\")\n\n\n  \n\n\n\n\nShow R codebmt1$mres = \n  bmt_coxph_TF |&gt; \n  update(. ~ . - `donor age`) |&gt; \n  residuals(type=\"martingale\")\n\nbmt1 |&gt; \n  ggplot(aes(x = `donor age`, y = mres)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", aes(col = \"loess\")) +\n  geom_smooth(method = 'lm', aes(col = \"lm\")) +\n  theme_classic() +\n  xlab(\"Donor age\") +\n  ylab(\"Martingale Residuals\") +\n  guides(col=guide_legend(title = \"\"))\n\n\nMartingale residuals for Donor age\n\n\n\n\nA more complex functional form for donor age seems warranted; left as an exercise for the reader.\nNow we will add the time-varying covariates:\n\nShow R code\n# add counting process formulation of Surv():\nbmt2 = \n  bmt2 |&gt; \n  mutate(\n    surv = \n      Surv(\n        time = tstart,\n        time2 = tstop,\n        event = status,\n        type = \"counting\"))\n\n\nLet’s see how the data looks for patient 15:\n\nShow R codebmt1 |&gt; dplyr::filter(id == 15) |&gt; dplyr::select(tp, dp, tc,dc, ta, da, FAB, surv, t1, d1, t2, d2, d3)\n\n\n  \n\n\nShow R codebmt2 |&gt; dplyr::filter(id == 15) |&gt; dplyr::select(id, agvhd, cgvhd, precovery, surv)\n\n\n  \n\n\n\n\n6.12.6 Model with Time-Dependent Covariates\n\nShow R codebmt_coxph_TV = coxph(\n  formula = \n    surv ~ \n    group + `patient age`*`donor age` + FAB + agvhd + cgvhd + precovery,\n  data = bmt2)\n\nsummary(bmt_coxph_TV)\n#&gt; Call:\n#&gt; coxph(formula = surv ~ group + `patient age` * `donor age` + \n#&gt;     FAB + agvhd + cgvhd + precovery, data = bmt2)\n#&gt; \n#&gt;   n= 341, number of events= 83 \n#&gt; \n#&gt;                                 coef exp(coef)  se(coef)     z Pr(&gt;|z|)   \n#&gt; groupLow Risk AML          -1.038514  0.353980  0.358220 -2.90   0.0037 **\n#&gt; groupHigh Risk AML         -0.380481  0.683533  0.374867 -1.01   0.3101   \n#&gt; `patient age`              -0.073351  0.929275  0.035956 -2.04   0.0413 * \n#&gt; `donor age`                -0.076406  0.926440  0.030196 -2.53   0.0114 * \n#&gt; FABGrade 4 Or 5 (AML only)  0.805700  2.238263  0.284273  2.83   0.0046 **\n#&gt; agvhd                       0.150565  1.162491  0.306848  0.49   0.6237   \n#&gt; cgvhd                      -0.116136  0.890354  0.289046 -0.40   0.6878   \n#&gt; precovery                  -0.941123  0.390190  0.347861 -2.71   0.0068 **\n#&gt; `patient age`:`donor age`   0.002895  1.002899  0.000944  3.07   0.0022 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;                            exp(coef) exp(-coef) lower .95 upper .95\n#&gt; groupLow Risk AML              0.354      2.825     0.175     0.714\n#&gt; groupHigh Risk AML             0.684      1.463     0.328     1.425\n#&gt; `patient age`                  0.929      1.076     0.866     0.997\n#&gt; `donor age`                    0.926      1.079     0.873     0.983\n#&gt; FABGrade 4 Or 5 (AML only)     2.238      0.447     1.282     3.907\n#&gt; agvhd                          1.162      0.860     0.637     2.121\n#&gt; cgvhd                          0.890      1.123     0.505     1.569\n#&gt; precovery                      0.390      2.563     0.197     0.772\n#&gt; `patient age`:`donor age`      1.003      0.997     1.001     1.005\n#&gt; \n#&gt; Concordance= 0.702  (se = 0.028 )\n#&gt; Likelihood ratio test= 40.3  on 9 df,   p=7e-06\n#&gt; Wald test            = 42.4  on 9 df,   p=3e-06\n#&gt; Score (logrank) test = 47.2  on 9 df,   p=4e-07\n\n\nPlatelet recovery is highly significant.\nNeither acute GVHD (agvhd) nor chronic GVHD (cgvhd) has a statistically significant effect here, nor are they significant in models with the other one removed.\n\nShow R code\nupdate(bmt_coxph_TV, .~.-agvhd) |&gt; summary()\n#&gt; Call:\n#&gt; coxph(formula = surv ~ group + `patient age` + `donor age` + \n#&gt;     FAB + cgvhd + precovery + `patient age`:`donor age`, data = bmt2)\n#&gt; \n#&gt;   n= 341, number of events= 83 \n#&gt; \n#&gt;                                 coef exp(coef)  se(coef)     z Pr(&gt;|z|)   \n#&gt; groupLow Risk AML          -1.049870  0.349983  0.356727 -2.94   0.0032 **\n#&gt; groupHigh Risk AML         -0.417049  0.658988  0.365348 -1.14   0.2537   \n#&gt; `patient age`              -0.070749  0.931696  0.035477 -1.99   0.0461 * \n#&gt; `donor age`                -0.075693  0.927101  0.030075 -2.52   0.0118 * \n#&gt; FABGrade 4 Or 5 (AML only)  0.807035  2.241253  0.283437  2.85   0.0044 **\n#&gt; cgvhd                      -0.095393  0.909015  0.285979 -0.33   0.7387   \n#&gt; precovery                  -0.983653  0.373942  0.338170 -2.91   0.0036 **\n#&gt; `patient age`:`donor age`   0.002859  1.002863  0.000936  3.05   0.0023 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;                            exp(coef) exp(-coef) lower .95 upper .95\n#&gt; groupLow Risk AML              0.350      2.857     0.174     0.704\n#&gt; groupHigh Risk AML             0.659      1.517     0.322     1.349\n#&gt; `patient age`                  0.932      1.073     0.869     0.999\n#&gt; `donor age`                    0.927      1.079     0.874     0.983\n#&gt; FABGrade 4 Or 5 (AML only)     2.241      0.446     1.286     3.906\n#&gt; cgvhd                          0.909      1.100     0.519     1.592\n#&gt; precovery                      0.374      2.674     0.193     0.726\n#&gt; `patient age`:`donor age`      1.003      0.997     1.001     1.005\n#&gt; \n#&gt; Concordance= 0.701  (se = 0.027 )\n#&gt; Likelihood ratio test= 40  on 8 df,   p=3e-06\n#&gt; Wald test            = 42.4  on 8 df,   p=1e-06\n#&gt; Score (logrank) test = 47.2  on 8 df,   p=1e-07\nupdate(bmt_coxph_TV, .~.-cgvhd) |&gt; summary()\n#&gt; Call:\n#&gt; coxph(formula = surv ~ group + `patient age` + `donor age` + \n#&gt;     FAB + agvhd + precovery + `patient age`:`donor age`, data = bmt2)\n#&gt; \n#&gt;   n= 341, number of events= 83 \n#&gt; \n#&gt;                                 coef exp(coef)  se(coef)     z Pr(&gt;|z|)   \n#&gt; groupLow Risk AML          -1.019638  0.360725  0.355311 -2.87   0.0041 **\n#&gt; groupHigh Risk AML         -0.381356  0.682935  0.374568 -1.02   0.3086   \n#&gt; `patient age`              -0.073189  0.929426  0.035890 -2.04   0.0414 * \n#&gt; `donor age`                -0.076753  0.926118  0.030121 -2.55   0.0108 * \n#&gt; FABGrade 4 Or 5 (AML only)  0.811716  2.251769  0.284012  2.86   0.0043 **\n#&gt; agvhd                       0.131621  1.140676  0.302623  0.43   0.6636   \n#&gt; precovery                  -0.946697  0.388021  0.347265 -2.73   0.0064 **\n#&gt; `patient age`:`donor age`   0.002904  1.002908  0.000943  3.08   0.0021 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;                            exp(coef) exp(-coef) lower .95 upper .95\n#&gt; groupLow Risk AML              0.361      2.772     0.180     0.724\n#&gt; groupHigh Risk AML             0.683      1.464     0.328     1.423\n#&gt; `patient age`                  0.929      1.076     0.866     0.997\n#&gt; `donor age`                    0.926      1.080     0.873     0.982\n#&gt; FABGrade 4 Or 5 (AML only)     2.252      0.444     1.291     3.929\n#&gt; agvhd                          1.141      0.877     0.630     2.064\n#&gt; precovery                      0.388      2.577     0.196     0.766\n#&gt; `patient age`:`donor age`      1.003      0.997     1.001     1.005\n#&gt; \n#&gt; Concordance= 0.701  (se = 0.027 )\n#&gt; Likelihood ratio test= 40.1  on 8 df,   p=3e-06\n#&gt; Wald test            = 42.1  on 8 df,   p=1e-06\n#&gt; Score (logrank) test = 47.1  on 8 df,   p=1e-07\n\n\nLet’s drop them both:\n\nShow R codebmt_coxph_TV2 = update(bmt_coxph_TV, . ~ . - agvhd -cgvhd)\nbmt_coxph_TV2 |&gt; summary()\n#&gt; Call:\n#&gt; coxph(formula = surv ~ group + `patient age` + `donor age` + \n#&gt;     FAB + precovery + `patient age`:`donor age`, data = bmt2)\n#&gt; \n#&gt;   n= 341, number of events= 83 \n#&gt; \n#&gt;                                 coef exp(coef)  se(coef)     z Pr(&gt;|z|)   \n#&gt; groupLow Risk AML          -1.032520  0.356108  0.353202 -2.92   0.0035 **\n#&gt; groupHigh Risk AML         -0.413888  0.661075  0.365209 -1.13   0.2571   \n#&gt; `patient age`              -0.070965  0.931495  0.035453 -2.00   0.0453 * \n#&gt; `donor age`                -0.076052  0.926768  0.030007 -2.53   0.0113 * \n#&gt; FABGrade 4 Or 5 (AML only)  0.811926  2.252242  0.283231  2.87   0.0041 **\n#&gt; precovery                  -0.983505  0.373998  0.337997 -2.91   0.0036 **\n#&gt; `patient age`:`donor age`   0.002872  1.002876  0.000936  3.07   0.0021 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;                            exp(coef) exp(-coef) lower .95 upper .95\n#&gt; groupLow Risk AML              0.356      2.808     0.178     0.712\n#&gt; groupHigh Risk AML             0.661      1.513     0.323     1.352\n#&gt; `patient age`                  0.931      1.074     0.869     0.999\n#&gt; `donor age`                    0.927      1.079     0.874     0.983\n#&gt; FABGrade 4 Or 5 (AML only)     2.252      0.444     1.293     3.924\n#&gt; precovery                      0.374      2.674     0.193     0.725\n#&gt; `patient age`:`donor age`      1.003      0.997     1.001     1.005\n#&gt; \n#&gt; Concordance= 0.7  (se = 0.027 )\n#&gt; Likelihood ratio test= 39.9  on 7 df,   p=1e-06\n#&gt; Wald test            = 42.2  on 7 df,   p=5e-07\n#&gt; Score (logrank) test = 47.1  on 7 df,   p=5e-08",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "proportional-hazards-models.html#recurrent-events",
    "href": "proportional-hazards-models.html#recurrent-events",
    "title": "\n6  Proportional Hazards Models\n",
    "section": "\n6.13 Recurrent Events",
    "text": "6.13 Recurrent Events\n(Adapted from Kleinbaum and Klein (2012), Ch 8)\n\nSometimes an appropriate analysis requires consideration of recurrent events.\nA patient with arthritis may have more than one flareup. The same is true of many recurring-remitting diseases.\nIn this case, we have more than one line in the data frame, but each line may have an event.\nWe have to use a “robust” variance estimator to account for correlation of time-to-events within a patient.\n\n\n6.13.1 Bladder Cancer Data Set\nThe bladder cancer dataset from Kleinbaum and Klein (2012) contains recurrent event outcome information for eighty-six cancer patients followed for the recurrence of bladder cancer tumor after transurethral surgical excision (Byar and Green 1980). The exposure of interest is the effect of the drug treatment of thiotepa. Control variables are the initial number and initial size of tumors. The data layout is suitable for a counting processes approach.\nThis drug is still a possible choice for some patients. Another therapeutic choice is Bacillus Calmette-Guerin (BCG), a live bacterium related to cow tuberculosis.\nData dictionary\n\nVariables in the bladder dataset\n\nVariable\nDefinition\n\n\n\nid\nPatient unique ID\n\n\nstatus\nfor each time interval: 1 = recurred, 0 = censored\n\n\ninterval\n1 = first recurrence, etc.\n\n\nintime\n`tstop - tstart (all times in months)\n\n\ntstart\nstart of interval\n\n\ntstop\nend of interval\n\n\ntx\ntreatment code, 1 = thiotepa\n\n\nnum\nnumber of initial tumors\n\n\nsize\nsize of initial tumors (cm)\n\n\n\n\nThere are 85 patients and 190 lines in the dataset, meaning that many patients have more than one line.\nPatient 1 with 0 observation time was removed.\nOf the 85 patients, 47 had at least one recurrence and 38 had none.\n18 patients had exactly one recurrence.\nThere were up to 4 recurrences in a patient.\nOf the 190 intervals, 112 terminated with a recurrence and 78 were censored.\nDifferent intervals for the same patient are correlated.\n\nIs the effective sample size 47 or 112? This might narrow confidence intervals by as much as a factor of \\(\\sqrt{112/47}=1.54\\)\nWhat happens if I have 5 treatment and 5 control values and want to do a t-test and I then duplicate the 10 values as if the sample size was 20? This falsely narrows confidence intervals by a factor of \\(\\sqrt{2}=1.41\\).\n\n\nShow R codebladder = \n  paste0(\n    \"http://web1.sph.emory.edu/dkleinb/allDatasets\",\n    \"/surv2datasets/bladder.dta\") |&gt; \n  read_dta() |&gt; \n  as_tibble()\n\nbladder = bladder[-1,]  #remove subject with 0 observation time\nprint(bladder)\n\n\n\nShow R code\nbladder = \n  bladder |&gt; \n  mutate(\n    surv = \n      Surv(\n        time = start,\n        time2 = stop,\n        event = event,\n        type = \"counting\"))\n\nbladder.cox1 = coxph(\n  formula = surv~tx+num+size,\n  data = bladder)\n\n#results with biased variance-covariance matrix:\nsummary(bladder.cox1)\n#&gt; Call:\n#&gt; coxph(formula = surv ~ tx + num + size, data = bladder)\n#&gt; \n#&gt;   n= 190, number of events= 112 \n#&gt; \n#&gt;         coef exp(coef) se(coef)     z Pr(&gt;|z|)    \n#&gt; tx   -0.4116    0.6626   0.1999 -2.06  0.03947 *  \n#&gt; num   0.1637    1.1778   0.0478  3.43  0.00061 ***\n#&gt; size -0.0411    0.9598   0.0703 -0.58  0.55897    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;      exp(coef) exp(-coef) lower .95 upper .95\n#&gt; tx       0.663      1.509     0.448      0.98\n#&gt; num      1.178      0.849     1.073      1.29\n#&gt; size     0.960      1.042     0.836      1.10\n#&gt; \n#&gt; Concordance= 0.624  (se = 0.032 )\n#&gt; Likelihood ratio test= 14.7  on 3 df,   p=0.002\n#&gt; Wald test            = 15.9  on 3 df,   p=0.001\n#&gt; Score (logrank) test = 16.2  on 3 df,   p=0.001\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe likelihood ratio and score tests assume independence of observations within a cluster. The Wald and robust score tests do not.\n\n\nadding cluster = id\n\nIf we add cluster= id to the call to coxph, the coefficient estimates don’t change, but we get an additional column in the summary() output: robust se:\n\nShow R code\nbladder.cox2 = coxph(\n  formula = surv ~ tx + num + size,\n  cluster = id,\n  data = bladder)\n\n#unbiased though this reduces power:\nsummary(bladder.cox2)\n#&gt; Call:\n#&gt; coxph(formula = surv ~ tx + num + size, data = bladder, cluster = id)\n#&gt; \n#&gt;   n= 190, number of events= 112 \n#&gt; \n#&gt;         coef exp(coef) se(coef) robust se     z Pr(&gt;|z|)   \n#&gt; tx   -0.4116    0.6626   0.1999    0.2488 -1.65   0.0980 . \n#&gt; num   0.1637    1.1778   0.0478    0.0584  2.80   0.0051 **\n#&gt; size -0.0411    0.9598   0.0703    0.0742 -0.55   0.5799   \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;      exp(coef) exp(-coef) lower .95 upper .95\n#&gt; tx       0.663      1.509     0.407      1.08\n#&gt; num      1.178      0.849     1.050      1.32\n#&gt; size     0.960      1.042     0.830      1.11\n#&gt; \n#&gt; Concordance= 0.624  (se = 0.031 )\n#&gt; Likelihood ratio test= 14.7  on 3 df,   p=0.002\n#&gt; Wald test            = 11.2  on 3 df,   p=0.01\n#&gt; Score (logrank) test = 16.2  on 3 df,   p=0.001,   Robust = 10.8  p=0.01\n#&gt; \n#&gt;   (Note: the likelihood ratio and score tests assume independence of\n#&gt;      observations within a cluster, the Wald and robust score tests do not).\n\n\nrobust se is larger than se, and accounts for the repeated observations from the same individuals:\n\nShow R coderound(bladder.cox2$naive.var, 4)\n#&gt;         [,1]    [,2]   [,3]\n#&gt; [1,]  0.0400 -0.0014 0.0000\n#&gt; [2,] -0.0014  0.0023 0.0007\n#&gt; [3,]  0.0000  0.0007 0.0049\nround(bladder.cox2$var, 4)\n#&gt;         [,1]    [,2]    [,3]\n#&gt; [1,]  0.0619 -0.0026 -0.0004\n#&gt; [2,] -0.0026  0.0034  0.0013\n#&gt; [3,] -0.0004  0.0013  0.0055\n\n\nThese are the ratios of correct confidence intervals to naive ones:\n\nShow R codewith(bladder.cox2, diag(var)/diag(naive.var)) |&gt; sqrt()\n#&gt; [1] 1.244 1.223 1.056\n\n\nWe might try dropping the non-significant size variable:\n\nShow R code#remove non-significant size variable:\nbladder.cox3 = bladder.cox2 |&gt; update(. ~ . - size)\nsummary(bladder.cox3)\n#&gt; Call:\n#&gt; coxph(formula = surv ~ tx + num, data = bladder, cluster = id)\n#&gt; \n#&gt;   n= 190, number of events= 112 \n#&gt; \n#&gt;        coef exp(coef) se(coef) robust se     z Pr(&gt;|z|)   \n#&gt; tx  -0.4117    0.6625   0.2003    0.2515 -1.64   0.1017   \n#&gt; num  0.1700    1.1853   0.0465    0.0564  3.02   0.0026 **\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;     exp(coef) exp(-coef) lower .95 upper .95\n#&gt; tx      0.663      1.509     0.405      1.08\n#&gt; num     1.185      0.844     1.061      1.32\n#&gt; \n#&gt; Concordance= 0.623  (se = 0.031 )\n#&gt; Likelihood ratio test= 14.3  on 2 df,   p=8e-04\n#&gt; Wald test            = 10.2  on 2 df,   p=0.006\n#&gt; Score (logrank) test = 15.8  on 2 df,   p=4e-04,   Robust = 10.6  p=0.005\n#&gt; \n#&gt;   (Note: the likelihood ratio and score tests assume independence of\n#&gt;      observations within a cluster, the Wald and robust score tests do not).\n\n\n\nWays to check PH assumption:\n\ncloglog\nschoenfeld residuals\ninteraction with time",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "proportional-hazards-models.html#age-as-the-time-scale",
    "href": "proportional-hazards-models.html#age-as-the-time-scale",
    "title": "\n6  Proportional Hazards Models\n",
    "section": "\n6.14 Age as the time scale",
    "text": "6.14 Age as the time scale\nSee Canchola et al. (2003).\n\n\n\n\n\n\nCanchola, Alison J, Susan L Stewart, Leslie Bernstein, Dee W West, Ronald K Ross, Dennis Deapen, Richard Pinder, et al. 2003. “Cox Regression Using Different Time-Scales.” Western Users of SAS Software. https://www.lexjansen.com/wuss/2003/DataAnalysis/i-cox_time_scales.pdf.\n\n\nCopelan, Edward A, James C Biggs, James M Thompson, Pamela Crilley, Jeff Szer, John P Klein, Neena Kapoor, Belinda R Avalos, Isabel Cunningham, and Kerry Atkinson. 1991. “Treatment for Acute Myelocytic Leukemia with Allogeneic Bone Marrow Transplantation Following Preparation with BuCy2.” https://doi.org/10.1182/blood.V78.3.838.838.\n\n\nDobson, Annette J, and Adrian G Barnett. 2018. An Introduction to Generalized Linear Models. 4th ed. CRC press. https://doi.org/10.1201/9781315182780.\n\n\nGrambsch, Patricia M, and Terry M Therneau. 1994. “Proportional Hazards Tests and Diagnostics Based on Weighted Residuals.” Biometrika 81 (3): 515–26. https://doi.org/10.1093/biomet/81.3.515.\n\n\nKlein, John P, and Melvin L Moeschberger. 2003. Survival Analysis: Techniques for Censored and Truncated Data. Vol. 1230. Springer. https://link.springer.com/book/10.1007/b97377.\n\n\nKleinbaum, David G, and Mitchel Klein. 2012. Survival Analysis a Self-Learning Text. 3rd ed. Springer. https://link.springer.com/book/10.1007/978-1-4419-6646-9.",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Proportional Hazards Models</span>"
    ]
  },
  {
    "objectID": "parametric-survival-models.html",
    "href": "parametric-survival-models.html",
    "title": "\n7  Parametric survival models\n",
    "section": "",
    "text": "Configuring R\nFunctions from these packages will be used throughout this document:\nShow R codelibrary(conflicted) # check for conflicting function definitions\n# library(printr) # inserts help-file output into markdown output\nlibrary(rmarkdown) # Convert R Markdown documents into a variety of formats.\nlibrary(pander) # format tables for markdown\nlibrary(ggplot2) # graphics\nlibrary(ggeasy) # help with graphics\nlibrary(ggfortify) # help with graphics\nlibrary(dplyr) # manipulate data\nlibrary(tibble) # `tibble`s extend `data.frame`s\nlibrary(magrittr) # `%&gt;%` and other additional piping tools\nlibrary(haven) # import Stata files\nlibrary(knitr) # format R output for markdown\nlibrary(tidyr) # Tools to help to create tidy data\nlibrary(plotly) # interactive graphics\nlibrary(dobson) # datasets from Dobson and Barnett 2018\nlibrary(parameters) # format model output tables for markdown\nlibrary(haven) # import Stata files\nlibrary(latex2exp) # use LaTeX in R code (for figures and tables)\nlibrary(fs) # filesystem path manipulations\nlibrary(survival) # survival analysis\nlibrary(survminer) # survival analysis graphics\nlibrary(KMsurv) # datasets from Klein and Moeschberger\nlibrary(parameters) # format model output tables for\nlibrary(webshot2) # convert interactive content to static for pdf\nlibrary(forcats) # functions for categorical variables (\"factors\")\nlibrary(stringr) # functions for dealing with strings\nlibrary(lubridate) # functions for dealing with dates and times\nHere are some R settings I use in this document:\nShow R coderm(list = ls()) # delete any data that's already loaded into R\n\nconflicts_prefer(dplyr::filter)\nggplot2::theme_set(\n  ggplot2::theme_bw() + \n        # ggplot2::labs(col = \"\") +\n    ggplot2::theme(\n      legend.position = \"bottom\",\n      text = ggplot2::element_text(size = 12, family = \"serif\")))\n\nknitr::opts_chunk$set(message = FALSE)\noptions('digits' = 4)\n\npanderOptions(\"big.mark\", \",\")\npander::panderOptions(\"table.emphasize.rownames\", FALSE)\npander::panderOptions(\"table.split.table\", Inf)\nconflicts_prefer(dplyr::filter) # use the `filter()` function from dplyr() by default\nlegend_text_size = 9",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parametric survival models</span>"
    ]
  },
  {
    "objectID": "parametric-survival-models.html#parametric-survival-models",
    "href": "parametric-survival-models.html#parametric-survival-models",
    "title": "\n7  Parametric survival models\n",
    "section": "\n7.1 Parametric Survival Models",
    "text": "7.1 Parametric Survival Models\n\n7.1.1 Exponential Distribution\n\nThe exponential distribution is the basic distribution for survival analysis.\n\n\\[\n\\begin{aligned}\nf(t) &= \\lambda e^{-\\lambda t}\\\\\n\\text{log}\\left\\{f(t)\\right\\} &= \\text{log}\\left\\{\\lambda\\right\\}-\\lambda t\\\\\nF(t) &= 1-e^{-\\lambda t}\\\\\nS(t)&= e^{-\\lambda t}\\\\\nH(t) &= \\text{log}\\left\\{S(t)\\right\\}\n\\\\   &= -\\lambda t\\\\\nh(t) &= \\lambda\\\\\n\\text{E}(T) &= \\lambda^{-1}\n\\end{aligned}\n\\]\n\n7.1.2 Weibull Distribution\nUsing the Kalbfleisch and Prentice (2002) notation:\n\\[\n\\begin{aligned}\nf(t)&= \\lambda p (\\lambda t)^{p-1}e^{-(\\lambda t)^p}\\\\\nF(t)&=1 - e^{-(\\lambda t)^p}\\\\\nS(t)&=e^{-(\\lambda t)^p}\\\\\nh(t)&=\\lambda p (\\lambda t)^{p-1}\\\\\nH(t)&=(\\lambda t)^p\\\\\n\\text{log}\\left\\{H(t)\\right\\} &= p \\text{log}\\left\\{\\lambda t\\right\\}\n\\\\ &= p \\text{log}\\left\\{\\lambda\\right\\} + p \\text{log}\\left\\{t\\right\\}\n\\\\ \\text{E}(T) &= \\lambda^{-1} \\cdot \\Gamma\\left(1 + \\frac{1}{p}\\right)\n\\end{aligned}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nRecall from calculus:\n\n\\(\\Gamma(t) \\stackrel{\\text{def}}{=}\\int_{u=0}^{\\infty}u^{t-1}e^{-u}du\\)\n\\(\\Gamma(t) = (t-1)!\\) for integers \\(t \\in \\mathbb Z\\)\nIt is implemented by the gamma() function in R.\n\n\n\n\n\n\n\n\n\n\n\nHere are some Weibull density functions, with \\(\\lambda = 1\\) and \\(p\\) varying:\n\nShow R codelibrary(ggplot2)\nlambda = 1\nggplot() +\n  geom_function(\n    aes(col = \"0.25\"),\n    fun = \\(x) dweibull(x, shape = 0.25, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"0.5\"),\n    fun = \\(x) dweibull(x, shape = 0.5, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"1\"),\n    fun = \\(x) dweibull(x, shape = 1, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"1.5\"),\n    fun = \\(x) dweibull(x, shape = 1.5, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"2\"),\n    fun = \\(x) dweibull(x, shape = 2, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"5\"),\n    fun = \\(x) dweibull(x, shape = 5, scale = 1/lambda)) +\n  theme_bw() + \n  xlim(0, 2.5) +\n  ylab(\"f(t)\") +\n  theme(axis.title.y = element_text(angle=0)) +\n  theme(legend.position=\"bottom\") +\n  guides(\n    col = \n      guide_legend(\n        title = \"p\",\n        label.theme = \n          element_text(\n            size = 12)))\n\n\nDensity functions for Weibull distribution\n\n\n\n\nProperties of Weibull hazard functions\n\nTheorem 7.1 If \\(T\\) has a Weibull distribution, then:\n\nWhen \\(p=1\\), the Weibull distribution simplifies to the exponential distribution\nWhen \\(p &gt; 1\\), the hazard is increasing: \\(h'(t) &gt; 0\\)\n\nWhen \\(p &lt; 1\\), the hazard is decreasing: \\(h'(t) &lt; 0\\)\n\n\n\\(\\text{log}\\left\\{H(t)\\right\\}\\) is a straight line relative to \\(\\text{log}\\left\\{t\\right\\}\\): \\(\\text{log}\\left\\{H(t)\\right\\} = p \\text{log}\\left\\{\\lambda\\right\\} + p \\text{log}\\left\\{t\\right\\}\\)\n\n\n\n\n\nExercise 7.1 Prove Theorem 7.1.\n\n\n\nThe Weibull distribution provides more flexibility than the exponential. Figure 7.1 shows some Weibull hazard functions, with \\(\\lambda = 1\\) and \\(p\\) varying:\n\n\nShow R codelibrary(ggplot2)\nlibrary(eha)\nlambda = 1\n\nggplot() +\n  geom_function(\n    aes(col = \"0.25\"),\n    fun = \\(x) hweibull(x, shape = 0.25, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"0.5\"),\n    fun = \\(x) hweibull(x, shape = 0.5, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"1\"),\n    fun = \\(x) hweibull(x, shape = 1, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"1.5\"),\n    fun = \\(x) hweibull(x, shape = 1.5, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"2\"),\n    fun = \\(x) hweibull(x, shape = 2, scale = 1/lambda)) +\n  theme_bw() + \n  xlim(0, 2.5) + \n  ylab(\"h(t)\") +\n  theme(axis.title.y = element_text(angle=0)) +\n  theme(legend.position=\"bottom\") +\n  guides(\n    col = \n      guide_legend(\n        title = \"p\",\n        label.theme = \n          element_text(\n            size = 12)))\n\n\n\nFigure 7.1: Hazard functions for Weibull distribution\n\n\n\n\n\n\n\n\n\nShow R codelibrary(ggplot2)\nlambda = 1\n\nggplot() +\n  geom_function(\n    aes(col = \"0.25\"),\n    fun = \\(x) pweibull(lower = FALSE, x, shape = 0.25, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"0.5\"),\n    fun = \\(x) pweibull(lower = FALSE, x, shape = 0.5, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"1\"),\n    fun = \\(x) pweibull(lower = FALSE, x, shape = 1, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"1.5\"),\n    fun = \\(x) pweibull(lower = FALSE, x, shape = 1.5, scale = 1/lambda)) +\n  geom_function(\n    aes(col = \"2\"),\n    fun = \\(x) pweibull(lower = FALSE, x, shape = 2, scale = 1/lambda)) +\n  theme_bw() + \n  xlim(0, 2.5) + \n  ylab(\"S(t)\") +\n  theme(axis.title.y = element_text(angle=0)) +\n  theme(legend.position=\"bottom\") +\n  guides(\n    col = \n      guide_legend(\n        title = \"p\",\n        label.theme = \n          element_text(\n            size = 12)))\n\n\n\nFigure 7.2: Survival functions for Weibull distribution\n\n\n\n\n\n\n\n\n7.1.3 Exponential Regression\nFor each subject \\(i\\), define a linear predictor:\n\\[\n\\begin{aligned}\n\\eta(\\boldsymbol x) &= \\beta_0 + (\\beta_1x_1 + \\dots + \\beta_p x_p)\\\\\nh(t|\\boldsymbol x) &= \\text{exp}\\left\\{\\eta(\\boldsymbol x)\\right\\}\\\\\nh_0 &\\stackrel{\\text{def}}{=} h(t|\\boldsymbol 0)\\\\\n&= \\text{exp}\\left\\{\\eta(\\boldsymbol 0)\\right\\}\\\\\n&= \\text{exp}\\left\\{\\beta_0 + (\\beta_1 \\cdot 0 + \\dots + \\beta_p \\cdot 0)\\right\\}\\\\\n&= \\text{exp}\\left\\{\\beta_0 + 0\\right\\}\\\\\n&= \\text{exp}\\left\\{\\beta_0\\right\\}\\\\\n\\end{aligned}\n\\]\nWe let the linear predictor have a constant term, and when there are no additional predictors the hazard is \\(\\lambda = \\text{exp}\\left\\{\\beta_0\\right\\}\\). This has a log link as in a generalized linear model. Since the hazard does not depend on \\(t\\), the hazards are (trivially) proportional.\n\n7.1.4 Accelerated Failure Time\nPreviously, we assumed the hazards were proportional; that is, the covariates multiplied the baseline hazard function:\n\\[\n\\begin{aligned}\nh(T=t|X=x)\n&\\stackrel{\\text{def}}{=} p(T=t|X=x,T \\ge t)\\\\\n&= h(t|X=0)\\cdot \\text{exp}\\left\\{\\eta(x)\\right\\}\\\\\n&= h(t|X=0)\\cdot \\theta(x)\\\\\n&= h_0(t)\\cdot \\theta(x)\n\\end{aligned}\n\\]\nand correspondingly,\n\\[\n\\begin{aligned}\nH(t|x)\n&= \\theta(x)H_0(t)\\\\\nS(t|x)\n&= \\text{exp}\\left\\{-H(t|x)\\right\\}\\\\\n&= \\text{exp}\\left\\{-\\theta(x)\\cdot H_0(t)\\right\\}\\\\\n&= \\left(\\text{exp}\\left\\{- H_0(t)\\right\\}\\right)^{\\theta(x)}\\\\\n&= \\left(S_0(t)\\right)^{\\theta(x)}\\\\\n\\end{aligned}\n\\]\nAn alternative modeling assumption would be \\[S(t|X=x)=S_0(t\\cdot \\theta(x))\\] where \\(\\theta(x)=\\text{exp}\\left\\{\\eta(x)\\right\\}\\), \\(\\eta(x) =\\beta_1x_1+\\cdots+\\beta_px_p\\), and \\(S_0(t)=P(T\\ge t|X=0)\\) is the base survival function.\nThen\n\\[\n\\begin{aligned}\nE(T|X=x)\n&= \\int_{t=0}^{\\infty} S(t|x)dt\\\\\n&= \\int_{t=0}^{\\infty} S_0(t\\cdot \\theta(x))dt\\\\\n&= \\int_{u=0}^{\\infty} S_0(u)du \\cdot \\theta(x)^{-1}\\\\\n&= \\theta(x)^{-1} \\cdot \\int_{u=0}^{\\infty} S_0(u)du\\\\\n&= \\theta(x)^{-1} \\cdot \\text{E}(T|X=0)\\\\\n\\end{aligned}\n\\] So the mean of \\(T\\) given \\(X=x\\) is the baseline mean divided by \\(\\theta(x) = \\text{exp}\\left\\{\\eta(x)\\right\\}\\).\nThis modeling strategy is called an accelerated failure time model, because covariates cause uniform acceleration (or slowing) of failure times.\nAdditionally:\n\\[\n\\begin{aligned}\nH(t|x) &= H_0(\\theta(x)\\cdot t)\\\\\nh(t|x) &= \\theta(x) \\cdot h_0(\\theta(x)\\cdot t)\n\\end{aligned}\n\\]\nIf the base distribution is exponential with parameter \\(\\lambda\\) then\n\\[\n\\begin{aligned}\nS(t|x)\n&= \\text{exp}\\left\\{-\\lambda \\cdot t \\theta(x)\\right\\}\\\\\n&= [\\text{exp}\\left\\{-\\lambda t\\right\\}]^{\\theta(x)}\\\\\n\\end{aligned}\n\\]\nwhich is an exponential model with base hazard multiplied by \\(\\theta(x)\\), which is also the proportional hazards model.\n\nIn terms of the log survival time \\(Y=\\text{log}\\left\\{T\\right\\}\\) the model can be written as\n\\[\n\\begin{aligned}\nY&=\\alpha-\\eta+W\\\\\n\\alpha&= -\\text{log}\\left\\{\\lambda\\right\\}\n\\end{aligned}\n\\]\nwhere \\(W\\) has the extreme value distribution. The estimated parameter \\(\\lambda\\) is the intercept and the other coefficients are those of \\(\\eta\\), which will be the opposite sign of those for coxph.\n\nFor a Weibull distribution, the hazard function and the survival function are\n\\[\n\\begin{aligned}\nh(t)&=\\lambda p (\\lambda t)^{p-1}\\\\\nS(t)&=e^{-(\\lambda t)^p}\n\\end{aligned}\n\\]\nWe can construct a proportional hazards model by using a linear predictor \\(\\eta_i\\) without constant term and letting \\(\\theta_i=e^{\\eta_i}\\) we have\n\\[\n\\begin{aligned}\nh(t)&=\\lambda p (\\lambda t)^{p-1}\\theta_i\n\\end{aligned}\n\\]\nA distribution with \\(h(t)=\\lambda p (\\lambda t)^{p-1}\\theta_i\\) is a Weibull distribution with parameters \\(\\lambda^*=\\lambda \\theta_i^{1/p}\\) and \\(p\\) so the survival function is\n\\[\n\\begin{aligned}\nS^*(t)&=e^{-(\\lambda^* t)^p}\\\\\n&=e^{-(\\lambda \\theta^{1/p} t)^p}\\\\\n&= S(t\\theta^{1/p})\n\\end{aligned}\n\\]\nso this is also an accelerated failure time model.\n\nIn terms of the log survival time \\(Y=\\text{log}\\left\\{T\\right\\}\\) the model can be written as\n\\[\n\\begin{aligned}\nY&=\\alpha-\\sigma\\eta+\\sigma W\\\\\n\\alpha&= -\\text{log}\\left\\{\\lambda\\right\\}\\\\\n\\sigma &= 1/p\n\\end{aligned}\n\\]\nwhere \\(W\\) has the extreme value distribution. The estimated parameter \\(\\lambda\\) is the intercept and the other coefficients are those of \\(\\eta\\), which will be the opposite sign of those for coxph.\n\nThese AFT models are log-linear, meaning that the linear predictor has a log link. The exponential and the Weibull are the only log-linear models that are simultaneously proportional hazards models. Other parametric distributions can be used for survival regression either as a proportional hazards model or as an accelerated failure time model.\n\n7.1.5 Dataset: Leukemia treatments\nRemission survival times on 42 leukemia patients, half on new treatment, half on standard treatment.\nThis is the same data as the drug6mp data from KMsurv, but with two other variables and without the pairing.\n\nShow R codelibrary(haven)\nlibrary(survival)\nanderson = \n  paste0(\n    \"http://web1.sph.emory.edu/dkleinb/allDatasets\",\n    \"/surv2datasets/anderson.dta\") |&gt; \n  read_dta() |&gt; \n  mutate(\n    status = status |&gt; \n      case_match(\n        1 ~ \"relapse\",\n        0 ~ \"censored\"\n      ),\n    sex = sex |&gt; \n      case_match(\n        0 ~ \"female\",\n        1 ~ \"male\"\n      ),\n    \n    rx = rx |&gt; \n      case_match(\n        0 ~ \"new\",\n        1 ~ \"standard\"\n      ),\n    \n    surv = Surv(time = survt,event = (status == \"relapse\"))\n  ) \n\nprint(anderson)\n\n\nCox semi-parametric model\n\nShow R code\nanderson.cox0 = coxph(\n  formula = surv ~ rx,\n  data = anderson)\nsummary(anderson.cox0)\n#&gt; Call:\n#&gt; coxph(formula = surv ~ rx, data = anderson)\n#&gt; \n#&gt;   n= 42, number of events= 30 \n#&gt; \n#&gt;             coef exp(coef) se(coef)    z Pr(&gt;|z|)    \n#&gt; rxstandard 1.572     4.817    0.412 3.81  0.00014 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt;            exp(coef) exp(-coef) lower .95 upper .95\n#&gt; rxstandard      4.82      0.208      2.15      10.8\n#&gt; \n#&gt; Concordance= 0.69  (se = 0.041 )\n#&gt; Likelihood ratio test= 16.4  on 1 df,   p=5e-05\n#&gt; Wald test            = 14.5  on 1 df,   p=1e-04\n#&gt; Score (logrank) test = 17.2  on 1 df,   p=3e-05\n\n\nWeibull parametric model\n\nShow R codeanderson.weib &lt;- survreg(\n  formula = surv ~ rx,\n  data = anderson,\n  dist = \"weibull\")\nsummary(anderson.weib)\n#&gt; \n#&gt; Call:\n#&gt; survreg(formula = surv ~ rx, data = anderson, dist = \"weibull\")\n#&gt;              Value Std. Error     z       p\n#&gt; (Intercept)  3.516      0.252 13.96 &lt; 2e-16\n#&gt; rxstandard  -1.267      0.311 -4.08 4.5e-05\n#&gt; Log(scale)  -0.312      0.147 -2.12   0.034\n#&gt; \n#&gt; Scale= 0.732 \n#&gt; \n#&gt; Weibull distribution\n#&gt; Loglik(model)= -106.6   Loglik(intercept only)= -116.4\n#&gt;  Chisq= 19.65 on 1 degrees of freedom, p= 9.3e-06 \n#&gt; Number of Newton-Raphson Iterations: 5 \n#&gt; n= 42\n\n\nExponential parametric model\n\nShow R codeanderson.exp &lt;- survreg(\n  formula = surv ~ rx,\n  data = anderson,\n  dist = \"exp\")\nsummary(anderson.exp)\n#&gt; \n#&gt; Call:\n#&gt; survreg(formula = surv ~ rx, data = anderson, dist = \"exp\")\n#&gt;              Value Std. Error     z       p\n#&gt; (Intercept)  3.686      0.333 11.06 &lt; 2e-16\n#&gt; rxstandard  -1.527      0.398 -3.83 0.00013\n#&gt; \n#&gt; Scale fixed at 1 \n#&gt; \n#&gt; Exponential distribution\n#&gt; Loglik(model)= -108.5   Loglik(intercept only)= -116.8\n#&gt;  Chisq= 16.49 on 1 degrees of freedom, p= 4.9e-05 \n#&gt; Number of Newton-Raphson Iterations: 4 \n#&gt; n= 42\n\n\nDiagnostic - complementary log-log survival plot\n\nShow R codelibrary(survminer)\nsurvfit(\n  formula = surv ~ rx,\n  data = anderson) |&gt; \n  ggsurvplot(fun = \"cloglog\")\n\n\n\n\n\n\n\nIf the cloglog plot is linear, then a Weibull model may be ok.",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parametric survival models</span>"
    ]
  },
  {
    "objectID": "parametric-survival-models.html#combining-left-truncation-and-interval-censoring",
    "href": "parametric-survival-models.html#combining-left-truncation-and-interval-censoring",
    "title": "\n7  Parametric survival models\n",
    "section": "\n7.2 Combining left-truncation and interval-censoring",
    "text": "7.2 Combining left-truncation and interval-censoring\nFrom [https://stat.ethz.ch/pipermail/r-help/2015-August/431733.html]:\n\ncoxph does left truncation but not left (or interval) censoring survreg does interval censoring but not left truncation (or time dependent covariates).\n\n\nTerry Therneau, August 31, 2015",
    "crumbs": [
      "Time to Event Models",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Parametric survival models</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Agresti, Alan. 2012. Categorical Data Analysis. Vol. 792. John\nWiley & Sons. https://www.wiley.com/en-us/Categorical+Data+Analysis%2C+3rd+Edition-p-9780470463635.\n\n\n———. 2015. Foundations of Linear and Generalized Linear Models.\nJohn Wiley & Sons. https://www.wiley.com/en-us/Foundations+of+Linear+and+Generalized+Linear+Models-p-9781118730034.\n\n\n———. 2018. An Introduction to Categorical Data Analysis. John\nWiley & Sons. https://www.wiley.com/en-us/An+Introduction+to+Categorical+Data+Analysis%2C+3rd+Edition-p-9781119405283.\n\n\nAnderson, Edgar. 1935. “The Irises of the Gaspe Peninsula.”\nBulletin of American Iris Society 59: 2–5.\n\n\nBache, Stefan Milton, and Hadley Wickham. 2022. Magrittr: A\nForward-Pipe Operator for r. https://CRAN.R-project.org/package=magrittr.\n\n\nBanerjee, Sudipto, and Anindya Roy. 2014. Linear Algebra and Matrix\nAnalysis for Statistics. Vol. 181. Crc Press Boca Raton. https://www.routledge.com/Linear-Algebra-and-Matrix-Analysis-for-Statistics/Banerjee-Roy/p/book/9781420095388.\n\n\nCanchola, Alison J, Susan L Stewart, Leslie Bernstein, Dee W West,\nRonald K Ross, Dennis Deapen, Richard Pinder, et al. 2003. “Cox\nRegression Using Different Time-Scales.” Western Users of SAS\nSoftware. https://www.lexjansen.com/wuss/2003/DataAnalysis/i-cox_time_scales.pdf.\n\n\nCasella, George, and Roger Berger. 2002. Statistical Inference.\n2nd ed. Cengage Learning. https://www.cengage.com/c/statistical-inference-2e-casella-berger/9780534243128/.\n\n\nChang, Winston. 2024. R Graphics Cookbook: Practical Recipes for\nVisualizing Data. O’Reilly Media. https://r-graphics.org/.\n\n\nChatterjee, Samprit, and Ali S Hadi. 2015. Regression Analysis by\nExample. John Wiley & Sons. https://www.wiley.com/en-us/Regression+Analysis+by+Example%2C+4th+Edition-p-9780470055458.\n\n\nClayton, David, and Michael Hills. 2013. Statistical Models in\nEpidemiology. Oxford University Press. https://global.oup.com/academic/product/statistical-models-in-epidemiology-9780199671182.\n\n\nCopelan, Edward A, James C Biggs, James M Thompson, Pamela Crilley, Jeff\nSzer, John P Klein, Neena Kapoor, Belinda R Avalos, Isabel Cunningham,\nand Kerry Atkinson. 1991. “Treatment for Acute Myelocytic Leukemia\nwith Allogeneic Bone Marrow Transplantation Following Preparation with\nBuCy2.” https://doi.org/10.1182/blood.V78.3.838.838.\n\n\nDalgaard, Peter. 2008. Introductory Statistics with r. New\nYork, NY: Springer New York. https://link.springer.com/book/10.1007/978-0-387-79054-1.\n\n\nDobson, Annette J, and Adrian G Barnett. 2018. An Introduction to\nGeneralized Linear Models. 4th ed. CRC press. https://doi.org/10.1201/9781315182780.\n\n\nDunn, Peter K, Gordon K Smyth, et al. 2018. Generalized Linear\nModels with Examples in r. Vol. 53. Springer. https://link.springer.com/book/10.1007/978-1-4419-0118-7.\n\n\nEfron, Bradley, and David V Hinkley. 1978. “Assessing the Accuracy\nof the Maximum Likelihood Estimator: Observed Versus Expected Fisher\nInformation.” Biometrika 65 (3): 457–83.\n\n\nFaraway, Julian J. 2016. Extending the Linear Model with r:\nGeneralized Linear, Mixed Effects and Nonparametric Regression\nModels. 2nd ed. Chapman; Hall/CRC. https://doi.org/10.1201/9781315382722.\n\n\nFay, Colin, Sébastien Rochette, Vincent Guyader, and Cervan Girard.\n2021. Engineering Production-Grade Shiny Apps. Chapman;\nHall/CRC. https://engineering-shiny.org/.\n\n\nFieller, Nick. 2016. Basics of Matrix Algebra for Statistics with\nr. Chapman; Hall/CRC. https://doi.org/10.1201/9781315370200.\n\n\nFox, John. 2015. Applied Regression Analysis and Generalized Linear\nModels. Sage publications.\n\n\nGrambsch, Patricia M, and Terry M Therneau. 1994. “Proportional\nHazards Tests and Diagnostics Based on Weighted Residuals.”\nBiometrika 81 (3): 515–26. https://doi.org/10.1093/biomet/81.3.515.\n\n\nHarrell, Frank E. 2015. Regression Modeling Strategies: With\nApplications to Linear Models, Logistic Regression, and Survival\nAnalysis. 2nd ed. Springer. https://doi.org/10.1007/978-3-319-19425-7.\n\n\nHosmer, David W, Stanley Lemeshow, and Rodney X Sturdivant. 2013.\nApplied Logistic Regression. John Wiley & Sons. https://onlinelibrary.wiley.com/doi/book/10.1002/9781118548387.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al.\n2013. An Introduction to Statistical Learning. Vol. 112.\nSpringer. https://www.statlearning.com/.\n\n\nJewell, Nicholas P. 2003. Statistics for Epidemiology. Oxford,\nUK: Chapman; Hall/CRC. https://www.routledge.com/Statistics-for-Epidemiology/Jewell/p/book/9781584884330.\n\n\nKalbfleisch, John D, and Ross L Prentice. 2011. The Statistical\nAnalysis of Failure Time Data. John Wiley & Sons.\n\n\nKhuri, André I. 2003. Advanced Calculus with Applications in\nStatistics. John Wiley & Sons. https://www.wiley.com/en-us/Advanced+Calculus+with+Applications+in+Statistics%2C+2nd+Edition-p-9780471391043.\n\n\nKlein, John P, and Melvin L Moeschberger. 2003. Survival Analysis:\nTechniques for Censored and Truncated Data. Vol. 1230. Springer. https://link.springer.com/book/10.1007/b97377.\n\n\nKleinbaum, David G, and Mitchel Klein. 2010. Logistic\nRegression. 3rd ed. Springer. https://link.springer.com/book/10.1007/978-1-4419-1742-3.\n\n\n———. 2012. Survival Analysis a Self-Learning Text. 3rd ed.\nSpringer. https://link.springer.com/book/10.1007/978-1-4419-6646-9.\n\n\nKleinbaum, David G, Lawrence L Kupper, Azhar Nizam, K Muller, and ES\nRosenberg. 2014. Applied Regression Analysis and Other Multivariable\nMethods. 5th ed. Cengage Learning. https://www.cengage.com/c/applied-regression-analysis-and-other-multivariable-methods-5e-kleinbaum/9781285051086/.\n\n\nKleinman, Ken, and Nicholas J Horton. 2009. SAS and r: Data\nManagement, Statistical Analysis, and Graphics. Chapman; Hall/CRC.\nhttps://www.routledge.com/SAS-and-R-Data-Management-Statistical-Analysis-and-Graphics-Second-Edition/Kleinman-Horton/p/book/9781466584495.\n\n\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with r. \"\nO’Reilly Media, Inc.\". https://www.tmwr.org/.\n\n\nKutner, Michael H, Christopher J Nachtsheim, John Neter, and William Li.\n2005. Applied Linear Statistical Models. McGraw-Hill.\n\n\nLawrance, Rachael, Evgeny Degtyarev, Philip Griffiths, Peter Trask,\nHelen Lau, Denise D’Alessio, Ingolf Griebsch, Gudrun Wallenstein, Kim\nCocks, and Kaspar Rufibach. 2020. “What Is an Estimand, and How\nDoes It Relate to Quantifying the Effect of Treatment on\nPatient-Reported Quality of Life Outcomes in Clinical Trials?”\nJournal of Patient-Reported Outcomes 4 (1): 1–8. https://link.springer.com/article/10.1186/s41687-020-00218-5.\n\n\nMcCullagh, Peter, and J. A. Nelder. 1989. Generalized Linear\nModels. 2nd ed. Routledge. https://www.utstat.toronto.edu/~brunner/oldclass/2201s11/readings/glmbook.pdf.\n\n\nMcLachlan, Geoffrey J, and Thriyambakam Krishnan. 2007. The EM\nAlgorithm and Extensions. 2nd ed. John Wiley & Sons. https://doi.org/10.1002/9780470191613.\n\n\nMoore, Dirk F. 2016. Applied Survival Analysis Using r. Vol.\n473. Springer. https://doi.org/10.1007/978-3-319-31245-3.\n\n\nNahhas, Ramzi W. 2023. An Introduction to r for Research. https://bookdown.org/rwnahhas/IntroToR/.\n\n\n———. 2024. Introduction to Regression Methods for Public Health\nUsing r. CRC Press. https://www.bookdown.org/rwnahhas/RMPH/.\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial Data Science: With\nApplications in R. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nPohl, Moritz, Lukas Baumann, Rouven Behnisch, Marietta Kirchner,\nJohannes Krisam, and Anja Sander. 2021. “Estimands—A Basic Element for Clinical\nTrials.” Deutsches Ärzteblatt\nInternational 118 (51-52): 883–88. https://doi.org/10.3238/arztebl.m2021.0373.\n\n\nPolin, Richard A, William W Fox, and Steven H Abman. 2011. Fetal and\nNeonatal Physiology. 4th ed. Elsevier health sciences.\n\n\nRosenman, Ray H, Richard J Brand, C David Jenkins, Meyer Friedman,\nReuben Straus, and Moses Wurm. 1975. “Coronary Heart Disease in\nthe Western Collaborative Group Study: Final Follow-up Experience of 8\n1/2 Years.” JAMA 233 (8): 872–77. https://doi.org/10.1001/jama.1975.03260080034016.\n\n\nSearle, Shayle R, and Andre I Khuri. 2017. Matrix Algebra Useful for\nStatistics. John Wiley & Sons.\n\n\nSeber, George AF, and Alan J Lee. 2012. Linear Regression\nAnalysis. 2nd ed. John Wiley & Sons. https://www.wiley.com/en-us/Linear+Regression+Analysis%2C+2nd+Edition-p-9781118274422.\n\n\nSelvin, Steve. 2001. Epidemiologic Analysis: A Case-Oriented\nApproach. Oxford University Press.\n\n\nVan Buuren, Stef. 2018. Flexible Imputation of Missing Data.\nCRC press. https://stefvanbuuren.name/fimd/.\n\n\nVenables, Bill. 2023. codingMatrices: Alternative Factor Coding\nMatrices for Linear Model Formulae. https://CRAN.R-project.org/package=codingMatrices.\n\n\nVittinghoff, Eric, David V Glidden, Stephen C Shiboski, and Charles E\nMcCulloch. 2012. Regression Methods in Biostatistics: Linear,\nLogistic, Survival, and Repeated Measures Models. 2nd ed. Springer.\nhttps://doi.org/10.1007/978-1-4614-1353-0.\n\n\nWickham, Hadley. 2019. Advanced r. Chapman; Hall/CRC. https://adv-r.hadley.nz/index.html.\n\n\n———. 2021. Mastering Shiny. \" O’Reilly Media, Inc.\". https://mastering-shiny.org/.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy\nD’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019.\n“Welcome to the tidyverse.”\nJournal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. R Packages. \"\nO’Reilly Media, Inc.\". https://r-pkgs.org/.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023.\nR for Data Science. \" O’Reilly Media, Inc.\". https://r4ds.hadley.nz/.\n\n\nWoodward, Mark. 2013. Epidemiology: Study Design and Data\nAnalysis. CRC press. https://www.routledge.com/Epidemiology-Study-Design-and-Data-Analysis-Third-Edition/Woodward/p/book/9781439839706.\n\n\nZeileis, Achim, Christian Kleiber, and Simon Jackman. 2008.\n“Regression Models for Count Data in R.”\nJournal of Statistical Software 27 (8). https://www.jstatsoft.org/v27/i08/.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "math-prereqs.html",
    "href": "math-prereqs.html",
    "title": "Appendix A — Mathematics",
    "section": "",
    "text": "A.1 Elementary Algebra",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematics</span>"
    ]
  },
  {
    "objectID": "math-prereqs.html#elementary-algebra",
    "href": "math-prereqs.html#elementary-algebra",
    "title": "Appendix A — Mathematics",
    "section": "",
    "text": "Mastery of Elementary Algebra (a.k.a. “College Algebra”) is a prerequisite for calculus, which is a prerequisite for Epi 202 and Epi 203, which are prerequisites for this course (Epi 204). Nevertheless, each year, some Epi 204 students are still uncomfortable with algebraic manipulations of mathematical formulas. Therefore, I include this section as a quick reference.\n\n\nA.1.1 Equalities\n\nTheorem A.1 (Equalities are transitive) If \\(a=b\\) and \\(b=c\\), then \\(a=c\\)\n\n\n\nTheorem A.2 (Substituting equivalent expressions) If \\(a = b\\), then for any function \\(f(x)\\), \\(f(a) = f(b)\\)\n\n\n\n\nA.1.2 Inequalities\n\nTheorem A.3 If \\(a&lt;b\\), then \\(a+c &lt; b+c\\)\n\n\n\nTheorem A.4 (negating both sides of an inequality) If \\(a &lt; b\\), then: \\(-a &gt; -b\\)\n\n\n\nTheorem A.5 If \\(a &lt; b\\) and \\(c \\geq 0\\), then \\(ca &lt; cb\\).\n\n\n\nTheorem A.6 \\[-a = (-1)*a\\]\n\n\n\n\nA.1.3 Sums\n\nTheorem A.7 (adding zero changes nothing) \\[a+0=a\\]\n\n\n\nTheorem A.8 (Sums are symmetric) \\[a+b = b+a\\]\n\n\n\nTheorem A.9 (Sums are associative)  \n\nWhen summing three or more terms, the order in which you sum them does not matter:\n\n\\[(a + b) + c = a + (b + c)\\]\n\n\n\n\nA.1.4 Products\n\n\nTheorem A.10 (Multiplying by 1 changes nothing) \\[a \\times 1 = a\\]\n\n\n\nTheorem A.11 (Products are symmetric) \\[a \\times b = b \\times a\\]\n\n\n\nTheorem A.12 (Products are associative) \\[(a \\times b) \\times c = a \\times (b \\times c)\\]\n\n\n\nA.1.5 Sums and products together\n\n\nTheorem A.13 (Multiplication is distributive) \\[a(b+c) = ab + ac\\]\n\n\n\n\nA.1.6 Quotients\n\nDefinition A.1 (Quotients, fractions, rates)  \n\nA quotient, fraction, or rate is a division of one quantity by another:\n\n\\[\\frac{a}{b}\\]\n\nIn epidemiology, rates typically have a quantity involving time or population in the denominator.\nc.f. https://en.wikipedia.org/wiki/Rate_(mathematics)\n\n\n\nDefinition A.2 (Ratios) A ratio is a quotient in which the numerator and denominator are measured using the same unit scales.\n\nc.f. https://en.wikipedia.org/wiki/Ratio\n\n\n\nAdditional reference: https://en.wikipedia.org/wiki/Elementary_algebra",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematics</span>"
    ]
  },
  {
    "objectID": "math-prereqs.html#exponentials-and-logarithms",
    "href": "math-prereqs.html#exponentials-and-logarithms",
    "title": "Appendix A — Mathematics",
    "section": "A.2 Exponentials and Logarithms",
    "text": "A.2 Exponentials and Logarithms\n\nTheorem A.14 (logarithm of a product is the sum of the logs of the factors) \\[\n\\text{log}\\left\\{a\\cdot b\\right\\} = \\text{log}\\left\\{a\\right\\} + \\text{log}\\left\\{b\\right\\}\n\\]\n\n\nCorollary A.1 (logarithm of a quotient)  \n\nThe logarithm of a quotient is equal to the difference of the logs of the factors:\n\n\\[\\text{log}\\left\\{\\frac{a}{b}\\right\\} = \\text{log}\\left\\{a\\right\\} - \\text{log}\\left\\{b\\right\\}\\]\n\n\nTheorem A.15 (logarithm of an exponential function) \\[\n\\text{log}\\left\\{a^b\\right\\} = b \\cdot\\text{log}\\left\\{a\\right\\}\n\\]\n\n\nTheorem A.16 (exponential of a sum)  \n\nThe exponential of a sum is equal to the product of the exponentials of the addends:\n\n\\[\\text{exp}\\left\\{a+b\\right\\} = \\text{exp}\\left\\{a\\right\\} \\cdot\\text{exp}\\left\\{b\\right\\}\\]\n\n\nCorollary A.2 (exponential of a difference)  \n\nThe exponential of a difference is equal to the quotient of the exponentials of the addends:\n\n\\[\\text{exp}\\left\\{a-b\\right\\} = \\frac{\\text{exp}\\left\\{a\\right\\}}{\\text{exp}\\left\\{b\\right\\}}\\]\n\n\n\nTheorem A.17 (exponential of a product) \\[a^{bc} = \\left(a^b\\right)^c = \\left(a^c\\right)^b\\]\n\n\n\nCorollary A.3 (natural exponential of a product) \\[\\text{exp}\\left\\{ab\\right\\} = (\\text{exp}\\left\\{a\\right\\})^b = (\\text{exp}\\left\\{b\\right\\})^a\\]\n\n\n\nTheorem A.18 (exp{} and log{} are mutual inverses) \\[\\text{exp}\\left\\{\\text{log}\\left\\{a\\right\\}\\right\\} = \\text{log}\\left\\{\\text{exp}\\left\\{a\\right\\}\\right\\} = a\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematics</span>"
    ]
  },
  {
    "objectID": "math-prereqs.html#derivatives",
    "href": "math-prereqs.html#derivatives",
    "title": "Appendix A — Mathematics",
    "section": "A.3 Derivatives",
    "text": "A.3 Derivatives\n\nTheorem A.19 (Derivatives of polynomials) \\[\\frac{\\partial}{\\partial x}x^q = qx^{q-1}\\]\n\n\n\nTheorem A.20 (derivative of natural logarithm) \\[\\text{log}'\\left\\{x\\right\\} = \\frac{1}{x} = x^{-1}\\]\n\n\nTheorem A.21 (derivative of exponential) \\[\\text{exp}'\\left\\{x\\right\\} = \\text{exp}\\left\\{x\\right\\}\\]\n\n\n\nTheorem A.22 (Product rule) \\[(ab)' = ab' + ba'\\]\n\n\nTheorem A.23 (Quotient rule) \\[(a/b)' = 1/b - (a/b^2)b'\\]\n\n\nTheorem A.24 (Chain rule) \\[\\frac{\\partial a}{\\partial c} = \\frac{\\partial a}{\\partial b} \\frac{\\partial b}{\\partial c}\\]\n\n\n\nCorollary A.4 (Chain rule for logarithms) \\[\n\\frac{\\partial}{\\partial x}\\text{log}\\left\\{f(x)\\right\\} = \\frac{f'(x)}{f(x)}\n\\]\n\n\nProof. Apply Theorem A.24 and Theorem A.20.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematics</span>"
    ]
  },
  {
    "objectID": "math-prereqs.html#sec-vector-calculus",
    "href": "math-prereqs.html#sec-vector-calculus",
    "title": "Appendix A — Mathematics",
    "section": "A.4 Vector Calculus",
    "text": "A.4 Vector Calculus\n(adapted from Fieller (2016), §7.2)\nLet \\(\\tilde{x}\\) and \\(\\tilde{\\beta}\\) be vectors of length \\(p\\), or in other words, matrices of length \\(p \\times 1\\):\n\\[\n\\tilde{x}= \\begin{bmatrix}\nx_{1} \\\\\nx_{2} \\\\\n\\vdots \\\\\nx_{p}\n\\end{bmatrix} \\\\\n\\]\n\\[\n\\tilde{\\beta}= \\begin{bmatrix}\n\\beta_{1} \\\\\n\\beta_{2} \\\\\n\\vdots \\\\\n\\beta_{p}\n\\end{bmatrix}\n\\]\n\nDefinition A.3 (Transpose) The transpose of a row vector is the column vector with the same sequence of entries:\n\\[\n\\tilde{x}' \\equiv \\tilde{x}^\\top \\equiv [x_1, x_2, ..., x_p]\n\\]\n\n\nExample A.1 (Dot product as matrix multiplication) \\[\n\\begin{aligned}\n\\tilde{x}\\cdot \\tilde{\\beta}\n&= \\tilde{x}^{\\top} \\tilde{\\beta}\n\\\\ &= [x_1, x_2, ..., x_p]\n\\begin{bmatrix}\n\\beta_{1} \\\\\n\\beta_{2} \\\\\n\\vdots \\\\\n\\beta_{p}\n\\end{bmatrix}\n\\\\ &=\nx_1\\beta_1+x_2\\beta_2 +...+x_p \\beta_p\n\\end{aligned}\n\\]\n\n\nTheorem A.25 (Transpose of a sum) \\[(\\tilde{x}+\\tilde{y})^{\\top} = \\tilde{x}^{\\top} + \\tilde{y}^{\\top}\\]\n\n\n\nDefinition A.4 (Vector derivative of a vector-to-scalar function) If \\(f(\\tilde{\\beta})\\) is a function that takes a vector \\(\\tilde{\\beta}\\) as input and outputs a scalar, such as \\(f(\\tilde{\\beta}) = x'\\tilde{\\beta}\\), then:\n\\[\n\\frac{\\partial}{\\partial  \\tilde{\\beta}} f(\\tilde{\\beta}) =\n\\begin{bmatrix}\n\\frac{\\partial}{\\partial \\beta_1}f(\\tilde{\\beta}) \\\\\n\\frac{\\partial}{\\partial \\beta_2}f(\\tilde{\\beta}) \\\\\n\\vdots \\\\\n\\frac{\\partial}{\\partial \\beta_p}f(\\tilde{\\beta})\n\\end{bmatrix}\n\\]\n\n\n\nTheorem A.26 (Derivative of a linear combination) \\[\n\\frac{\\partial}{\\partial \\tilde{\\beta}} \\tilde{x}^{\\top} \\tilde{\\beta}= x\n\\]\n\nThis looks a lot like non-vector calculus, except that you have to transpose the coefficient.\n\n\n\n\nProof. \\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial  \\beta} (x^{\\top}\\beta)\n&=\n\\begin{bmatrix}\n\\frac{\\partial}{\\partial \\beta_1}(x_1\\beta_1+x_2\\beta_2 +...+x_p \\beta_p ) \\\\\n\\frac{\\partial}{\\partial \\beta_2}(x_1\\beta_1+x_2\\beta_2 +...+x_p \\beta_p ) \\\\\n\\vdots \\\\\n\\frac{\\partial}{\\partial \\beta_p}(x_1\\beta_1+x_2\\beta_2 +...+x_p \\beta_p )\n\\end{bmatrix}\n\\\\ &=\n\\begin{bmatrix}\nx_{1} \\\\\nx_{2} \\\\\n\\vdots \\\\\nx_{p}\n\\end{bmatrix}\n\\\\ &= \\tilde{x}\n\\end{aligned}\n\\]\n\n\n\nDefinition A.5 (Quadratic form) A quadratic form is a mathematical expression with the structure\n\\[\\tilde{x}^{\\top} \\mathbf{S} \\tilde{x}\\]\nwhere \\(\\tilde{x}\\) is a vector and \\(\\mathbf{S}\\) is a matrix with compatible dimensions for vector-matrix multiplication.\n\n\nQuadratic forms occur frequently in regression models. They are the matrix-vector generalizations of the scalar quadratic form \\(cx^2 = xcx\\).\n\n\n\nTheorem A.27 (Derivative of a quadratic form) If \\(S\\) is a \\(p\\times p\\) matrix that is constant with respect to \\(\\beta\\), then:\n\\[\n\\frac{\\partial}{\\partial \\beta} \\beta'S\\beta = 2S\\beta\n\\]\n\n\nThis is like taking the derivative of \\(cx^2\\) with respect to \\(x\\) in non-vector calculus.\n\n\n\nCorollary A.5 (Derivative of a simple quadratic form) \\[\n\\frac{\\partial}{\\partial \\tilde{\\beta}} \\tilde{\\beta}'\\tilde{\\beta}= 2\\tilde{\\beta}\n\\]\n\n\nThis is like taking the derivative of \\(x^2\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematics</span>"
    ]
  },
  {
    "objectID": "math-prereqs.html#additional-resources",
    "href": "math-prereqs.html#additional-resources",
    "title": "Appendix A — Mathematics",
    "section": "A.5 Additional resources",
    "text": "A.5 Additional resources\n\nA.5.1 Calculus\n\nKhuri (2003)\n\n\n\nA.5.2 Linear Algebra and Vector Calculus\n\nFieller (2016)\nBanerjee and Roy (2014)\nSearle and Khuri (2017)\n\n\n\nA.5.3 Numerical Analysis\n\nHua Zhou’s lecture notes for “UCLA Biostat 216 - Mathematical Methods for Biostatistics” (2023 Fall)\n\n\n\n\n\n\n\nBanerjee, Sudipto, and Anindya Roy. 2014. Linear Algebra and Matrix Analysis for Statistics. Vol. 181. Crc Press Boca Raton. https://www.routledge.com/Linear-Algebra-and-Matrix-Analysis-for-Statistics/Banerjee-Roy/p/book/9781420095388.\n\n\nFieller, Nick. 2016. Basics of Matrix Algebra for Statistics with r. Chapman; Hall/CRC. https://doi.org/10.1201/9781315370200.\n\n\nKhuri, André I. 2003. Advanced Calculus with Applications in Statistics. John Wiley & Sons. https://www.wiley.com/en-us/Advanced+Calculus+with+Applications+in+Statistics%2C+2nd+Edition-p-9780471391043.\n\n\nSearle, Shayle R, and Andre I Khuri. 2017. Matrix Algebra Useful for Statistics. John Wiley & Sons.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Mathematics</span>"
    ]
  },
  {
    "objectID": "probability.html",
    "href": "probability.html",
    "title": "Appendix B — Probability",
    "section": "",
    "text": "Configuring R\nFunctions from these packages will be used throughout this document:\nShow R codelibrary(conflicted) # check for conflicting function definitions\n# library(printr) # inserts help-file output into markdown output\nlibrary(rmarkdown) # Convert R Markdown documents into a variety of formats.\nlibrary(pander) # format tables for markdown\nlibrary(ggplot2) # graphics\nlibrary(ggeasy) # help with graphics\nlibrary(ggfortify) # help with graphics\nlibrary(dplyr) # manipulate data\nlibrary(tibble) # `tibble`s extend `data.frame`s\nlibrary(magrittr) # `%&gt;%` and other additional piping tools\nlibrary(haven) # import Stata files\nlibrary(knitr) # format R output for markdown\nlibrary(tidyr) # Tools to help to create tidy data\nlibrary(plotly) # interactive graphics\nlibrary(dobson) # datasets from Dobson and Barnett 2018\nlibrary(parameters) # format model output tables for markdown\nlibrary(haven) # import Stata files\nlibrary(latex2exp) # use LaTeX in R code (for figures and tables)\nlibrary(fs) # filesystem path manipulations\nlibrary(survival) # survival analysis\nlibrary(survminer) # survival analysis graphics\nlibrary(KMsurv) # datasets from Klein and Moeschberger\nlibrary(parameters) # format model output tables for\nlibrary(webshot2) # convert interactive content to static for pdf\nlibrary(forcats) # functions for categorical variables (\"factors\")\nlibrary(stringr) # functions for dealing with strings\nlibrary(lubridate) # functions for dealing with dates and times\nHere are some R settings I use in this document:\nShow R coderm(list = ls()) # delete any data that's already loaded into R\n\nconflicts_prefer(dplyr::filter)\nggplot2::theme_set(\n  ggplot2::theme_bw() + \n        # ggplot2::labs(col = \"\") +\n    ggplot2::theme(\n      legend.position = \"bottom\",\n      text = ggplot2::element_text(size = 12, family = \"serif\")))\n\nknitr::opts_chunk$set(message = FALSE)\noptions('digits' = 4)\n\npanderOptions(\"big.mark\", \",\")\npander::panderOptions(\"table.emphasize.rownames\", FALSE)\npander::panderOptions(\"table.split.table\", Inf)\nconflicts_prefer(dplyr::filter) # use the `filter()` function from dplyr() by default\nlegend_text_size = 9\nMost of the content in this chapter should be review from UC Davis Epi 202.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#statistical-events",
    "href": "probability.html#statistical-events",
    "title": "Appendix B — Probability",
    "section": "\nB.1 Statistical events",
    "text": "B.1 Statistical events\n\n\nTheorem B.1 If \\(A\\) and \\(B\\) are statistical events and \\(A\\subseteq B\\), then \\(p(A, B) = p(A)\\).\n\n\n\nProof. Left to the reader.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#random-variables",
    "href": "probability.html#random-variables",
    "title": "Appendix B — Probability",
    "section": "\nB.2 Random variables",
    "text": "B.2 Random variables\n\nB.2.1 Binary variables\n\nDefinition B.1 (binary variable) A binary variable is a random variable which has only two possible values in its range.\n\n\nExercise B.1 (Examples of binary variables) What are some examples of binary variables in the health sciences?\n\n\n\nSolution. Examples of binary outcomes include:\n\nexposure (exposed vs unexposed)\ndisease (diseased vs healthy)\nrecovery (recovered vs unrecovered)\nrelapse (relapse vs remission)\nreturn to hospital (returned vs not)\nvital status (dead vs alive)\n\n\n\nB.2.2 Count variables\n\nDefinition B.2 (Count variable) A count variable is a random variable whose possible values are some subset of the non-negative integers; that is, a random variable \\(X\\) such that:\n\\[\\mathcal{R}(X) \\in \\mathbb{N}\\]\n\n\n\nExercise B.2 What are some examples of count variables?\n\n\n\nSolution. \n\nNumber of fish in a pond\nNumber of cyclones per season\nSeconds of tooth-brushing per session (if rounded)\nInfections per person-year\nVisits to ER per person-month\nCar accidents per 1000 miles driven\n\n\n\nExposure magnitude\n\nDefinition B.3 (Exposure magnitude) For many count outcomes, there is some sense of exposure magnitude, population size, or duration of observation.\n\n\n\nExercise B.3 What are some examples of exposure magnitudes?\n\n\n\nSolution. \n\n\nTable B.1: Examples of exposure units\n\n\n\n\n\n\n\noutcome\nexposure units\n\n\n\ndisease incidence\nnumber of individuals exposed; time at risk\n\n\ncar accidents\nmiles driven\n\n\nworksite accidents\nperson-hours worked\n\n\npopulation size\nsize of habitat\n\n\n\n\n\n\n\n\nExposure units are similar to the number of trials in a binomial distribution, but in non-binomial count outcomes, there can be more than one event per unit of exposure.\nWe can use \\(t\\) to represent continuous-valued exposures/observation durations, and \\(n\\) to represent discrete-valued exposures.\n\n\n\nDefinition B.4 (Event rate)  \n\nFor a count outcome \\(Y\\) with exposure magnitude \\(t\\), the event rate (denoted \\(\\lambda\\)) is defined as the mean of \\(Y\\) divided by the the exposure magnitude. That is:\n\n\\[\\mu \\stackrel{\\text{def}}{=}\\mathbb{E}[Y|T=t]\\]\n\\[\\lambda \\stackrel{\\text{def}}{=}\\frac{\\mu}{t} \\tag{B.1}\\]\n\n\nEvent rate is somewhat analogous to odds in binary outcome models; it typically serves as an intermediate transformation between the mean of the outcome and the linear component of the model. However, in contrast with the odds function, the transformation \\(\\lambda = \\mu/t\\) is not considered part of the Poisson model’s link function, and it treats the exposure magnitude covariate differently from the other covariates.\n\n\n\nTheorem B.2 (Transformation function from event rate to mean) For a count variable with mean \\(\\mu\\), event rate \\(\\lambda\\), and exposure magnitude \\(t\\):\n\\[\\therefore\\mu  = \\lambda \\cdot t \\tag{B.2}\\]\n\n\n\nSolution. Start from definition of event rate and use algebra to solve for \\(\\mu\\).\n\n\nEquation B.2 is analogous to the inverse-odds function for binary variables.\n\n\nTheorem B.3 When the exposure magnitude is 0, there is no opportunity for events to occur:\n\\[\\mathbb{E}[Y|T=0] = 0\\]\n\n\n\nProof. \\[\\mathbb{E}[Y|T=0] = \\lambda \\cdot 0 = 0\\]\n\nProbability distributions for count outcomes\n\nPoisson distribution\nNegative binomial distribution",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#key-probability-distributions",
    "href": "probability.html#key-probability-distributions",
    "title": "Appendix B — Probability",
    "section": "\nB.3 Key probability distributions",
    "text": "B.3 Key probability distributions\n\nB.3.1 The Bernoulli distribution\n\nDefinition B.5 (Bernoulli distribution) The Bernoulli distribution family for a random variable \\(X\\) is defined as:\n\\[\n\\begin{aligned}\n\\Pr(X=x) &= \\mathbb{1}_{x\\in \\left\\{0,1\\right\\}}\\pi^x(1-\\pi)^{1-x}\\\\\n&= \\left\\{{\\pi, x=1}\\atop{1-\\pi, x=0}\\right.\n\\end{aligned}\n\\]\n\n\nB.3.2 The Poisson distribution\n\n\nFigure B.1: “Les Poissons”\n\n\n\n\nSiméon Denis Poisson\n\n\n\n\n\nLes Poissons\n\n\n\n\n\n\n\n\n\nDefinition B.6 (Poisson distribution) \\[\\mathcal{R}(Y) = \\left\\{0, 1, 2, ...\\right\\} = \\mathbb{N}\\]\n\\[\\text{P}(Y = y) = \\frac{\\mu^{y} e^{-\\mu}}{y!}, y \\in \\mathbb{N} \\tag{B.3}\\]\n\n(see Figure B.2)\n\n\\[\\text{P}(Y \\le y) = e^{-\\mu} \\sum_{j=0}^{\\left \\lfloor{y}\\right \\rfloor}\\frac{\\mu^j}{j!} \\tag{B.4}\\]\n\n(see Figure B.3)\n\n\n\n\nShow R codelibrary(dplyr)\npois_dists = tibble(\n  mu = c(0.5, 1, 2, 5, 10, 20)) |&gt; \n  reframe(\n    .by = mu,\n    x = 0:30\n  ) |&gt; \n  mutate(\n    `P(X = x)` = dpois(x, lambda = mu),\n    `P(X &lt;= x)` = ppois(x, lambda = mu),\n    mu = factor(mu)\n  )\n\nlibrary(ggplot2)\nlibrary(latex2exp)\n\nplot0 = pois_dists |&gt; \n  ggplot(\n    aes(\n      x = x,\n      y = `P(X = x)`,\n      fill = mu,\n      col = mu)) +\n  theme(legend.position = \"bottom\") +\n  labs(\n    fill = latex2exp::TeX(\"$\\\\mu$\"),\n    col = latex2exp::TeX(\"$\\\\mu$\"),\n    y = latex2exp::TeX(\"$\\\\Pr_{\\\\mu}(X = x)$\"))\n\nplot1 = plot0 + \n  geom_col(position = \"identity\", alpha  = .5) +\n  facet_wrap(~mu)\n  # geom_point(alpha = 0.75) +\n  # geom_line(alpha = 0.75)\nprint(plot1)\n\n\n\nFigure B.2: Poisson PMFs, by mean parameter \\(\\mu\\)\n\n\n\n\n\n\n\n\n\nShow R codelibrary(ggplot2)\n\nplot2 = \n  plot0 + \n  geom_step(alpha = 0.75) +\n  aes(y = `P(X &lt;= x)`) + \n  labs(y = latex2exp::TeX(\"$\\\\Pr_{\\\\mu}(X \\\\leq x)$\"))\n\nprint(plot2)\n\n\n\nFigure B.3: Poisson CDFs\n\n\n\n\n\n\n\n\n\nExercise B.4 (Poisson distribution functions) Let \\(X \\sim \\text{Pois}(\\mu = 3.75)\\).\nCompute:\n\n\\(\\text{P}(X = 4 | \\mu = 3.75)\\)\n\\(\\text{P}(X \\le 7 | \\mu = 3.75)\\)\n\\(\\text{P}(X &gt; 5 | \\mu = 3.75)\\)\n\n\n\n\nSolution. \n\n\\(\\text{P}(X=4) = 0.1938\\)\n\\(\\text{P}(X\\le 7) = 0.9624\\)\n\\(\\text{P}(X &gt; 5) = 0.1771\\)\n\n\n\n\nTheorem B.4 (Properties of the Poisson distribution) If \\(X \\sim \\text{Pois}(\\mu)\\), then:\n\n\\(\\mathbb{E}[Y] = \\mu\\)\n\\(\\text{Var}(Y) = \\mu\\)\n\n\n\nExercise B.5 Prove Theorem B.4.\n\n\n\nSolution. \\[\n\\begin{aligned}\n\\text{E}[X]\n&= \\sum_{x=0}^\\infty x \\cdot P(X=x)\\\\\n&= 0 \\cdot P(X=0) + \\sum_{x=1}^\\infty x \\cdot P(X=x)\\\\\n&= 0 + \\sum_{x=1}^\\infty x \\cdot P(X=x)\\\\\n&= \\sum_{x=1}^\\infty x \\cdot P(X=x)\\\\\n&= \\sum_{x=1}^\\infty x \\cdot \\frac{\\lambda^x e^{-\\lambda}}{x!}\\\\\n&= \\sum_{x=1}^\\infty x \\cdot \\frac{\\lambda^x e^{-\\lambda}}{x \\cdot (x-1)!} & [\\text{definition of factorial (\"!\") function}]\\\\\n&= \\sum_{x=1}^\\infty \\frac{\\lambda^x e^{-\\lambda}}{ (x-1)!}\\\\\n&= \\sum_{x=1}^\\infty \\frac{(\\lambda \\cdot \\lambda^{x-1}) e^{-\\lambda}}{ (x-1)!}\\\\\n&= \\lambda \\cdot \\sum_{x=1}^\\infty \\frac{( \\lambda^{x-1}) e^{-\\lambda}}{ (x-1)!}\\\\\n&= \\lambda \\cdot \\sum_{y=0}^\\infty \\frac{( \\lambda^{y}) e^{-\\lambda}}{ (y)!} &[\\text{substituting } y \\stackrel{\\text{def}}{=}x-1]\\\\\n&= \\lambda \\cdot 1 &[\\text{because PDFs sum to 1}]\\\\\n&= \\lambda\\\\\n\\end{aligned}\n\\]\n\n\nAccounting for exposure\nIf the exposures/observation durations, denoted \\(T=t\\) or \\(N=n\\), vary between observations, we model:\n\\[\\mu = \\lambda\\cdot t\\]\n\\(\\lambda\\) is interpreted as the “expected event rate per unit of exposure”; that is,\n\\[\\lambda = \\frac{\\mathbb{E}[Y|T=t]}{t}\\]\n\n\n\n\n\n\nImportant\n\n\n\nThe exposure magnitude, \\(T\\), is similar to a covariate in linear or logistic regression. However, there is an important difference: in count regression, there is no intercept corresponding to \\(\\mathbb{E}[Y|T=0]\\). In other words, this model assumes that if there is no exposure, there can’t be any events.\n\n\n\nTheorem B.5 If \\(\\mu = \\lambda\\cdot t\\), then:\n\\[\\text{log}\\left\\{\\mu \\right\\}= \\text{log}\\left\\{\\lambda\\right\\} + \\text{log}\\left\\{t\\right\\}\\]\n\n\nDefinition B.7 (Offset) When the linear component of a model involves a term without an unknown coefficient, that term is called an offset.\n\n\nB.3.3 The Negative-Binomial distribution\n\nDefinition B.8 (Negative binomial distribution) \\[\n\\text{P}(Y=y)\n= \\frac{\\mu^y}{y!}\n\\cdot \\frac{\\Gamma(\\rho + y)}{\\Gamma(\\rho) \\cdot (\\rho + \\mu)^y}\n\\cdot \\left(1+\\frac{\\mu}{\\rho}\\right)^{-\\rho}\n\\]\nwhere \\(\\rho\\) is an overdispersion parameter and \\(\\Gamma(x) = (x-1)!\\) for integers \\(x\\).\n\n\nYou don’t need to memorize or understand this expression.\nAs \\(\\rho \\rightarrow \\infty\\), the second term converges to 1 and the third term converges to \\(\\text{exp}\\left\\{-\\mu\\right\\}\\), which brings us back to the Poisson distribution.\n\n\n\nTheorem B.6 If \\(Y \\sim \\text{NegBin}(\\mu, \\rho)\\), then:\n\n\\(\\mathbb{E}[Y] = \\mu\\)\n\\(\\text{Var}\\left(Y\\right) = \\mu + \\frac{\\mu^2}{\\rho} &gt; \\mu\\)\n\n\n\nB.3.4 Weibull Distribution\n\\[\n\\begin{aligned}\np(t)&= \\alpha\\lambda x^{\\alpha-1}\\text{e}^{-\\lambda x^\\alpha}\\\\\nh(t)&=\\alpha\\lambda x^{\\alpha-1}\\\\\nS(t)&=\\text{e}^{-\\lambda x^\\alpha}\\\\\nE(T)&= \\Gamma(1+1/\\alpha)\\cdot \\lambda^{-1/\\alpha}\n\\end{aligned}\n\\]\nWhen \\(\\alpha=1\\) this is the exponential. When \\(\\alpha&gt;1\\) the hazard is increasing and when \\(\\alpha &lt; 1\\) the hazard is decreasing. This provides more flexibility than the exponential.\nWe will see more of this distribution later.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "probability.html#characteristics-of-probability-distributions",
    "href": "probability.html#characteristics-of-probability-distributions",
    "title": "Appendix B — Probability",
    "section": "\nB.4 Characteristics of probability distributions",
    "text": "B.4 Characteristics of probability distributions\n\nB.4.1 Probability density function\n\nDefinition B.9 (probability density) If \\(X\\) is a continuous random variable, then the probability density of \\(X\\) at value \\(x\\), denoted \\(f(x)\\), \\(f_X(x)\\), \\(\\text{p}(x)\\), \\(\\text{p}_X(x)\\), or \\(\\text{p}(X=x)\\), is defined as the limit of the probability (mass) that \\(X\\) is in an interval around \\(x\\), divided by the width of that interval, as that width reduces to 0.\n\\[\n\\begin{aligned}\nf(x) &\\stackrel{\\text{def}}{=}\\lim_{\\delta \\rightarrow 0}\n   \\frac{\\text{P}(X \\in [x, x + \\delta])}{\\delta}\n\\end{aligned}\n\\]\n\nSee also https://en.wikipedia.org/wiki/Probability_density_function#Formal_definition\n\n\n\n\nTheorem B.7 (Density function is derivative of CDF) The density function \\(f(t)\\) or \\(\\text{p}(T=t)\\) for a random variable \\(T\\) at value \\(t\\) is equal to the derivative of the cumulative probability function \\(F(t) \\stackrel{\\text{def}}{=}P(T\\le t)\\); that is:\n\\[f(t) \\stackrel{\\text{def}}{=}\\frac{\\partial}{\\partial t} F(t)\\]\n\n\n\nTheorem B.8 (Density functions integrate to 1) For any density function \\(f(x)\\),\n\\[\\int_{x \\in \\mathcal{R}(X)} f(x) dx = 1\\]\n\n\nB.4.2 Hazard function\n\nDefinition B.10 (Hazard function, hazard rate, hazard rate function)  \n\nThe hazard function, hazard rate, hazard rate function, for a random variable \\(T\\) at value \\(t\\), typically denoted as \\(h(t)\\) or \\(\\lambda(t)\\), is the conditional density of \\(T\\) at \\(t\\), given \\(T \\ge t\\). That is:\n\n\\[h(t) \\stackrel{\\text{def}}{=}\\text{p}(T=t|T\\ge t)\\]\n\nIf \\(T\\) represents the time at which an event occurs, then \\(h(t)\\) is the probability that the event occurs at time \\(t\\), given that it has not occurred prior to time \\(t\\).\n\n\n\nB.4.3 Expectation\n\nDefinition B.11 (Expectation, expected value, population mean ) The expectation, expected value, or population mean of a continuous random variable \\(X\\), denoted \\(\\mathbb{E}\\left[X\\right]\\), \\(\\mu(X)\\), or \\(\\mu_X\\), is the weighted mean of \\(X\\)’s possible values, weighted by the probability density function of those values:\n\\[\\mathbb{E}\\left[X\\right] = \\int_{x\\in \\mathcal{R}(X)} x \\cdot \\text{p}(X=x)dx\\]\nThe expectation, expected value, or population mean of a discrete random variable \\(X\\), denoted \\(\\mathbb{E}\\left[X\\right]\\), \\(\\mu(X)\\), or \\(\\mu_X\\), is the mean of \\(X\\)’s possible values, weighted by the probability mass function of those values:\n\\[\\mathbb{E}\\left[X\\right] = \\sum_{x \\in \\mathcal{R}(X)} x \\cdot \\text{P}(X=x)\\]\n(c.f. https://en.wikipedia.org/wiki/Expected_value)\n\n\n\nTheorem B.9 (Expectation of the Bernoulli distribution) The expectation of a Bernoulli random variable with parameter \\(\\pi\\) is:\n\\[\\mathbb{E}\\left[X\\right] = \\pi\\]\n\n\n\nProof. \\[\n\\begin{aligned}\n\\mathbb{E}\\left[X\\right]\n&= \\sum_{x\\in \\mathcal{R}(X)} x \\cdot\\text{P}(X=x)\n\\\\&= \\sum_{x\\in \\left\\{0,1\\right\\}} x \\cdot\\text{P}(X=x)\n\\\\&= \\left(0 \\cdot\\text{P}(X=0)\\right) + \\left(1 \\cdot\\text{P}(X=1)\\right)\n\\\\&= \\left(0 \\cdot(1-\\pi)\\right) + \\left(1 \\cdot\\pi\\right)\n\\\\&= 0 + \\pi\n\\\\&= \\pi\n\\end{aligned}\n\\]\n\n\nB.4.4 Variance and related characteristics\n\nDefinition B.12 (Variance) The variance of a random variable \\(X\\) is the expectation of the squared difference between \\(X\\) and \\(\\mathbb{E}\\left[X\\right]\\); that is:\n\\[\n\\text{Var}\\left(X\\right) \\stackrel{\\text{def}}{=}\\mathbb{E}\\left[(X-\\mathbb{E}\\left[X\\right])^2\\right]\n\\]\n\n\n\nTheorem B.10 (Simplified expression for variance) \\[\\text{Var}\\left(X\\right)=\\mathbb{E}\\left[X^2\\right] - \\left(\\mathbb{E}\\left[X\\right]\\right)^2\\]\n\n\nProof. By linearity of expectation, we have:\n\\[\n\\begin{aligned}\n\\text{Var}\\left(X\\right)\n&\\stackrel{\\text{def}}{=}\\mathbb{E}\\left[(X-\\mathbb{E}\\left[X\\right])^2\\right]\\\\\n&=\\mathbb{E}\\left[X^2 - 2X\\mathbb{E}\\left[X\\right] + \\left(\\mathbb{E}\\left[X\\right]\\right)^2\\right]\\\\\n&=\\mathbb{E}\\left[X^2\\right] - \\mathbb{E}\\left[2X\\mathbb{E}\\left[X\\right]\\right] + \\mathbb{E}\\left[\\left(\\mathbb{E}\\left[X\\right]\\right)^2\\right]\\\\\n&=\\mathbb{E}\\left[X^2\\right] - 2\\mathbb{E}\\left[X\\right]\\mathbb{E}\\left[X\\right] + \\left(\\mathbb{E}\\left[X\\right]\\right)^2\\\\\n&=\\mathbb{E}\\left[X^2\\right] - \\left(\\mathbb{E}\\left[X\\right]\\right)^2\\\\\n\\end{aligned}\n\\]\n\n\n\n\nDefinition B.13 (Precision) The precision of a random variable \\(X\\), often denoted \\(\\tau(X)\\), \\(\\tau_X\\), or shorthanded as \\(\\tau\\), is the inverse of that random variable’s variance; that is:\n\\[\\tau(X) \\stackrel{\\text{def}}{=}\\left(\\text{Var}\\left(X\\right)\\right)^{-1}\\]\n\n\nDefinition B.14 (Standard deviation) The standard deviation of a random variable \\(X\\) is the square-root of the variance of \\(X\\):\n\\[\\text{SD}\\left(X\\right) \\stackrel{\\text{def}}{=}\\sqrt{\\text{Var}\\left(X\\right)}\\]\n\n\n\nDefinition B.15 (Covariance) For any two one-dimensional random variables, \\(X,Y\\):\n\\[\\text{Cov}\\left(X,Y\\right) \\stackrel{\\text{def}}{=}\\mathbb{E}\\left[(X - \\mathbb{E}\\left[X\\right])(Y - \\mathbb{E}\\left[Y\\right])\\right]\\]\n\n\n\nTheorem B.11 \\[\\text{Cov}\\left(X,Y\\right)= \\mathbb{E}\\left[XY\\right] - \\mathbb{E}\\left[X\\right] \\mathbb{E}\\left[Y\\right]\\]\n\n\n\nProof. Left to the reader.\n\n\n\nLemma B.1 (The covariance of a variable with itself is its variance) For any random variable \\(X\\):\n\\[\\text{Cov}\\left(X,X\\right) = \\text{Var}\\left(X\\right)\\]\n\n\nProof. \\[\n\\begin{aligned}\n\\text{Cov}\\left(X,X\\right) &= E[XX] - E[X]E[X]\n\\\\ &= E[X^2]-(E[X])^2\n\\\\ &= \\text{Var}\\left(X\\right)\n\\end{aligned}\n\\]\n\n\n\nDefinition B.16 (Variance/covariance of a \\(p \\times 1\\) random vector) For a \\(p \\times 1\\) dimensional random vector \\(X\\),\n\\[\n\\begin{aligned}\n\\text{Var}(X)\n&\\stackrel{\\text{def}}{=}\\text{Cov}(X)\\\\\n&\\stackrel{\\text{def}}{=}E[ \\left( X - E\\lbrack X\\rbrack \\right)^{\\top}\\left( X - E\\lbrack X\\rbrack \\right) ]\\\\\n\\end{aligned}\n\\]\n\n\n\nTheorem B.12 (Alternate expression for variance of a random vector) \\[\n\\begin{aligned}\n\\text{Var}\\left(X\\right)\n&= E[ X^{\\top}X ] - {E\\lbrack X\\rbrack}^{\\top}E\\lbrack X\\rbrack\n\\end{aligned}\n\\]\n\n\n\nProof. \\[\n\\begin{aligned}\n\\text{Var}\\left(X\\right)\n&= E[ \\left( X^{\\top} - E\\lbrack X\\rbrack^{\\top} \\right)\\left( X - E\\lbrack X\\rbrack \\right) ]\\\\\n&= E[ X^{\\top}X - E\\lbrack X\\rbrack^{\\top}X - X^{\\top}E\\lbrack X\\rbrack + E\\lbrack X\\rbrack^{\\top}E\\lbrack X\\rbrack ]\\\\\n&= E[ X^{\\top}X ] - E\\lbrack X\\rbrack^{\\top}E\\lbrack X\\rbrack - {E\\lbrack X\\rbrack}^{\\top}E\\lbrack X\\rbrack + E\\lbrack X\\rbrack^{\\top}E\\lbrack X\\rbrack\\\\\n&= E[ X^{\\top}X ] - 2{E\\lbrack X\\rbrack}^{\\top}E\\lbrack X\\rbrack + E\\lbrack X\\rbrack^{\\top}E\\lbrack X\\rbrack\\\\\n&= E[ X^{\\top}X ] - {E\\lbrack X\\rbrack}^{\\top}E\\lbrack X\\rbrack\n\\end{aligned}\n\\]\n\n\n\nTheorem B.13 (Variance of a linear combination) For any set of random variables \\(X_1, \\ldots, X_n\\) and corresponding constants \\(a_1, ... ,a_n\\):\n\\[\\text{Var}\\left(\\sum_{i=1}^na_i X_i\\right) = \\sum_{i=1}^n\\sum_{j=1}^n a_i a_j \\text{Cov}\\left(X_i,X_j\\right)\\]\n\n\n\nProof. Left to the reader…\n\n\n\nLemma B.2 For any two random variables \\(X\\) and \\(Y\\) and scalars \\(a\\) and \\(b\\):\n\\[\\text{Var}\\left(aX + bY\\right) = a^2 \\text{Var}\\left(X\\right) + b^2 \\text{Var}\\left(Y\\right) + 2(a \\cdot b) \\text{Cov}\\left(X,Y\\right)\\]\n\n\n\nProof. Apply Theorem B.13 with \\(n=2\\), \\(X_1 = X\\), and \\(X_2 = Y\\).\nOr, see https://statproofbook.github.io/P/var-lincomb.html\n\n\n\nDefinition B.17 (homoskedastic, heteroskedastic) A random variable \\(Y\\) is homoskedastic (with respect to covariates \\(X\\)) if the variance of \\(Y\\) does not vary with \\(X\\):\n\\[\\text{Var}(Y|X=x) = \\sigma^2, \\forall x\\]\nOtherwise it is heteroskedastic.\n\n\n\nDefinition B.18 (Statistical independence) A set of random variables \\(X_1, \\ldots, X_n\\) are statistically independent if their joint probability is equal to the product of their marginal probabilities:\n\\[\\Pr(X_1=x_1, \\ldots, X_n = x_n) = \\prod_{i=1}^n{\\Pr(X_i=x_i)}\\]\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe symbol for independence, \\(⫫\\), is essentially just \\(\\prod\\) upside-down. So the symbol can remind you of its definition (Definition B.18).\n\n\n\n\n\nDefinition B.19 (Conditional independence) A set of random variables \\(Y_1, \\ldots, Y_n\\) are conditionally statistically independent given a set of covariates \\(X_1, \\ldots, X_n\\) if the joint probability of the \\(Y_i\\)s given the \\(X_i\\)s is equal to the product of their marginal probabilities:\n\\[\\Pr(Y_1=y_1, \\ldots, Y_n = y_n|X_1=x_1, \\ldots, X_n = x_n) = \\prod_{i=1}^n{\\Pr(Y_i=y_i|X_i=x_i)}\\]\n\n\n\nDefinition B.20 (Identically distributed) A set of random variables \\(X_1, \\ldots, X_n\\) are identically distributed if they have the same range \\(\\mathcal{R}(X)\\) and if their marginal distributions \\(\\text{P}(X_1=x_1), ..., \\text{P}(X_n=x_n)\\) are all equal to some shared distribution \\(\\text{P}(X=x)\\):\n\\[\n\\forall i\\in \\left\\{1:n\\right\\}, \\forall x \\in \\mathcal{R}(X): \\text{P}(X_i=x) = \\text{P}(X=x)\n\\]\n\n\n\nDefinition B.21 (Conditionally identically distributed) A set of random variables \\(Y_1, \\ldots, Y_n\\) are conditionally identically distributed given a set of covariates \\(X_1, \\ldots, X_n\\) if \\(Y_1, \\ldots, Y_n\\) have the same range \\(\\mathcal{R}(X)\\) and if the distributions \\(\\text{P}(Y_i=y_i|X_i =x_i)\\) are all equal to the same distribution \\(\\text{P}(Y=y|X=x)\\):\n\\[\n\\text{P}(Y_i=y|X_i=x) = \\text{P}(Y=y|X=x)\n\\]\n\n\n\nDefinition B.22 (Independent and identically distributed) A set of random variables \\(X_1, \\ldots, X_n\\) are independent and identically distributed (shorthand: “\\(X_i\\ \\text{iid}\\)”) if they are statistically independent and identically distributed.\n\n\n\nDefinition B.23 (Conditionally independent and identically distributed) A set of random variables \\(Y_1, \\ldots, Y_n\\) are conditionally independent and identically distributed (shorthand: “\\(Y_i | X_i\\ \\text{ciid}\\)” or just “\\(Y_i |X_i\\ \\text{iid}\\)”) given a set of covariates \\(X_1, \\ldots, X_n\\) if \\(Y_1, \\ldots, Y_n\\) are conditionally independent given \\(X_1, \\ldots, X_n\\) and \\(Y_1, \\ldots, Y_n\\) are identically distributed given \\(X_1, \\ldots, X_n\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "estimation.html",
    "href": "estimation.html",
    "title": "Appendix C — Estimation",
    "section": "",
    "text": "C.1 Probabilistic models",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "estimation.html#probabilistic-models",
    "href": "estimation.html#probabilistic-models",
    "title": "Appendix C — Estimation",
    "section": "",
    "text": "Definition C.1 (Scientific models) Scientific models are attempts to describe physical conditions or changes that occur in the world and universe around us.\n\n\nExample C.1 (Scientific models in epidemiology) Epidemiologists typically study biological conditions and changes, such as the spread of infectious diseases through populations, or the effects of environmental factors on individuals.\n\n\nC.1.1 Statistical analysis of scientific models\nWhen we perform statistical analyses, we use data to help us choose between models - specifically, to determine which models best explain that data.\nHowever, physical processes do not produce data on their own. Data is only produced when scientists implement an observation process (i.e., a scientific study), which is distinct from the underlying physical process. In some cases, the observation process and the physical process interact with each other. This phenomenon is called the “observer effect”.\nIn order to learn about the physical processes we are ultimately interested in, we often need to make special considerations for the observation process that produced the data which we are analyzing. In particular, if some of the planned observations in the study design were not completed, we will likely need to account for the incompleteness of the resulting data set in our analysis. If we are not sure why some observations are incomplete, we may need to model the observation process in addition to the physical process we were originally interested in. For example, if some participants in a study dropped out part-way through the study, we may need investigate why those participants dropped out, as opposed to other participants who completed the study.\nThese kinds of missing data issues are outside of the scope of this course; see Van Buuren (2018) for more details.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "estimation.html#estimands-estimates-and-estimators",
    "href": "estimation.html#estimands-estimates-and-estimators",
    "title": "Appendix C — Estimation",
    "section": "C.2 Estimands, estimates, and estimators",
    "text": "C.2 Estimands, estimates, and estimators\n\nC.2.1 Estimands\n\n\nDefinition C.2 (Estimand) An estimand is an unknown quantity whose value we want to know (Pohl et al. 2021; Lawrance et al. 2020).\n\n\nExample C.2 (Mean height of students) If we are trying to determine the mean height of students at our school, then the population mean is our estimand.\n\nIn statistical contexts, most estimands are parameters of probabilistic models, or functions of model parameters.\n\n\n\n\n\n\nNotation for estimands\n\n\n\nModel paramaters and other estimands are often symbolized using lower-case Greek letters: \\(\\alpha, \\beta, \\gamma, \\delta\\), etc.\n\n\n\n\nC.2.2 Estimates\n \n\nDefinition C.3 (Estimate/estimated value) In statistics, an estimate or estimated value is an informed guess of an estimand’s value, based on observed data.\n\n\nExample C.3 (Mean height of students) Suppose we measure the heights of 50 random students from our school, and the sample mean was 175cm. We might use 175cm as an estimate of the population mean.\n\n\n\nC.2.3 Estimators\n\n\nDefinition C.4 (Estimator) An estimator is a function \\(\\hat\\theta(x_1,...x_n)\\) that transforms data \\(x_1,...x_n\\) into an estimate.\n\n\n\n\n\n\n\nEstimators are random variables\n\n\n\nWhen estimators are applied to random variables, the estimators are also random variables.\n\n\n\n\n\n\n\n\nNotation for estimators\n\n\n\nEstimators are often symbolized by placing a ^ (“hat”) symbol on top of the corresponding estimand; for example, \\(\\hat\\theta\\).\nUsually, their dependence on the data is implicit:\n\\[\\hat\\theta\\stackrel{\\text{def}}{=}\\hat\\theta(x_1,...x_n)\\]\n\n\n\nExample C.4 (Mean height of students) If we want to estimate the mean height of students at our university, which we will represent as \\(\\mu\\), we might measure the heights of \\(n= 50\\) randomly sampled students as random variables \\(X_1,...,X_n\\). Then we could use the function\n\\[\\hat\\mu(X_1,...,X_n) = \\frac{1}{n} \\sum_{i=1}^n X_i \\stackrel{\\text{def}}{=}\\bar X\\]\nas an estimator to produce an estimate \\(\\hat\\mu = \\bar x\\) of \\(\\mu\\).\nAnother estimator would be just the height of the first student sampled:\n\\[\\hat\\mu^{(2)}(X_1,...,X_n) = X_1\\]\nA third possible estimator would be the mean of all sampled students’ heights, except for the two most extreme; that is, if we re-order the observations \\(X_{(1)} = \\min_{i\\in 1:n} X_i\\), \\(X_{(2)} = \\min_{i\\in \\{1:n\\} - \\arg X_{(1)}} X_i\\), …, \\(X_{(n)} = \\max_{i\\in 1:n} X_i\\), then we could define the estimator:\n\\[\\hat\\mu^{(3)}(X_1,...,X_n) = \\frac{1}{n}\\sum_{i=2}^{n-1} X_{(i)}\\]\nWhich of these estimators is best? It depends on how we evaluate them (see Section C.3 below).\n\n\n\nC.2.4 Contrasting estimands, estimates, and estimators\nIt’s helpful to keep in mind the mathematical type of each estimation concept:\n\nestimands are numbers (or vector of numbers)\nestimates are also numbers (or vectors)\nestimators are functions of random variables, so they are also random variables",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "estimation.html#sec-est-accuracy",
    "href": "estimation.html#sec-est-accuracy",
    "title": "Appendix C — Estimation",
    "section": "C.3 Accuracy of estimators",
    "text": "C.3 Accuracy of estimators\n\nC.3.1 Accuracy\nTo determine which estimator is best, we need to define best. “Accuracy” is usually most important; easy computation is usually secondary.\n\nDefinition C.5 (Accuracy) The accuracy of an estimator for a given estimand does not have a consensus formal definition, but all of the usual candidates are related to the distributions of the errors made by the resulting estimates.\n\n\n\nC.3.2 Error\n\nDefinition C.6 (Error) The error of an estimate \\(\\hat\\theta\\) of a true value \\(\\theta\\), often denoted \\(\\varepsilon(\\hat\\theta)\\), or more completely \\(\\varepsilon(\\hat\\theta, \\theta)\\), is the difference between the estimate and its estimand \\(\\theta\\); that is:\n\\[\\varepsilon(\\hat\\theta) \\stackrel{\\text{def}}{=}\\hat\\theta- \\theta\\]\n\nSome frequently-used measures of accuracy include:\n\n\nC.3.3 Mean squared error\n\nDefinition C.7 (Mean squared error) The mean squared error of an estimator \\(\\hat\\theta\\), denoted \\(\\text{MSE}\\left(\\hat\\theta\\right)\\), is the expectation of the square of the error1:\n\\[\\text{MSE}\\left(\\hat\\theta\\right) \\stackrel{\\text{def}}{=}\\mathbb{E}\\left[(\\varepsilon(\\hat\\theta))^2\\right]\\]\n\n\n\nC.3.4 Mean absolute error\n\nDefinition C.8 (Mean absolute error) The mean absolute error of an estimator is the expectation of the absolute value of the error:\n\\[\n\\text{MAE}\\left(\\hat\\theta\\right) \\stackrel{\\text{def}}{=}\\mathbb{E}\\left[\\left|\\varepsilon(\\hat\\theta)\\right|\\right]\n\\]\n\n\n\nC.3.5 Bias\n\nDefinition C.9 (Bias) The bias of an estimator \\(\\hat\\theta\\) for an estimand \\(\\theta\\) is the expected value of the error:\n\\[\\text{Bias}\\left(\\hat\\theta\\right) \\stackrel{\\text{def}}{=}\\mathbb{E}\\left[\\varepsilon(\\hat\\theta)\\right] \\tag{C.1}\\]\n\n\n\nTheorem C.1 (Bias equals Expectation minus Truth) \\[\\text{Bias}\\left(\\hat\\theta\\right) =\\mathbb{E}\\left[\\hat\\theta\\right] - \\theta\\]\n\n\nProof. \\[\n\\begin{aligned}\n\\text{Bias}\\left(\\hat\\theta\\right)\n&\\stackrel{\\text{def}}{=}\\mathbb{E}\\left[\\varepsilon(\\hat\\theta)\\right]\\\\\n&= \\mathbb{E}\\left[\\hat\\theta- \\theta\\right]\\\\\n&=\\mathbb{E}\\left[\\hat\\theta\\right] - \\mathbb{E}\\left[\\theta\\right]\\\\\n&=\\mathbb{E}\\left[\\hat\\theta\\right] - \\theta\n\\end{aligned}\n\\]\nThe third equality is by the linearity of expectation.\n\n\n\nTheorem C.2 (Mean Squared Error equals Bias Squared plus Variance) For any one-dimensional estimator \\(\\hat\\theta\\):\n\\[\\text{MSE}\\left(\\hat\\theta\\right) = \\left(\\text{Bias}\\left(\\hat\\theta\\right)\\right)^2 + \\text{Var}\\left(\\hat\\theta\\right) \\tag{C.2}\\]\n\n\nProof. Let’s start by expanding each term of the right-hand side:\n\\[\n\\begin{aligned}\n\\left(\\text{Bias}\\left(\\hat\\theta\\right)\\right)^2\n&=\\left(\\mathbb{E}\\left[\\hat\\theta\\right] - \\theta\\right)^2\\\\\n&=\\left(\\mathbb{E}\\left[\\hat\\theta\\right]\\right)^2 - 2\\mathbb{E}\\left[\\hat\\theta\\right]\\theta+\\theta^2\\\\\n\\end{aligned}\n\\]\n\\[\\text{Var}\\left(\\hat\\theta\\right) = \\mathbb{E}\\left[\\hat\\theta^2\\right] - \\left(\\mathbb{E}\\left[\\hat\\theta\\right]\\right)^2\\\\\\]\nNow, add them together and simplify:\n\\[\n\\begin{aligned}\n\\left(\\text{Bias}\\left(\\hat\\theta\\right)\\right)^2 + \\text{Var}\\left(\\hat\\theta\\right)\n&=\\left(\\mathbb{E}\\left[\\hat\\theta\\right]\\right)^2 - 2\\mathbb{E}\\left[\\hat\\theta\\right]\\theta+\\theta^2 + \\mathbb{E}\\left[\\hat\\theta^2\\right] - \\left(\\mathbb{E}\\left[\\hat\\theta\\right]\\right)^2\\\\\n&=\\mathbb{E}\\left[\\hat\\theta^2\\right] - 2\\mathbb{E}\\left[\\hat\\theta\\right]\\theta+\\theta^2\\\\\n\\end{aligned}\n\\]\nNow let’s expand the left-hand side to reach the same expression:\n\\[\n\\begin{aligned}\n\\text{MSE}\\left(\\hat\\theta\\right)\n&= \\mathbb{E}\\left[(\\text{e}^{(}\\hat\\theta))^2\\right]\\\\\n&= \\mathbb{E}\\left[(\\hat\\theta- \\theta)^2\\right]\\\\\n&= \\mathbb{E}\\left[\\hat\\theta^2 - 2\\hat\\theta\\theta- \\theta^2\\right]\\\\\n&=\\mathbb{E}\\left[\\hat\\theta^2\\right] - \\mathbb{E}\\left[2\\hat\\theta\\theta\\right]+\\mathbb{E}\\left[\\theta^2\\right]\\\\\n&=\\mathbb{E}\\left[\\hat\\theta^2\\right] - 2\\mathbb{E}\\left[\\hat\\theta\\right]\\theta+\\theta^2\\\\\n\\end{aligned}\n\\]\n\\(\\text{MSE}\\left(\\hat\\theta\\right)\\) and \\(\\left(\\text{Bias}\\left(\\hat\\theta\\right)\\right)^2 + \\text{Var}\\left(\\hat\\theta\\right)\\) both equal \\(\\mathbb{E}\\left[\\hat\\theta^2\\right] - 2\\mathbb{E}\\left[\\hat\\theta\\right]\\theta+\\theta^2\\). Equality is transitive, so \\(\\text{MSE}\\left(\\hat\\theta\\right)\\) and \\(\\left(\\text{Bias}\\left(\\hat\\theta\\right)\\right)^2 + \\text{Var}\\left(\\hat\\theta\\right)\\) are equal to each other:\n\\[\\text{MSE}\\left(\\hat\\theta\\right) = \\left(\\text{Bias}\\left(\\hat\\theta\\right)\\right)^2 + \\text{Var}\\left(\\hat\\theta\\right)\\]\n\n\n\nUnbiased estimators\n\nDefinition C.10 (unbiased estimator) An estimator \\(\\hat\\theta\\) is unbiased if \\(\\text{Bias}\\left(\\hat\\theta\\right) = 0\\).\n\n\nTheorem C.3 (properties of unbiased estimators) If \\(\\hat\\theta\\) is unbiased, then:\n\\[\\mathbb{E}\\left[\\hat\\theta\\right] = \\theta \\tag{C.3}\\] \\[\\text{MSE}\\left(\\hat\\theta\\right) = \\text{Var}\\left(\\hat\\theta\\right) \\tag{C.4}\\]\n\n\nProof. If \\(\\hat\\theta\\) is unbiased, then:\nEquation C.3:\n\\[\n\\begin{aligned}\n\\text{Bias}\\left(\\hat\\theta\\right) &= 0\\\\\n\\mathbb{E}\\left[\\hat\\theta\\right] - \\theta &= 0\\\\\n\\mathbb{E}\\left[\\hat\\theta\\right] &= \\theta\n\\end{aligned}\n\\]\nEquation C.4:\n\\[\n\\begin{aligned}\n\\text{MSE}\\left(\\hat\\theta\\right)\n&\\stackrel{\\text{def}}{=}\\mathbb{E}\\left[\\left(\\varepsilon(\\hat\\theta)\\right)^2\\right]\\\\\n&= \\mathbb{E}\\left[\\left(\\hat\\theta- \\theta\\right)^2\\right]\\\\\n&= \\mathbb{E}\\left[\\left(\\hat\\theta- \\mathbb{E}\\left[\\hat\\theta\\right]\\right)^2\\right]\\\\\n&\\stackrel{\\text{def}}{=}\\text{Var}\\left(\\hat\\theta\\right)\n\\end{aligned}\n\\]\n(Alternative proof of Equation C.4) We could have started from Theorem C.2 instead:\n\\[\n\\begin{aligned}\n\\text{MSE}\\left(\\hat\\theta\\right)\n&= \\left(\\text{Bias}\\left(\\hat\\theta\\right)\\right)^2 + \\text{Var}\\left(\\hat\\theta\\right)\\\\\n&= \\left(0\\right)^2 + \\text{Var}\\left(\\hat\\theta\\right)\\\\\n&= 0 + \\text{Var}\\left(\\hat\\theta\\right)\\\\\n&= \\text{Var}\\left(\\hat\\theta\\right)\\\\\n\\end{aligned}\n\\]\n\n\n\n\n\nC.3.6 Standard error\n\nDefinition C.11 (Standard error) The standard error of an estimator \\(\\hat\\theta\\) is just the standard deviation of \\(\\hat\\theta\\); that is:\n\\[\\text{SE}\\left(\\hat\\theta\\right) \\stackrel{\\text{def}}{=}\\text{SD}\\left(\\hat\\theta\\right)\\]\n\n“Standard error” is a confusing concept in a few ways. First of all, it isn’t even defined as a characteristic of the error, \\(\\varepsilon(\\hat\\theta)\\)! Moreover, it is just a synonym for standard deviation, so it seems like a redundant concept. However, standard errors help us construct p-values and confidence intervals, so they come up a lot - often enough to give them their own name.\nWe can relate standard error to actual error, by rearranging the result from Theorem C.2:\n\\[\n\\begin{aligned}\n\\text{Var}\\left(\\hat\\theta\\right) &= \\text{Var}\\left(\\hat\\theta- \\theta\\right)\\\\\n&= \\text{Var}\\left(\\varepsilon(\\hat\\theta)\\right)\\\\\n\\end{aligned}\n\\] So the variance of the estimator is equal to the variance of the error, and the standard error is equal to the standard deviation of the error:\n\\[\\text{SE}\\left(\\hat\\theta\\right) = \\text{SD}\\left(\\varepsilon(\\hat\\theta)\\right)\\]\n\n\nCorollary C.1 (Standard error squared equals MSE minus squared bias) standard error is what is left over of MSE after bias is removed:\n\\[\\left(\\text{SE}\\left(\\hat\\theta\\right)\\right)^2 = \\text{MSE}\\left(\\hat\\theta\\right) - \\left(\\text{Bias}\\left(\\hat\\theta\\right)\\right)^2\\]\n\n\nProof. \\[\n\\begin{aligned}\n\\text{MSE}\\left(\\hat\\theta\\right) &= \\left(\\text{Bias}\\left(\\hat\\theta\\right)\\right)^2 + \\text{Var}\\left(\\hat\\theta\\right)\\\\\n\\therefore\\text{Var}\\left(\\hat\\theta\\right) &= \\text{MSE}\\left(\\hat\\theta\\right) - \\left(\\text{Bias}\\left(\\hat\\theta\\right)\\right)^2\\\\\n\\therefore\\left(\\text{SE}\\left(\\hat\\theta\\right)\\right)^2 &= \\text{MSE}\\left(\\hat\\theta\\right) - \\left(\\text{Bias}\\left(\\hat\\theta\\right)\\right)^2\\\\\n\\end{aligned}\n\\]\n\n\nCorollary C.2 (For unbiased estimators, SE = RMSE) If \\(\\mathbb{E}\\left[\\varepsilon\\left(\\hat\\theta\\right)\\right] = 0\\), then:\n\\[\\text{SE}\\left(\\hat\\theta\\right) = \\sqrt{\\text{MSE}\\left(\\hat\\theta\\right)}\\]\n(this result is equivalent to Equation C.4)\n\n\n\n\n\n\n\n\nLawrance, Rachael, Evgeny Degtyarev, Philip Griffiths, Peter Trask, Helen Lau, Denise D’Alessio, Ingolf Griebsch, Gudrun Wallenstein, Kim Cocks, and Kaspar Rufibach. 2020. “What Is an Estimand, and How Does It Relate to Quantifying the Effect of Treatment on Patient-Reported Quality of Life Outcomes in Clinical Trials?” Journal of Patient-Reported Outcomes 4 (1): 1–8. https://link.springer.com/article/10.1186/s41687-020-00218-5.\n\n\nPohl, Moritz, Lukas Baumann, Rouven Behnisch, Marietta Kirchner, Johannes Krisam, and Anja Sander. 2021. “Estimands—A Basic Element for Clinical Trials.” Deutsches Ärzteblatt International 118 (51-52): 883–88. https://doi.org/10.3238/arztebl.m2021.0373.\n\n\nVan Buuren, Stef. 2018. Flexible Imputation of Missing Data. CRC press. https://stefvanbuuren.name/fimd/.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "estimation.html#footnotes",
    "href": "estimation.html#footnotes",
    "title": "Appendix C — Estimation",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Does_exactly_what_it_says_on_the_tin↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "inference.html",
    "href": "inference.html",
    "title": "Appendix D — Inference",
    "section": "",
    "text": "D.1 Interpretation of Negative Findings\nSee Vittinghoff et al. (2012) §3.7 (p64).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "inference.html#interpretation-of-negative-findings",
    "href": "inference.html#interpretation-of-negative-findings",
    "title": "Appendix D — Inference",
    "section": "",
    "text": "Vittinghoff, Eric, David V Glidden, Stephen C Shiboski, and Charles E McCulloch. 2012. Regression Methods in Biostatistics: Linear, Logistic, Survival, and Repeated Measures Models. 2nd ed. Springer. https://doi.org/10.1007/978-1-4614-1353-0.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Inference</span>"
    ]
  },
  {
    "objectID": "intro-MLEs.html",
    "href": "intro-MLEs.html",
    "title": "Appendix E — Introduction to Maximum Likelihood Inference",
    "section": "",
    "text": "Configuring R\nFunctions from these packages will be used throughout this document:\nShow R codelibrary(conflicted) # check for conflicting function definitions\n# library(printr) # inserts help-file output into markdown output\nlibrary(rmarkdown) # Convert R Markdown documents into a variety of formats.\nlibrary(pander) # format tables for markdown\nlibrary(ggplot2) # graphics\nlibrary(ggeasy) # help with graphics\nlibrary(ggfortify) # help with graphics\nlibrary(dplyr) # manipulate data\nlibrary(tibble) # `tibble`s extend `data.frame`s\nlibrary(magrittr) # `%&gt;%` and other additional piping tools\nlibrary(haven) # import Stata files\nlibrary(knitr) # format R output for markdown\nlibrary(tidyr) # Tools to help to create tidy data\nlibrary(plotly) # interactive graphics\nlibrary(dobson) # datasets from Dobson and Barnett 2018\nlibrary(parameters) # format model output tables for markdown\nlibrary(haven) # import Stata files\nlibrary(latex2exp) # use LaTeX in R code (for figures and tables)\nlibrary(fs) # filesystem path manipulations\nlibrary(survival) # survival analysis\nlibrary(survminer) # survival analysis graphics\nlibrary(KMsurv) # datasets from Klein and Moeschberger\nlibrary(parameters) # format model output tables for\nlibrary(webshot2) # convert interactive content to static for pdf\nlibrary(forcats) # functions for categorical variables (\"factors\")\nlibrary(stringr) # functions for dealing with strings\nlibrary(lubridate) # functions for dealing with dates and times\nHere are some R settings I use in this document:\nShow R coderm(list = ls()) # delete any data that's already loaded into R\n\nconflicts_prefer(dplyr::filter)\nggplot2::theme_set(\n  ggplot2::theme_bw() + \n        # ggplot2::labs(col = \"\") +\n    ggplot2::theme(\n      legend.position = \"bottom\",\n      text = ggplot2::element_text(size = 12, family = \"serif\")))\n\nknitr::opts_chunk$set(message = FALSE)\noptions('digits' = 4)\n\npanderOptions(\"big.mark\", \",\")\npander::panderOptions(\"table.emphasize.rownames\", FALSE)\npander::panderOptions(\"table.split.table\", Inf)\nconflicts_prefer(dplyr::filter) # use the `filter()` function from dplyr() by default\nlegend_text_size = 9",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Introduction to Maximum Likelihood Inference</span>"
    ]
  },
  {
    "objectID": "intro-MLEs.html#overview-of-maximum-likelihood-estimation",
    "href": "intro-MLEs.html#overview-of-maximum-likelihood-estimation",
    "title": "Appendix E — Introduction to Maximum Likelihood Inference",
    "section": "\nE.1 Overview of maximum likelihood estimation",
    "text": "E.1 Overview of maximum likelihood estimation\n\nE.1.1 The likelihood function\n\nDefinition E.1 (Likelihood of a single observation) Let \\(X\\) be a random variable and let \\(x\\) be \\(X\\)’s observed data value. Let \\(\\text{p}_{\\Theta}(X=x)\\) be a probability model for the distribution of \\(X\\), with parameter vector \\(\\Theta\\).\nThen the likelihood of parameter value \\(\\theta\\), for model \\(\\text{p}_{\\Theta}(X=x)\\) and data \\(X = x\\), is simply the probability of the event \\(X=x\\) given \\(\\Theta= \\theta\\):\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\theta) &\\stackrel{\\text{def}}{=}\\text{P}_{\\theta}(X = x)\n\\end{aligned}\n\\]\n\n\n\nDefinition E.2 (Likelihood of a dataset) Let \\(\\tilde{x}\\) be a dataset with corresponding random variable \\(\\tilde{X}\\). Let \\(\\text{p}_{\\Theta}(\\tilde{X})\\) be a probability model for the distribution of \\(\\tilde{X}\\) with unknown parameter vector \\(\\Theta\\).\nThen the likelihood of parameter value \\(\\theta\\), for model \\(\\text{p}_{\\Theta}(X)\\) and data \\(\\tilde{X}= \\tilde{x}\\), is the joint probability of \\(\\tilde{X}= \\tilde{x}\\) given \\(\\Theta= \\theta\\):\n\\[\n\\begin{aligned}\n\\mathcal{L}(\\theta) &\\stackrel{\\text{def}}{=}p_{\\theta}(\\tilde{X}= \\tilde{x})\n\\\\&=p_{\\theta}(X_1=x_1, ..., X_n = x_n)\n\\end{aligned}\n\\]\n\n\n\n\n\n\n\nNotation for the likelihood function\n\n\n\nThe likelihood function can be written as:\n\n\\(\\mathcal{L}(\\theta)\\)\n\\(\\mathcal{L}(\\tilde{x};\\theta)\\)\n\\(\\mathcal{L}(\\theta; \\tilde{x})\\)\n\\(\\mathcal{L}_{\\tilde{x}}(\\theta)\\)\n\\(\\mathcal{L}_{\\theta}(\\tilde{x})\\)\n\\(\\mathcal{L}(\\tilde{x} | \\theta)\\)\n\nAll of these notations mean the same thing.\n\n\n\nThe likelihood is a function that takes \\(\\theta\\) (and implicitly, \\(\\tilde{X}\\)) as inputs and outputs a single number, the joint probability of \\(\\tilde{x}\\) for model \\(p_\\Theta(\\tilde{X}=\\tilde{x})\\) with \\(\\Theta = \\theta\\).\n\n\n\nTheorem E.1 (Likelihood of an independent sample) For mutually independent data \\(X_1, ..., X_n\\):\n\\[\\mathcal{L}(\\tilde{x}|\\theta) = \\prod_{i=1}^n \\text{p}(X_i=x_i|\\theta) \\tag{E.1}\\]\n\n\nProof. \\[\n\\begin{aligned}\n\\mathcal{L}(\\tilde{x}|\\theta)\n&\\stackrel{\\text{def}}{=}\\text{p}(X_1 = x_1, …,X_n =x_n|\\theta)\n\\\\&= \\prod_{i=1}^n \\text{p}(X_i=x_i|\\theta)\n\\end{aligned}\n\\] The second equality is by the definition of statistical independence.\n\n\n\nDefinition E.3 (Likelihood components) Given an \\(\\text{iid}\\) dataset \\(\\tilde{x}\\), the likelihood component or likelihood factor of observation \\(X_i=x_i\\) is the marginal likelihood of \\(X_i=x_i\\):\n\\[\\mathcal{L}_i(\\theta) = \\text{P}(X_i=x_i)\\]\n\n\n\nTheorem E.2 For \\(\\text{iid}\\) data \\(\\tilde{x}\\stackrel{\\text{def}}{=}x_1, \\ldots, x_n\\), the likelihood of the dataset is equal to the product of the observation-specific likelihood factors:\n\\[\\mathcal{L}(\\theta) = \\prod_{i=1}^n\\mathcal{L}_i(\\theta)\\]\n\n\nE.1.2 The maximum likelihood estimate\n\nDefinition E.4 (Maximum likelihood estimate) The maximum likelihood estimate of a parameter vector \\(\\Theta\\), denoted \\(\\hat\\theta_{\\text{ML}}\\), is the value of \\(\\Theta\\) that maximizes the likelihood:\n\\[\n\\hat\\theta_{\\text{ML}}\\stackrel{\\text{def}}{=}\\arg \\max_\\Theta\\mathcal{L}(\\Theta)\n\\tag{E.2}\\]\n\n\nE.1.3 Finding the maximum of a function\nRecall from calculus: the maxima of a continuous function \\(f(x)\\) over a range of input values \\(\\mathcal{R}(x)\\) can be found either:\n\nat the edges of the range of input values, OR:\nwhere the function is flat (i.e. where the gradient function \\(f'(x) = 0\\)) AND the second derivative is negative definite (\\(f''(x) &lt; 0\\)).\n\nE.1.4 Directly maximizing the likelihood function for iid data\nTo find the maximizer(s) of the likelihood function, we need to solve \\(\\mathcal{L}'(\\theta) = 0\\) for \\(\\theta\\). However, even for mutually independent data, we quickly run into a problem:\n\\[\n\\begin{aligned}\n\\mathcal{L}'(\\theta)\n&= \\frac{\\partial}{\\partial \\theta} \\mathcal{L}(\\theta)\n\\\\ &= \\frac{\\partial}{\\partial \\theta} \\prod_{i=1}^n p(X_i=x_i|\\theta)\n\\end{aligned}\n\\tag{E.3}\\]\nThe derivative of the likelihood of independent data is the derivative of a product. We will have to perform a massive application of the product rule to evaluate this derivative.\n\nE.1.5 The log-likelihood function\nIt is typically easier to work with the log of the likelihood function:\n\nDefinition E.5 (Log-likelihood) The log-likelihood of parameter value \\(\\theta\\), for model \\(\\text{p}_{\\Theta}(\\tilde{X})\\) and data \\(\\tilde{X}= \\tilde{x}\\), is the natural logarithm of the likelihood1:\n\\[\\ell(\\theta) \\stackrel{\\text{def}}{=}\\text{log}\\left\\{\\mathcal{L}(\\theta)\\right\\}\\]\n\n\n\nDefinition E.6 (Log-likelihood components) Given a dataset \\(\\tilde{X}= \\tilde{x}\\), the log-likelihood component of observation \\(X_i=x_i\\) is the natural logarithm of the likelihood component:\n\\[\\ell_i(\\theta) \\stackrel{\\text{def}}{=}\\text{log}\\left\\{\\mathcal{L}_i(\\theta)\\right\\}\\]\n\n\n\nTheorem E.3 The likelihood and log-likelihood have the same maximizer:\n\\[\n\\arg \\max_\\theta\\mathcal{L}(\\theta) = \\arg \\max_\\theta\\ell(\\theta)\n\\]\n\n\nProof. Left to the reader.\n\n\n\nTheorem E.4 (Log-likelihood of an \\(\\text{iid}\\) sample) For \\(\\text{iid}\\) data \\(X_1, ..., X_n\\) with shared distribution \\(\\text{p}(X=x)\\):\n\\[\\ell(x|\\theta) = \\sum_{i=1}^n \\text{log}\\left\\{p(X=x_i|\\theta)\\right\\} \\tag{E.4}\\]\n\n\nProof. \\[\n\\begin{aligned}\n\\ell(x|\\theta)\n&\\stackrel{\\text{def}}{=}\\text{log}\\left\\{\\mathcal{L}(\\tilde{x}|\\theta)\\right\\}\n\\\\&= \\text{log}\\left\\{\\prod_{i=1}^n \\text{p}(X_i=x_i|\\theta)\\right\\}\n\\\\&= \\sum_{i=1}^n \\text{log}\\left\\{p(X=x_i|\\theta)\\right\\}\n\\end{aligned}\n\\]\n\n\n\nFor \\(\\text{iid}\\) data, we will have a much easier time taking the derivative of the log-likelihood:\n\n\nTheorem E.5 (Derivative of the log-likelihood function for \\(\\text{iid}\\) data) For \\(\\text{iid}\\) data:\n\\[\\ell'(\\theta) = \\sum_{i=1}^n\\frac{\\partial}{\\partial \\theta} \\text{log}\\left\\{\\text{p}(X=x_i|\\theta)\\right\\} \\tag{E.5}\\]\n\n\nProof. \\[\n\\begin{aligned}\n\\ell'(\\theta)\n&= \\frac{\\partial}{\\partial \\theta} \\ell(\\theta)\n\\\\ &= \\frac{\\partial}{\\partial \\theta} \\sum_{i=1}^n \\text{log}\\left\\{\\text{p}(X=x_i|\\theta)\\right\\}\n\\\\ &= \\sum_{i=1}^n \\frac{\\partial}{\\partial \\theta} \\text{log}\\left\\{\\text{p}(X=x_i|\\theta)\\right\\}\n\\end{aligned}\n\\]\n\n\nE.1.6 The score function\nThe first derivative2 of the log-likelihood, \\(\\ell'(\\theta)\\), is important enough to have its own name: the score function.\n\nDefinition E.7 (Score function) The score function of a statistical model \\(\\text{p}(\\tilde{X}=\\tilde{x})\\) is the gradient (i.e., first derivative) of the log-likelihood of that model:\n\\[\\ell'(\\theta) \\stackrel{\\text{def}}{=}\\frac{\\partial}{\\partial \\theta} \\ell(\\theta)\\]\n\n\nWe often skip writing the arguments \\(x\\) and/or \\(\\theta)\\), so \\(\\ell' \\stackrel{\\text{def}}{=}\\ell'(\\tilde{x};\\theta) \\stackrel{\\text{def}}{=}\\ell'(\\theta)\\).3 Some statisticians use \\(U\\) or \\(S\\) instead of \\(\\ell'\\). I prefer \\(\\ell'\\). Why use up extra letters?\n\n\nE.1.7 Asymptotic distribution of the maximum likelihood estimate\n\nWe learned how to quantify our uncertainty about these maximum likelihood estimates; with sufficient sample size, \\(\\hat\\theta_{\\text{ML}}\\) has the approximate distribution:\n\n\\[\n\\hat\\theta_{ML} \\dot \\sim N(\\theta,\\mathcal I(\\theta)^{-1})\n\\]\nRecall:\n\n\\(\\mathcal{I}(\\theta) \\stackrel{\\text{def}}{=}\\mathbb{E}\\left[I(\\tilde{X};\\theta)\\right]\\)\n\\(I(\\tilde{X},\\theta) \\stackrel{\\text{def}}{=}-\\ell''(\\tilde{X};\\theta)\\)\n\nWe can estimate \\(\\mathcal{I}(\\theta)\\) using either \\(\\mathcal{I}(\\hat\\theta_{\\text{ML}})\\) or \\(I(\\tilde{x}; \\hat\\theta_{\\text{ML}})\\).\nSo we can estimate the standard error of \\(\\hat\\theta_k\\) as:\n\\[\n\\widehat{\\text{SE}}\\left(\\hat\\theta_k\\right) = \\sqrt{\\left[\\left(\\hat{\\mathcal{I}}\\left(\\hat\\theta_{\\text{ML}}\\right)\\right)^{-1}\\right]_{kk}}\n\\]\n\nE.1.8 The (Fisher) (expected) information matrix\nThe variance of \\(\\ell'(x,\\theta)\\), \\({Cov}\\left\\{ \\ell'(x,\\theta) \\right\\}\\), is also very important; we call it the “expected information matrix”, “Fisher information matrix”, or just “information matrix”, and we represent it using the symbol \\(\\mathcal{I}\\left(I\\right)\\) (\\frakturI in Unicode, \\mathfrak{I} in LaTeX).\n\\[\n\\begin{aligned}\n\\mathcal{I}\n\\stackrel{\\text{def}}{=}\\mathcal{I}(\\theta)\n\\\\ &\\stackrel{\\text{def}}{=}\\text{Cov}\\left(\\ell'|\\theta\\right)\n\\\\ &= \\mathbb{E}[ \\ell'{\\ell'}^{\\top} ] - \\mathbb{E}[ \\ell' ] \\ \\mathbb{E}[ \\ell' ]^{\\top}\n\\end{aligned}\n\\]\nThe elements of \\(\\mathfrak{I}\\) are:\n\\[\n\\begin{aligned}\n\\mathfrak{I}_{ij}\n&\\stackrel{\\text{def}}{=}\\text{Cov}\\left({\\ell'}_{i},{\\ell'}_{j}\\right)\n\\\\ &= \\mathbb{E}[ \\ell_{i}'\\ell_{j}' ] - \\mathbb{E}[ {\\ell'}_{i} ] \\mathbb{E}[ {\\ell'}_{j} ]\n\\end{aligned}\n\\]\nHere,\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\ell'\\right]\n&\\stackrel{\\text{def}}{=}\\int_{x \\in \\mathcal{R}(x)}\n{\n\\ell'(x,\\theta) \\text{p}(X = x | \\theta) dx\n}\n\\\\ &= \\int_{x \\in \\mathcal{R}(X)}\n{\n\\left(\n\\frac{\\partial}{\\partial \\theta}\n\\text{log}\\left\\{\\text{p}(X = x | \\theta)\\right\\}\n\\right)\n\\text{p}(X = x | \\theta) dx\n}\n\\\\ &=\n\\int_{x \\in \\mathcal{R}(X)}\n{\n\\frac\n{\\frac{\\partial}{\\partial \\theta} \\text{p}(X = x | \\theta)}\n{\\text{p}(X = x | \\theta)}\n\\text{p}(X = x | \\theta) dx\n}\n\\\\ &=\n\\int_{x \\in \\mathcal{R}(X)}\n{\n\\frac{\\partial}{\\partial \\theta} \\text{p}(X = x | \\theta) dx\n}\n\\end{aligned}\n\\]\nAnd similarly\n\\[\n\\mathbb{E}\\left[\\ell' \\ell'^{\\top}\\right]\n\\stackrel{\\text{def}}{=}\n\\int_{x \\in R(x)}\n{\\ell'(x,\\theta)\\ell'(x,\\theta)^{\\top}\\\n\\text{p}\\left(X = x | \\theta\\right)\\ dx}\n\\]\nNote that \\(\\mathbb{E}\\left[\\ell'\\right]\\) and \\(\\mathbb{E}\\left[\\ell'{\\ell'}^{\\top}\\right]\\) are functions of \\(\\theta\\) but not of \\(x\\); the expectation operator removed \\(x\\).\nAlso note that for most of the distributions you are familiar with (including Gaussian, binomial, Poisson, exponential):\n\\[\\mathbb{E}\\left[\\ell'\\right] = 0\\]\nSo\n\\[\\mathcal{I}\\left(\\theta\\right) = \\mathbb{E}\\left[\\ell'{\\ell'}^{\\top} \\right]\\]\nMoreover, for those distributions (called the “exponential family”), we have:\n\\[\n\\mathfrak{I} = -\\mathbb{E}\\left[\\ell''\\right]\n= \\mathbb{E}\\left[- \\ell''\\right]\n\\]\n(see Dobson and Barnett (2018), §3.17), where\n\\[\\ell'' \\stackrel{\\text{def}}{=}\\frac{\\partial}{\\partial \\theta}\\ell^{'(x,\\theta)^{\\top}} = \\frac{\\partial}{\\partial \\theta}\\frac{\\partial}{\\partial \\theta^{\\top}}\\ell(x,\\theta)\\]\nis the \\(p \\times p\\) matrix whose elements are:\n\\[\\ell_{ij}'' \\stackrel{\\text{def}}{=}\\frac{\\partial}{\\partial \\theta_{i}}\\frac{\\partial}{\\partial \\theta_{j}}\\text{log}\\left\\{ p\\left( X = x \\mid \\theta \\right)\\right\\}\\]\n\\(\\ell''\\) is called the “Hessian”4 of the log-likelihood function.\nSometimes, we use \\(I(\\theta;x) \\stackrel{\\text{def}}{=}- \\ell''\\) (note the standard-font “I” here). \\(I(\\theta;x)\\) is the observed information, precision, or concentration matrix (Negative Hessian).\n\n\n\n\n\n\nKey point\n\n\n\nThe asymptotics of MLEs gives us \\({\\widehat{\\theta}}_{ML} \\sim N\\left( \\theta,\\mathfrak{I}^{- 1}(\\theta) \\right)\\), approximately, for large sample sizes.\n\n\nWe can estimate \\(\\mathcal{I}^{- 1}(\\theta)\\) by working out \\(\\mathbb{E}\\left[-\\ell''\\right]\\) or \\(\\mathbb{E}\\left[\\ell'{\\ell'}^{\\top}\\right]\\) and plugging in \\(\\hat\\theta_{\\text{ML}}\\), but sometimes we instead use \\(I(\\hat\\theta_{\\text{ML}},\\tilde{x})\\) for convenience; there are some cases where it’s provably better according to some criteria (Efron and Hinkley (1978)).\n\nE.1.9 Iterative maximization\n(c.f., Dobson and Barnett (2018), Chapter 4)\n\nLater, when we are trying to find MLEs for likelihoods which we can’t easily differentiate, we will “hill-climb” using the Newton-Raphson algorithm:\n\n\\[\n\\begin{aligned}\n{\\widehat{\\theta}}^*\n&\\leftarrow {\\widehat{\\theta}}^* + \\left(I\\left(\\tilde{y};{\\widehat{\\theta}}^*\\right)\\right)^{-1}\n\\ell'\\left(\\tilde{y};{\\widehat{\\theta}}^*\\right)\n\\\\ &= {\\widehat{\\theta}}^* - \\left(\\ell''\\left(\\tilde{y};{\\widehat{\\theta}}^*\\right)\\right)^{-1}\n\\ell'\\left(\\tilde{y};{\\widehat{\\theta}}^*\\right)\n\\end{aligned}\n\\]\n\n\nThe reasoning for this algorithm is that we can approximate the the score function using the first-order Taylor polynomial:\n\n\\[\n\\begin{aligned}\n\\ell'(\\theta)\n&\\approx \\ell'^*(\\theta)\n\\\\ &\\stackrel{\\text{def}}{=}\\ell'({\\widehat{\\theta}}^*) + \\ell''({\\widehat{\\theta}}^*)(\\theta- {\\widehat{\\theta}}^*)\n\\end{aligned}\n\\]\n\n\nThe approximate score function, \\(\\ell'^*(\\theta)\\), is a linear function of \\(\\theta\\), so it is easy to solve the corresponding approximate score equation, \\(\\ell'^*(\\theta) = 0\\), for \\(\\theta\\):\n\n\\[\n\\begin{aligned}\n\\theta\n&= {\\widehat{\\theta}}^* - \\ell'({\\widehat{\\theta}}^*) \\cdot\\left(\\ell''({\\widehat{\\theta}}^*)\\right)^{-1}\n\\end{aligned}\n\\]\n\nFor computational simplicity, we will sometimes use \\(\\mathfrak{I}^{- 1}(\\theta)\\) in place of \\(I\\left( \\widehat{\\theta},y \\right)\\); doing so is called “Fisher scoring” or the “method of scoring”. Note that this is the opposite of the substitution that we are making for estimating the variance of the MLE; this time we should technically use the observed information but we use the expected information instead.\n\nThere’s also an “empirical information matrix” (see McLachlan and Krishnan (2007)):\n\\[I_{e}(\\theta,y) \\stackrel{\\text{def}}{=}\\sum_{i = 1}^{n}{\\ell_{i}'\\ {\\ell_{i}'}^{\\top}} - \\frac{1}{n}\\ell'{\\ell'}^{\\top}\\]\nwhere \\(\\ell_{i}\\) is the log-likelihood of the ith observation. Note that \\(\\ell' = \\sum_{i = 1}^{n}\\ell_{i}'\\).\n\\(\\frac{1}{n}I_{e}(\\theta,y)\\) is the sample equivalent of\n\\[\\mathfrak{I \\stackrel{\\text{def}}{=}I(}\\theta) \\stackrel{\\text{def}}{=}{Cov}\\left( \\ell'|\\theta \\right) = E[ \\ell'{\\ell'}^{\\top} ] - E[ \\ell' ]\\ E[ \\ell' ]^{\\top}\\]\n\\[\\left\\{ \\mathfrak{I}_{jk} \\stackrel{\\text{def}}{=}{Cov}\\left( {\\ell'}_{j},{\\ell'}_{k} \\right) = E[ \\ell_{j}'\\ell_{k}' ] - E[ {\\ell'}_{j} ] E[ {\\ell'}_{k} ] \\right\\}\\]\n\\(I_{e}(\\theta,y)\\) is sometimes computationally easier to compute for Newton-Raphson-type maximization algorithms.\nc.f. https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization\n\nE.1.10 Quantifying (un)certainty of MLEs\nConfidence intervals for MLEs\nAn asymptotic approximation of a 95% confidence interval for \\(\\theta_k\\) is\n\\[\n\\hat\\theta_{\\text{ML}}\\pm z_{0.975} \\times \\widehat{\\text{SE}}\\left(\\hat\\theta_k\\right)\n\\]\nwhere \\(z_\\beta\\) the \\(\\beta\\) quantile of the standard Gaussian distribution.\np-values and hypothesis tests for MLEs\n(to add)\nLikelihood ratio tests for MLEs\nlog(likelihood ratio) tests (c.f. Dobson and Barnett 2018, sec. 5.7):\n\\[-2\\ell_{0} \\sim \\chi^{2}(p - q)\\]\nSee also https://online.stat.psu.edu/stat504/book/export/html/657\nPrediction intervals for MLEs\n\\[\\overline{X} \\in [ \\widehat{\\mu} \\pm z_{1 - \\alpha\\text{/}2}\\frac{\\sigma}{m} ]\\]\nWhere \\(m\\) is the sample size of the new data to be predicted (typically 1, except for binary outcomes, where it needs to be bigger for prediction intervals to make sense)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Introduction to Maximum Likelihood Inference</span>"
    ]
  },
  {
    "objectID": "intro-MLEs.html#example-maximum-likelihood-for-tropical-cyclones-in-australia",
    "href": "intro-MLEs.html#example-maximum-likelihood-for-tropical-cyclones-in-australia",
    "title": "Appendix E — Introduction to Maximum Likelihood Inference",
    "section": "\nE.2 Example: Maximum likelihood for Tropical Cyclones in Australia",
    "text": "E.2 Example: Maximum likelihood for Tropical Cyclones in Australia\n(Adapted from Dobson and Barnett (2018) §1.6.5)\n\nE.2.1 Data\nThe cyclones dataset in the dobson package (Table E.1) records the number of tropical cyclones in Northeastern Australia during 13 November-to-April cyclone seasons (more details in Dobson and Barnett (2018) §1.6.5 and help(cyclones, package = \"dobson\")). Figure E.1 graphs the number of cyclones (y-axis) by season (x-axis). Let’s use \\(Y_i\\) to represent these counts, where \\(i\\) is an indexing variable for the seasons and \\(Y_i\\) is the number of cyclones in season \\(i\\).\n\nE.2.2 Exploratory analysis\nSuppose we want to learn about how many cyclones to expect per season.\n\nShow R codelibrary(dobson)\nlibrary(dplyr)\ndata(cyclones)\nlibrary(pander)\npander(cyclones |&gt; relocate(season, .before = everything()))\n\n\nTable E.1: Number of tropical cyclones during a season from November to April in Northeastern Australia\n\n\n\n\n\n\n\n\n\nseason\nyears\nnumber\n\n\n\n1\n1956/7\n6\n\n\n2\n1957/8\n5\n\n\n3\n1958/9\n4\n\n\n4\n1959/60\n6\n\n\n5\n1960/1\n6\n\n\n6\n1961/2\n3\n\n\n7\n1962/3\n12\n\n\n8\n1963/4\n7\n\n\n9\n1964/5\n4\n\n\n10\n1965/6\n2\n\n\n11\n1966/7\n6\n\n\n12\n1967/8\n7\n\n\n13\n1968/9\n4\n\n\n\n\n\n\n\n\n\nShow R codelibrary(ggplot2)\nlibrary(dplyr)\ncyclones |&gt;\n  mutate(years = years |&gt; factor(levels = years)) |&gt;\n  ggplot(aes(x = years, y = number, group = 1)) +\n  geom_point() +\n  geom_line() +\n  xlab(\"Season\") +\n  ylab(\"Number of cyclones\") +\n  expand_limits(y = 0) +\n  theme(axis.text.x = element_text(vjust = .5, angle = 45))\n\n\n\nFigure E.1: Number of tropical cyclones per season in northeastern Australia, 1956-1969\n\n\n\n\n\n\n\nThere’s no obvious correlation between adjacent seasons, so let’s assume that each season is independent of the others.\nLet’s also assume that they are identically distributed; let’s denote this distribution as \\(P(Y=y)\\) (note that there’s no index \\(i\\) in this expression, since we are assuming the \\(Y_i\\)s are identically distributed). We can visualize the distribution using a bar plot (Figure E.2). Table E.2 provides summary statistics.\n\nShow R codecyclones |&gt;\n  ggplot() +\n  geom_histogram(aes(x = number)) +\n  expand_limits(x = 0) +\n  xlab(\"Number of cyclones\") +\n  ylab(\"Count (number of seasons)\")\n\n\n\nFigure E.2: Bar plot of cyclones per season\n\n\n\n\n\n\n\n\nShow R coden = nrow(cyclones)\nsumx = cyclones |&gt; pull(number) |&gt; sum()\nxbar =  cyclones |&gt; pull(number) |&gt; mean()\n\ncyclones |&gt; table1::table1(x = ~ number)\n\n\nTable E.2: Summary statistics for cyclones data\n\n\n\n\n\n\n\n\n\n\n\nOverall(N=13)\n\n\n\nnumber\n\n\n\nMean (SD)\n5.54 (2.47)\n\n\nMedian [Min, Max]\n6.00 [2.00, 12.0]\n\n\n\n\n\n\n\n\n\n\n\nE.2.3 Model\nWe want to estimate \\(P(Y=y)\\); that is, \\(P(Y=y)\\) is our estimand.\nWe could estimate \\(P(Y=y)\\) for each value of \\(y\\) in \\(0:\\infty\\) separately (“nonparametrically”) using the fraction of our data with \\(Y_i=y\\), but then we would be estimating an infinitely large set of parameters, and we would have low precision. We will probably do better with a parametric model.\n\nExercise E.1 What parametric probability distribution family might we use to model this empirical distribution?\n\n\nSolution. Let’s use the Poisson. The Poisson distribution is appropriate for this data , because the data are counts that could theoretically take any integer value (discrete) in the range \\(0:\\infty\\). Visually, the plot of our data closely resembles a Poisson or binomial distribution. Since cyclones do not have an “upper limit” on the number of events we could potentially observe in one season, the Poisson distribution is more appropriate than the binomial.\n\n\nExercise E.2 Write down the Poisson distribution’s probability mass function.\n\n\nSolution. \\[P(Y=y) = \\frac{\\lambda^{y} e^{-\\lambda}}{y!} \\tag{E.6}\\]\n\n\nE.2.4 Estimating the model parameters using maximum likelihood\nNow, we can estimate the parameter \\(\\lambda\\) for this distribution using maximum likelihood estimation.\nWhat is the likelihood?\n\nExercise E.3 Write down the likelihood (probability mass function or probability density function) of a single observation \\(x\\), according to your model.\n\n\nSolution. \\[\n\\begin{aligned}\n\\mathcal{L}(\\lambda; x)\n&= p(X=x|\\Lambda = \\lambda)\\\\\n&= \\frac{\\lambda^x e^{-\\lambda}}{x!}\\\\\n\\end{aligned}\n\\]\n\n\nExercise E.4 Write down the vector of parameters in your model.\n\n\nSolution. There is only one parameter, \\(\\lambda\\):\n\\[\\theta = (\\lambda)\\]\n\n\nExercise E.5 Write down the population mean and variance of a single observation from your chosen probability model, as a function of the parameters (extra credit - derive them).\n\n\nSolution. \n\nPopulation mean: \\(\\text{E}[X]=\\lambda\\)\n\nPopulation variance: \\(\\text{Var}(X)=\\lambda\\)\n\n\n\n\nExercise E.6 Write down the likelihood of the full dataset.\n\n\nSolution. \\[\n\\begin{aligned}\n\\mathcal{L}(\\lambda; \\tilde{x})\n&= \\text{P}(\\tilde{X}= \\tilde{x}) \\\\\n&= \\text{P}(X_1 = x_1, X_2 = x_2, ..., X_{13} = x_{13}) \\\\\n&= \\prod_{i=1}^{13} \\text{P}(X_i = x_i) \\\\\n&= \\prod_{i=1}^{13} \\frac{\\lambda^{x_i} e^{-\\lambda}}{x_i!}\n\\end{aligned}\n\\]\n\n\nExercise E.7 Graph the likelihood as a function of \\(\\lambda\\).\n\nSolution. \n\nShow R codelik = function(lambda, y = cyclones$number, n = length(y)) \n{\nlambda^sum(y) * exp(-n*lambda) / prod(factorial(y))\n}\n\nlibrary(ggplot2)\nlik_plot = \nggplot() +\ngeom_function(fun = lik, n = 1001) +\nxlim(min(cyclones$number), max(cyclones$number)) +\nylab(\"likelihood\") +\nxlab('lambda')\n\nprint(lik_plot)\n\n\n\nFigure E.3: Likelihood of Dobson cyclone data\n\n\n\n\n\n\n\n\n\n\nExercise E.8 Write down the log-likelihood of the full dataset.\n\nSolution. \\[\n\\begin{aligned}\n\\ell(\\lambda; \\tilde{x}) &= \\text{log}\\left\\{\\mathcal{L}(\\lambda;\\tilde{x})\\right\\}\\\\\n&= \\text{log}\\left\\{\\prod_{i = 1}^n\\frac{\\lambda^{x_i}\\text{e}^{-\\lambda}}{x_i!}\\right\\}\\\\\n&= \\sum_{i = 1}^n\\text{log}\\left\\{\\frac{\\lambda^{x_i}\\text{e}^{-\\lambda}}{x_i!}\\right\\}\\\\\n&= \\sum_{i = 1}^n{\\text{log}\\left\\{\\lambda^{x_i}\\right\\} +\\text{log}\\left\\{\\text{e}^{-\\lambda}\\right\\} - \\text{log}\\left\\{x_i!\\right\\}}\\\\\n&= \\sum_{i = 1}^n{x_i\\text{log}\\left\\{\\lambda\\right\\} -\\lambda - \\text{log}\\left\\{x_i!\\right\\}}\\\\\n&= \\sum_{i = 1}^nx_i\\text{log}\\left\\{\\lambda\\right\\} - \\sum_{i = 1}^n\\lambda - \\sum_{i = 1}^n\\text{log}\\left\\{x_i!\\right\\}\\\\\n&= \\sum_{i = 1}^nx_i\\text{log}\\left\\{\\lambda\\right\\} - n\\lambda - \\sum_{i = 1}^n\\text{log}\\left\\{x_i!\\right\\}\\\\\n\\end{aligned}\n\\]\n\n\n\nExercise E.9 Graph the log-likelihood as a function of \\(\\lambda\\).\n\nSolution. \n\nShow R code\nloglik = function(lambda, y = cyclones$number, n = length(y))\n{\nsum(y) * log(lambda) - n*lambda - sum(log(factorial(y)))\n}\n\nll_plot = ggplot() +\ngeom_function(fun = loglik, n = 1001) +\nxlim(min(cyclones$number), max(cyclones$number)) +\nylab(\"log-likelihood\") +\nxlab('lambda')\nll_plot\n\n\n\nFigure E.4: log-likelihood of Dobson cyclone data\n\n\n\n\n\n\n\n\n\nThe score function\n\nExercise E.10 Derive the score function for the dataset.\n\nSolution. The score function is the first derivative of the log-likelihood:\n\\[\n\\begin{aligned}\n\\ell'( \\lambda; \\tilde{x} ) &=\n\\frac{\\partial}{\\partial \\lambda}{\\sum_{i = 1}^nx_i\\text{log}\\left\\{\\lambda\\right\\} - n\\lambda - \\sum_{i = 1}^n\\text{log}\\left\\{x_i!\\right\\}}\\\\\n&= \\frac{\\partial}{\\partial \\lambda}\\sum_{i = 1}^nx_i\\text{log}\\left\\{\\lambda\\right\\} - \\frac{\\partial}{\\partial \\lambda}n\\lambda - \\frac{\\partial}{\\partial \\lambda}\\sum_{i = 1}^n\\text{log}\\left\\{x_i!\\right\\}\\\\\n&= \\sum_{i = 1}^nx_i\\frac{\\partial}{\\partial \\lambda}\\text{log}\\left\\{\\lambda\\right\\} - n\\frac{\\partial}{\\partial \\lambda}\\lambda - \\sum_{i = 1}^n\\frac{\\partial}{\\partial \\lambda}\\text{log}\\left\\{x_i!\\right\\}\\\\\n&= \\sum_{i = 1}^nx_i\\frac{1}{\\lambda} - n - 0\\\\\n&= \\frac{1}{\\lambda} \\sum_{i = 1}^nx_i- n\n\\\\&= \\left(\\frac{1}{\\lambda} n \\bar{x}\\right) - n\n\\\\&= \\left(\\frac{1}{\\lambda} 72\\right) - 13\n\\end{aligned}\n\\]\n\n\n\nExercise E.11 Graph the score function.\n\nSolution. \n\nShow R code\nscore = function(lambda, y = cyclones$number, n = length(y))\n{\n  (sum(y) / lambda) - n\n}\n\nggplot() +\ngeom_function(fun = score, n = 1001) +\nxlim(min(cyclones$number), max(cyclones$number)) +\n\nylab(\"l'(lambda)\") +\nxlab('lambda') +\ngeom_hline(yintercept = 0, col = 'red')\n\n\n\nFigure E.5: score function of Dobson cyclone data\n\n\n\n\n\n\n\n\n\nThe Hessian matrix\n\nExercise E.12 Derive the Hessian matrix.\n\nSolution. The Hessian function for an iid sample is the 2nd derivative(s) of the log-likelihood:\n\\[\n\\begin{aligned}\n\\ell''( \\lambda; \\tilde{x} ) &= \\frac{\\partial}{\\partial \\lambda}\\left(\\frac{1}{\\lambda} \\sum_{i = 1}^nx_i- n\\right)\\\\\n&= \\frac{\\partial}{\\partial \\lambda}\\frac{1}{\\lambda} \\sum_{i = 1}^nx_i- \\frac{\\partial}{\\partial \\lambda}n\\\\\n&= -\\frac{1}{\\lambda^2} \\sum_{i = 1}^nx_i\\\\\n&= -\\frac{1}{\\lambda^2} n \\bar x\n\\\\&= -\\frac{1}{\\lambda^2} \\cdot 72\n\\end{aligned}\n\\]\n\n\n\nExercise E.13 Graph the Hessian.\n\nSolution. \n\nShow R code\nhessian = function(lambda, y = cyclones$number, n = length(y))\n{\n-sum(y)/(lambda^2)\n}\n\nggplot() +\ngeom_function(fun = hessian, n = 1001) +\nxlim(min(cyclones$number), max(cyclones$number)) +\n\nylab(\"l''(lambda)\") +\nxlab('lambda') +\ngeom_hline(yintercept = 0, col = 'red')\n\n\n\nFigure E.6: Hessian function of Dobson cyclone data\n\n\n\n\n\n\n\n\n\n\nExercise E.14 Write the score equation (estimating equation).\n\nSolution. \\[\\ell'( \\lambda; \\tilde{x} ) = 0\\]\n\n\n\nExercise E.15 Solve the estimating equation for \\(\\lambda\\):\n\nSolution. \\[\n\\begin{aligned}\n0 &= \\frac{1}{\\lambda}\\sum_{i = 1}^nx_i - n\\\\\nn &= \\frac{1}{\\lambda}\\sum_{i = 1}^nx_i\\\\\nn\\lambda &= \\sum_{i = 1}^nx_i\\\\\n\\lambda &=\n\\frac{1}{n}\\sum_{i = 1}^nx_i\\\\\n&=\\bar x\n\\end{aligned}\n\\]\n\n\nLet’s call this solution of the estimating equation \\(\\tilde \\lambda\\) for now:\n\\[\\tilde \\lambda \\stackrel{\\text{def}}{=}\\bar x\\]\n\nExercise E.16 Confirm that the Hessian \\(\\ell''(\\lambda; \\tilde{x})\\) is negative when evaluated at \\(\\tilde \\lambda\\).\n\nSolution. \\[\n\\begin{aligned}\n\\ell''( \\tilde\\lambda; \\tilde{x} ) &=\n-\\frac{1}{\\tilde\\lambda^2} n \\bar x\\\\\n&= -\\frac{1}{\\bar x^2} n\\bar x\\\\\n&= -\\frac{n}{\\bar x}\\\\\n&&lt;0\\\\\n\\end{aligned}\n\\]\n\n\n\nExercise E.17 Find the MLE of \\(\\lambda\\).\n\nSolution. Since \\(\\ell''(\\tilde \\lambda; \\tilde{x})&lt;0\\), \\(\\tilde \\lambda\\) is at least a local maximizer of the likelihood function \\(\\mathcal L(\\lambda)\\). Since there is only one solution to the estimating equation and the Hessian is negative definite everywhere, \\(\\tilde \\lambda\\) must also be the global maximizer of \\(\\mathcal L(\\lambda; \\tilde{x})\\):\n\nShow R codemle = mean(cyclones$number)\n\n\n\\[\\hat{\\lambda}_{\\text{ML}} = \\bar x = 5.5385\\]\n\n\n\nExercise E.18 Graph the log-likelihood with the MLE superimposed.\n\nSolution. \n\nShow R codelibrary(dplyr)\n\nmle_data = tibble(x = mle, y = loglik(mle))\nll_plot + geom_point(data = mle_data, aes(x = x, y = y), col = 'red')\n\n\n\nFigure E.7: log-likelihood of Dobson cyclone data with MLE\n\n\n\n\n\n\n\n\n\nInformation matrices\n\nShow R codeobs_inf = function(...) -hessian(...)\nggplot() +\ngeom_function(fun = obs_inf, n = 1001) +\nxlim(min(cyclones$number), max(cyclones$number)) +\nylab(\"I(lambda)\") +\nxlab('lambda') +\ngeom_hline(yintercept = 0, col = 'red') \n\n\n\nFigure E.8: Observed information function of Dobson cyclone data\n\n\n\n\n\n\n\n\n\nExample E.1 (Finding the MLE using the Newton-Raphson algorithm)  \n\nWe found that the MLE was \\(\\hat{\\lambda} = \\bar{x}\\), by solving the score equation \\(\\ell'(\\lambda)=0\\) for \\(\\lambda\\).\nWhat if we hadn’t been able to solve the score equation?\nThen we could start with some initial guess \\({\\widehat{\\lambda}}^*\\), such as \\({\\widehat{\\lambda}}^*= 3\\), and use the Newton-Raphson algorithm.\n\n\nShow R code# specify initial guess:\ncur_lambda_est = 3\n\n\n\n\n\nIn Exercise E.10, we found that the score function was:\n\n\\[\n\\ell'( \\lambda; \\tilde{x} ) = \\left(\\frac{72}{\\lambda} \\right) - n\n\\]\n\nIn Exercise E.12, we found that the Hessian was:\n\n\\[\n\\ell''( \\lambda; \\tilde{x} ) = -\\frac{72}{\\lambda^2}\n\\]\n\n\nSo we can approximate the the score function using the first-order Taylor polynomial:\n\n\\[\n\\begin{aligned}\n\\ell'(\\lambda)\n&\\approx \\ell'^*(\\lambda)\n\\\\ &\\stackrel{\\text{def}}{=}\\ell'({\\widehat{\\lambda}}^*) + \\ell''({\\widehat{\\lambda}}^*)(\\lambda - {\\widehat{\\lambda}}^*)\n\\\\ &= \\left(\\frac{72}{{\\widehat{\\lambda}}^*}  - n\\right) + \\left(-\\frac{72}{\\left({\\widehat{\\lambda}}^*\\right)^2}\\right) (\\lambda - {\\widehat{\\lambda}}^*)\n\\end{aligned}\n\\]\n\n\nFigure E.9 compares the true score function and the approximate score function at \\({\\widehat{\\lambda}}^*= 3\\).\n\n\nShow R codeapprox_score = function(lambda, lhat, ...)\n{\n  score(lambda = lhat, ...) +\n  hessian(lambda = lhat, ...) * (lambda - lhat)\n}\n\npoint_size = 5\n\nplot1 = ggplot() +\ngeom_function(\n  fun = score, \n  aes(col = \"true score function\"), \n  n = 1001) +\ngeom_function(\n  fun = approx_score, \n  aes(col = \"approximate score function\"),\n  n = 1001, \n  args = list(lhat = cur_lambda_est)) +\ngeom_point(\n  size = point_size,\n  aes(x = cur_lambda_est, y = score(lambda = cur_lambda_est),\n      col = \"current estimate\")\n) +\ngeom_point(\nsize = point_size,\naes(\nx = xbar,\ny = 0,\ncol = \"true MLE\"\n)\n) +\nxlim(min(cyclones$number), max(cyclones$number)) +\nylab(\"l'(lambda)\") +\nxlab('lambda') +\ngeom_hline(yintercept = 0)\n\nprint(plot1)\n\n\n\nFigure E.9: score function of Dobson cyclone data and approximate score function\n\n\n\n\n\n\n\n\n\nThis is equivalent to estimating the log-likelihood with a second-order Taylor polynomial:\n\n\\[\n\\ell^*(\\lambda) =\n\\ell({\\widehat{\\lambda}}^*) +\n(\\lambda - {\\widehat{\\lambda}}^*) \\ell'({\\widehat{\\lambda}}^*) +\n\\frac{1}{2}\\ell''({\\widehat{\\lambda}}^*)(\\lambda-{\\widehat{\\lambda}}^*)^2\n\\]\n\nShow R codeapprox_loglik = function(lambda, lhat, ...)\n{\nloglik(lambda = lhat, ...) +\nscore(lambda = lhat, ...) * (lambda - lhat) +\n  1/2 * hessian(lambda = lhat, ...) * (lambda - lhat)^2\n}\n\nplot_loglik = ggplot() +\ngeom_function(\n  fun = loglik, \n  aes(col = \"true log-likelihood\"), \n  n = 1001) +\ngeom_function(\n  fun = approx_loglik, \n  aes(col = \"approximate log-likelihood\"),\n  n = 1001, \n  args = list(lhat = cur_lambda_est)) +\ngeom_point(\n  size = point_size,\n  aes(x = cur_lambda_est, y = loglik(lambda = cur_lambda_est),\n      col = \"current estimate\")\n) +\ngeom_point(\nsize = point_size,\naes(\nx = xbar,\ny = loglik(xbar),\ncol = \"true MLE\"\n)\n) +\nxlim(min(cyclones$number) - 1, max(cyclones$number)) +\nylab(\"l'(lambda)\") +\nxlab('lambda')\n\nprint(plot_loglik)\n\n\n\nFigure E.10: log-likelihood of Dobson cyclone data and approximate log-likelihood function\n\n\n\n\n\n\n\n\n\nThe approximate score function, \\(\\ell'^*(\\lambda)\\), is a linear function of \\(\\lambda\\), so it is easy to solve the corresponding approximate score equation, \\(\\ell'^*(\\lambda) = 0\\), for \\(\\lambda\\):\n\n\\[\n\\begin{aligned}\n\\lambda\n&= {\\widehat{\\lambda}}^*- \\ell'({\\widehat{\\lambda}}^*) \\cdot\\left(\\ell''({\\widehat{\\lambda}}^*)\\right)^{-1}\n\\\\ &= 4.375\n\\end{aligned}\n\\]\n\nShow R codenew_lambda_est &lt;- \n   cur_lambda_est - \n   score(cur_lambda_est) * hessian(cur_lambda_est)^-1\n\n\n\n\nShow R codeplot2 = plot1 +\n  geom_point(size = point_size,\n             aes(x = new_lambda_est,\n                 y = 0,\n                 col = \"new estimate\")) +\n  geom_segment(\n    arrow = grid::arrow(),\n    linewidth = 2,\n    alpha = .7,\n    aes(\n      x = cur_lambda_est,\n      y = approx_score(lhat = cur_lambda_est,\n                       lambda = cur_lambda_est),\n      xend = new_lambda_est,\n      yend = 0,\n      col = \"update\"\n    )\n  )\nprint(plot2)\n\n\n\nFigure E.11: score function of Dobson cyclone data and approximate score function\n\n\n\n\n\n\n\n\n\nSo we update \\({\\widehat{\\lambda}}^*\\leftarrow 4.375\\) and repeat our estimation process:\n\n\nShow R codeplot2 +\ngeom_function(\n  fun = approx_score, \n  aes(col = \"new approximate score function\"),\n  n = 1001, \n  args = list(lhat = new_lambda_est)) +\ngeom_point(\n  size = point_size,\n  aes(x = new_lambda_est, y = score(lambda = new_lambda_est),\n      col = \"new estimate\")\n)\n\n\n\nFigure E.12: score function of Dobson cyclone data and approximate score function\n\n\n\n\n\n\n\n\n\nWe repeat this process until the likelihood converges:\n\n\nShow R code\nlibrary(tibble)\ncur_lambda_est = 3 # restarting\ndiff_loglik = Inf\ntolerance = 10 ^ -4\nmax_iter = 100\nNR_info = tibble(\n  iteration = 0,\n  lambda = cur_lambda_est |&gt; num(digits = 4),\n  likelihood = lik(cur_lambda_est),\n  `log(likelihood)` = loglik(cur_lambda_est)  |&gt; num(digits = 4),\n  score = score(cur_lambda_est),\n  hessian = hessian(cur_lambda_est)\n)\n\nfor (cur_iter in 1:max_iter)\n{\n  new_lambda_est &lt;-\n    cur_lambda_est - score(cur_lambda_est) * hessian(cur_lambda_est) ^ -1\n  \n  diff_loglik = loglik(new_lambda_est) - loglik(cur_lambda_est)\n  \n  new_NR_info = tibble(\n    iteration = cur_iter,\n    lambda = new_lambda_est,\n    likelihood = lik(new_lambda_est),\n    `log(likelihood)` = loglik(new_lambda_est),\n    score = score(new_lambda_est),\n    hessian = hessian(new_lambda_est),\n    `diff(loglik)` = diff_loglik\n  )\n  \n  NR_info = NR_info |&gt; bind_rows(new_NR_info)\n  \n  cur_lambda_est = new_lambda_est\n  \n  if (abs(diff_loglik) &lt; tolerance)\n    break\n  \n}\n\nNR_info\n\n\nTable E.3: Convergence of Newton-Raphson Algorithm for finding MLE of cyclone data\n\n\n\n  \n\n\n\n\n\n\n\nCompare with Exercise E.17\n\n\n\nShow R codell_plot +\n  geom_segment(\n    data = NR_info,\n    arrow = grid::arrow(),\n    # linewidth = 2,\n    alpha = .7,\n    aes(\n      x = lambda,\n      xend = lead(lambda),\n      y = `log(likelihood)`,\n      yend = lead(`log(likelihood)`),\n      col = factor(iteration)\n    )\n  )\n\n\n\nFigure E.13: Newton-Raphson algorithm for finding MLE of model E.6 for cyclone data",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Introduction to Maximum Likelihood Inference</span>"
    ]
  },
  {
    "objectID": "intro-MLEs.html#maximum-likelihood-inference-for-univariate-gaussian-models",
    "href": "intro-MLEs.html#maximum-likelihood-inference-for-univariate-gaussian-models",
    "title": "Appendix E — Introduction to Maximum Likelihood Inference",
    "section": "\nE.3 Maximum likelihood inference for univariate Gaussian models",
    "text": "E.3 Maximum likelihood inference for univariate Gaussian models\nSuppose \\(X_{1}, ..., X_{n} \\ \\sim_{\\text{iid}}\\ N(\\mu, \\sigma^{2})\\). Let \\(X = (X_{1},\\ldots,X_{n})^{\\top}\\) be these random variables in vector format. Let \\(x_{i}\\) and \\(x\\) denote the corresponding observed data. Then \\(\\theta = (\\mu,\\sigma^{2})\\) is the vector of true parameters, and \\(\\Theta = (\\text{M}, \\Sigma^2)\\) is the vector of parameters as a random vector.\nThen the log-likelihood is:\n\\[\n\\begin{aligned}\n\\ell\n&\\propto - \\frac{n}{2}\\text{log}\\left\\{\\sigma^{2}\\right\\} - \\frac{1}{2}\\sum_{i = 1}^{n}\\frac{( x_{i} - \\mu)^{2}}{\\sigma^{2}}\\\\\n&= - \\frac{n}{2}\\text{log}\\left\\{\\sigma^{2}\\right\\} - \\frac{1}{2\\sigma^{2}}\\sum_{i = 1}^{n}{x_{i}^{2} - 2x_{i}\\mu + \\mu^{2}}\n\\end{aligned}\n\\]\n\nE.3.1 The score function\n\\[\\ell'(x,\\theta) \\stackrel{\\text{def}}{=}\\frac{\\partial}{\\partial \\theta}\\ell(x,\\theta) = \\left( \\begin{array}{r}\n\\frac{\\partial}{\\partial \\mu}\\ell(\\theta;x) \\\\\n\\frac{\\partial}{\\partial \\sigma^{2}}\\ell(\\theta;x)\n\\end{array} \\right) = \\left( \\begin{array}{r}\n\\ell_{\\mu}'(\\theta;x) \\\\\n\\ell_{\\sigma^{2}}'(\\theta;x)\n\\end{array} \\right)\\].\n\\(\\ell'(x,\\theta)\\) is the function we set equal to 0 and solve to find the MLE:\n\\[{\\widehat{\\theta}}_{ML} = \\left\\{ \\theta:\\ell'(x,\\theta) = 0 \\right\\}\\]\n\nE.3.2 MLE of \\(\\mu\\)\n\n\\[\n\\begin{aligned}\n\\frac{d\\ell}{d\\mu}\n&= - \\frac{1}{2}\\sum_{i = 1}^{n}\n\\frac{- 2(x_{i} - \\mu)}{\\sigma^{2}}\n\\\\ &= \\frac{1}{\\sigma^{2}}\n\\left[\n    \\left(\n        \\sum_{i = 1}^{n}x_{i}\n    \\right)\n    - n\\mu\n\\right]\n\\end{aligned}\n\\]\nIf \\(\\frac{d\\ell}{d\\mu} = 0\\), then \\(\\mu = \\overline{x} \\stackrel{\\text{def}}{=}\\frac{1}{n}\\sum_{i = 1}^{n}x_{i}\\).\n\\[\\frac{d^{2}\\ell}{(d\\mu)^{2}} = \\frac{- n}{\\sigma^{2}} &lt; 0\\]\nSo \\({\\widehat{\\mu}}_{ML} = \\overline{x}\\).\n\nE.3.3 MLE of \\(\\sigma^{2}\\)\n\n\n\n\n\n\n\nReparametrizing the Gaussian distribution\n\n\n\nWhen solving for \\({\\widehat{\\sigma}}_{ML}\\), you can treat \\(\\sigma^{2}\\) as an atomic variable (don’t differentiate with respect to \\(\\sigma\\) or things get messy). In fact, you can replace \\(\\sigma^{2}\\) with \\(1/\\tau\\) and differentiate with respect to \\(\\tau\\) instead, and the process might be even easier.\n\n\n\\[\\frac{d\\ell}{d\\sigma^{2}} = \\frac{\\partial}{\\partial \\sigma^{2}}\\left( - \\frac{n}{2}\\text{log}\\left\\{\\sigma^{2}\\right\\} - \\frac{1}{2}\\sum_{i = 1}^{n}\\frac{\\left( x_{i} - \\mu \\right)^{2}}{\\sigma^{2}} \\right)\\ \\]\n\\[= - \\frac{n}{2}\\left( \\sigma^{2} \\right)^{- 1} + \\frac{1}{2}\\left( \\sigma^{2} \\right)^{- 2}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2}\\]\nIf \\(\\frac{d\\ell}{d\\sigma^{2}} = 0\\), then:\n\\[\\frac{n}{2}\\left( \\sigma^{2} \\right)^{- 1} = \\frac{1}{2}\\left( \\sigma^{2} \\right)^{- 2}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2}\\]\n\\[\\sigma^{2} = \\frac{1}{n}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2}\\]\nWe plug in \\({\\widehat{\\mu}}_{ML} = \\overline{x}\\) to maximize globally (a technique called profiling):\n\\[\\sigma_{ML}^{2} = \\frac{1}{n}\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}\\]\nNow:\n\\[\n\\begin{aligned}\n\\frac{d^{2}\\ell}{\\left( d\\sigma^{2} \\right)^{2}}\n&= \\frac{\\partial}{\\partial \\sigma^{2}}\\left\\{ - \\frac{n}{2}\\left( \\sigma^{2} \\right)^{- 1} + \\frac{1}{2}\\left( \\sigma^{2} \\right)^{- 2}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2} \\right\\}\\\\\n&= \\left\\{ - \\frac{n}{2}\\frac{\\partial}{\\partial \\sigma^{2}}\\left( \\sigma^{2} \\right)^{- 1} + \\frac{1}{2}\\frac{\\partial}{\\partial \\sigma^{2}}\\left( \\sigma^{2} \\right)^{- 2}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2} \\right\\}\\\\\n&= \\left\\{ \\frac{n}{2}\\left( \\sigma^{2} \\right)^{- 2} - \\left( \\sigma^{2} \\right)^{- 3}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2} \\right\\}\\\\\n&= \\left( \\sigma^{2} \\right)^{- 2}\\left\\{ \\frac{n}{2} - \\left( \\sigma^{2} \\right)^{- 1}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2} \\right\\}\n\\end{aligned}\n\\]\nEvaluated at \\(\\mu = \\overline{x},\\sigma^{2} = \\frac{1}{n}\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}\\), we have:\n\\[\n\\begin{aligned}\n\\frac{d^{2}\\ell}{\\left( d\\sigma^{2} \\right)^{2}}\n&= \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 2}\\left\\{ \\frac{n}{2} - \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 1}\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2} \\right\\}\\\\\n&= \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 2}\\left\\{ \\frac{n}{2} - \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 1}n{\\widehat{\\sigma}}^{2} \\right\\}\\\\\n&= \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 2}\\left\\{ \\frac{n}{2} - n \\right\\}\\\\\n&= \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 2}n\\left\\{ \\frac{1}{2} - 1 \\right\\}\\\\\n&= \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 2}n\\left( - \\frac{1}{2} \\right) &lt; 0\n\\end{aligned}\n\\]\nFinally, we have:\n\\[\n\\begin{aligned}\n\\frac{d^{2}\\ell}{d\\mu\\ d\\sigma^{2}}\n&= \\frac{\\partial}{\\partial \\mu}\\left\\{ - \\frac{n}{2}\\left( \\sigma^{2} \\right)^{- 1} + \\frac{1}{2}\\left( \\sigma^{2} \\right)^{- 2}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2} \\right\\}\\\\\n&= \\frac{1}{2}\\left( \\sigma^{2} \\right)^{- 2}\\frac{\\partial}{\\partial \\mu}\\sum_{i = 1}^{n}\\left( x_{i} - \\mu \\right)^{2}\\\\\n&= \\frac{1}{2}\\left( \\sigma^{2} \\right)^{- 2}\\sum_{i = 1}^{n}{- 2(x_{i} - \\mu)}\\\\\n&= - \\left( \\sigma^{2} \\right)^{- 2}\\sum_{i = 1}^{n}{(x_{i} - \\mu)}\n\\end{aligned}\n\\]\nEvaluated at \\(\\mu = \\widehat{\\mu} = \\overline{x},\\sigma^{2} = {\\widehat{\\sigma}}^{2} = \\frac{1}{n}\\sum_{i = 1}^{n}\\left( x_{i} - \\overline{x} \\right)^{2}\\), we have:\n\\[\\frac{d^{2}\\ell}{d\\mu\\ d\\sigma^{2}} = - \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 2}\\left( n\\overline{x} - n\\overline{x} \\right) = 0\\]\n\nE.3.4 Covariance matrix\n\\[I = \\begin{bmatrix}\n\\frac{n}{\\sigma^{2}} & 0 \\\\\n0 & \\left( {\\widehat{\\sigma}}^{2} \\right)^{- 2}n\\left( - \\frac{1}{2} \\right)\n\\end{bmatrix} = \\begin{bmatrix}\na & 0 \\\\\n0 & d\n\\end{bmatrix}\\]\nSo:\n\\[I^{- 1} = \\frac{1}{ad}\\begin{bmatrix}\nd & 0 \\\\\n0 & a\n\\end{bmatrix} = \\begin{bmatrix}\n\\frac{1}{a} & 0 \\\\\n0 & \\frac{1}{d}\n\\end{bmatrix}\\]\n\\[I^{- 1} = \\begin{bmatrix}\n\\frac{{\\widehat{\\sigma}}^{2}}{n} & 0 \\\\\n0 & \\frac{{2\\left( {\\widehat{\\sigma}}^{2} \\right)}^{2}}{n}\n\\end{bmatrix}\\]\nSee Casella and Berger (2002) p322, example 7.2.12.\nTo prove it’s a maximum, we need:\n\n\\(\\ell' = 0\\)\nAt least one diagonal element of \\(\\ell''\\) is negative.\nDeterminant of \\(\\ell''\\) is positive.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Introduction to Maximum Likelihood Inference</span>"
    ]
  },
  {
    "objectID": "intro-MLEs.html#example-hormone-therapy-study",
    "href": "intro-MLEs.html#example-hormone-therapy-study",
    "title": "Appendix E — Introduction to Maximum Likelihood Inference",
    "section": "\nE.4 Example: hormone therapy study",
    "text": "E.4 Example: hormone therapy study\n\nNow, we’re going to analyze some real-world data using a Gaussian model, and then we’re going to do a simulation to examine the properties of maximum likelihood estimation for that Gaussian model.\nHere we look at the “heart and estrogen/progestin study” (HERS), a clinical trial of hormone therapy for prevention of recurrent heart attacks and death among 2,763 post-menopausal women with existing coronary heart disease (CHD) (Hulley et al. 1998).\nWe are going to model the distribution of fasting glucose among nondiabetics who don’t exercise.\n\n\nShow R code# load the data directly from a UCSF website\nhers = haven::read_dta(\n  paste0( # I'm breaking up the url into two chunks for readability\n    \"https://regression.ucsf.edu/sites/g/files\",\n    \"/tkssra6706/f/wysiwyg/home/data/hersdata.dta\"\n  )\n)\n\n\n\nShow R codehers |&gt; head()\n\n\nTable E.4: HERS dataset\n\n\n\n  \n\n\n\n\n\n\n\n\nShow R code\nn.obs = 100 # we're going to take a small subset of the data to look at; \n# if we took the whole data set, the likelihood function would be hard to \n# graph nicely\n\nlibrary(dplyr)\ndata1 = \n  hers |&gt; \n  filter(\n    diabetes == 0,\n    exercise == 0) |&gt; \n  head(n.obs)\n\nglucose_data = \n  data1 |&gt; \n  pull(glucose)\n\nlibrary(ggplot2)\nlibrary(ggeasy)\nplot1 = \n  data1 |&gt; \n  ggplot(aes(x = glucose)) +\n  geom_histogram(aes(x = glucose, after_stat(density))) +\n  theme_classic() +\n  easy_labs()\n\nprint(plot1)\n\n\n\n\n\n\n\nLooks somewhat plausibly Gaussian. Good enough for this example!\n\nE.4.1 Find the MLEs\n\nShow R code\nmu_hat = mean(glucose_data)\nsigma_sq_hat = mean((glucose_data - mean(glucose_data))^2)\n\n\nOur MLEs are:\n\n\\(\\hat\\mu = 98.66\\)\n\\(\\hat\\sigma^2 = 104.7444\\)\n\nHere’s the estimated distribution, superimposed on our histogram:\n\nShow R code\nplot1 +\n  geom_function(\n    fun = function(x) dnorm(x, mean = mu_hat, sd = sqrt(sigma_sq_hat)),\n    col = \"red\"\n  )\n\n\n\n\n\n\n\nLooks like a somewhat decent fit? We could probably do better, but that’s for another time.\n\nE.4.2 Construct the likelihood and log-likelihood functions\n\nit’s often computationally more effective to construct the log-likelihood first and then exponentiate it to get the likelihood\n\n\nShow R code\nloglik = function(\n    mu, # I'm assigning default values, which the function will use \n    # unless we tell it otherwise\n    sigma = sd(x), # note that you can define some defaults based on other arguments\n    x = glucose_data, \n    n = length(x)\n)\n{\n  \n  normalizing_constants = -n/2 * log((sigma^2) * 2 * pi) \n  \n  likelihood_kernel = - 1/(2 * sigma^2) * \n    {\n      # I have to do this part in a somewhat complicated way\n      # so that we can pass in vectors of possible values of mu\n      # and get the likelihood for each value;\n      # for the binomial case it's easier\n      sum(x^2) - 2 * sum(x) * mu + n * mu^2\n    }\n  \n  answer = normalizing_constants + likelihood_kernel\n  \n  return(answer)\n  \n}\n\n# `...` means pass any inputs to lik() along to loglik()\nlik = function(...) exp(loglik(...))\n\n\n\nE.4.3 Graph the Likelihood as a function of \\(\\mu\\)\n\n(fixing \\(\\sigma^2\\) at \\(\\hat\\sigma^2 = 104.7444\\))\n\nShow R code\nggplot() + \n  geom_function(fun = function(x) lik(mu = x, sigma = sigma_sq_hat)) + \n  xlim(mean(glucose_data) + c(-1,1) * sd(glucose_data)) +\n  xlab(\"possible values of mu\") +\n  ylab(\"likelihood\") + \n  geom_vline(xintercept = mean(glucose_data), col = \"red\")\n\n\n\n\n\n\n\n\nE.4.4 Graph the Log-likelihood as a function of \\(\\mu\\)\n\n(fixing \\(\\sigma^2\\) at \\(\\hat\\sigma^2 = 104.7444\\))\n\nShow R code\nggplot() + \n  geom_function(fun = function(x) loglik(mu = x, sigma = sigma_sq_hat)) + \n  xlim(mean(glucose_data) + c(-1,1) * sd(glucose_data)) +\n  xlab(\"possible values of mu\") +\n  ylab('log(likelihood)') + \n  geom_vline(xintercept = mean(glucose_data), col = \"red\")\n\n\n\n\n\n\n\n\nE.4.5 Likelihood and log-likelihood for \\(\\sigma\\), conditional on \\(\\mu = \\hat\\mu\\):\n\nShow R code\n\nggplot() + \n  geom_function(fun = function(x) lik(sigma = x, mu = mean(glucose_data))) + \n  xlim(sd(glucose_data) * c(.9,1.1)) + \n  geom_vline(\n    xintercept = sd(glucose_data) * sqrt(n.obs - 1)/sqrt(n.obs), \n    col = \"red\") +\n  xlab(\"possible values for sigma\") +\n  ylab('Likelihood')\n\n\n\n\n\n\nShow R code\nggplot() + \n  geom_function(\n    fun = function(x) loglik(sigma = x, mu = mean(glucose_data))\n  ) + \n  xlim(sd(glucose_data) * c(0.9, 1.1)) +\n  geom_vline(\n    xintercept = \n      sd(glucose_data) * sqrt(n.obs - 1) / sqrt(n.obs), \n    col = \"red\") +\n  xlab(\"possible values for sigma\") +\n  ylab(\"log(likelihood)\")\n\n\n\n\n\n\n\n\nE.4.6 Standard errors by sample size:\n\nShow R code\nse.mu.hat = function(n, sigma = sd(glucose_data)) sigma/sqrt(n)\nggplot() + \n  geom_function(fun = se.mu.hat) + \n  scale_x_continuous(trans = \"log10\", limits = c(10, 10^5), name = \"Sample size\") +\n  ylab(\"Standard error of mu (mg/dl)\") +\n  theme_classic()\n\n\n\n\n\n\n\n\nE.4.7 Simulations\nCreate simulation framework\nHere’s a function that performs a single simulation of a Gaussian modeling analysis:\n\nShow R code\ndo_one_sim = function(\n    n = 100,\n    mu = mean(glucose_data),\n    mu0 = mean(glucose_data) * 0.9,\n    sigma2 = var(glucose_data), \n    return_data = FALSE # if this is set to true, we will create a list() containing both \n    # the analytic results and the vector of simulated data\n)\n{\n  \n  # generate data\n  x = rnorm(n = 100, mean = mu, sd = sqrt(sigma2))\n  \n  # analyze data\n  mu_hat = mean(x)\n  sigmahat = sd(x)\n  se_hat = sigmahat/sqrt(n)\n  confint = mu_hat + c(-1, 1) * se_hat * qt(.975, df = n - 1)\n  tstat = abs(mu_hat - mu0) / se_hat\n  pval = pt(df = n - 1, q = tstat, lower = FALSE) * 2\n  confint_covers = between(mu, confint[1], confint[2])\n  test_rejects = pval &lt; 0.05\n  \n  # if you want spaces, hyphens, or characters in your column names, use \"\", '', or ``:\n  to_return = tibble(\n    \"mu-hat\" = mu_hat,\n    \"sigma-hat\" = sigmahat,\n    \"se_hat\" = se_hat,\n    \"confint_left\" = confint[1],\n    \"confint_right\" = confint[2],\n    \"tstat\" = tstat,\n    \"pval\" = pval, \n    \"confint covers true mu\" = confint_covers,\n    \"test rejects null hypothesis\" = test_rejects\n  )\n  \n  if(return_data)\n  {\n    return(\n      list(\n        data = x, \n        results = to_return))\n  } else\n  {\n    return(to_return)\n  }\n  \n}\n\n\nLet’s see what this function outputs for us:\n\nShow R code\ndo_one_sim()\n\n\n  \n\n\n\nLooks good!\nNow let’s check it against the t.test() function from the stats package:\n\nShow R code\nset.seed(1)\nmu = mean(glucose_data)\nmu0 = 80\nsim.output = do_one_sim(mu0 = mu0, return_data = TRUE)\nour_results = \n  sim.output$results |&gt; \n  mutate(source = \"`do_one_sim()`\")\n\nresults_t.test = t.test(sim.output$data, mu = mu0)\n\nresults2 = \n  tibble(\n    source = \"`stats::t.test()`\",\n    \"mu-hat\" = results_t.test$estimate,\n    \"sigma-hat\" = results_t.test$stderr*sqrt(length(sim.output$data)),\n    \"se_hat\" = results_t.test$stderr,\n    confint_left = results_t.test$conf.int[1],\n    confint_right = results_t.test$conf.int[2],\n    tstat = results_t.test$statistic,\n    pval = results_t.test$p.value,\n    \"confint covers true mu\" = between(mu, confint_left, confint_right),\n     `test rejects null hypothesis` = pval &lt; 0.05\n  )\n\ncomparison = \n  bind_rows(\n    our_results,\n    results2\n  ) |&gt; \n  relocate(\n    \"source\",\n    .before = everything()\n  )\n\ncomparison\n\n\n  \n\n\n\nLooks like we got it right!\nRun 1000 simulations\nHere’s a function that calls the previous function n_sims times and summarizes the results:\n\nShow R code\ndo_n_sims = function(\n    n_sims = 1000,\n    ... # this symbol means \"allow additional arguments to be passed on to the `do_sim_once` function\n)\n{\n  \n  sim_results = NULL # we're going to create a \"tibble\" of results, \n  # row by row (slightly different from the hint on the homework)\n  \n  for (i in 1:n_sims)\n  {\n    \n    set.seed(i)\n    \n    current_results = \n      do_one_sim(...) |&gt; # here's where the simulation actually gets run\n      mutate(\n        sim_number = i\n      ) |&gt; \n      relocate(sim_number, .before = everything())\n    \n    sim_results = \n      sim_results |&gt; \n      bind_rows(current_results)\n    \n  }\n  \n  return(sim_results)\n}\n\n\n\nShow R code\nsim_results = do_n_sims(\n  n_sims = 100,\n  mu = mean(glucose_data),\n  sigma2 = var(glucose_data),  \n  n = 100 # this is the number of samples per simulated data set\n)\n\nsim_results |&gt; head(10)\n\n\n  \n\n\n\nThe simulation results are in! Now we have to analyze them.\nAnalyze simulation results\nTo do that, we write another function:\n\nShow R code\nsummarize_sim = function(\n    sim_results, \n    mu = mean(glucose_data),\n    sigma2 = var(glucose_data), \n    n = 100)\n{\n  \n  \n  # calculate the true standard error based on the data-generating parameters:\n  `se(mu-hat)` = sqrt(sigma2/n)\n  \n  sim_results |&gt; \n    summarize(\n      `bias[mu-hat]` = mean(`mu-hat`) - mu,\n      `SE(mu-hat)` = sd(`mu-hat`),\n      `bias[SE-hat]` = mean(se_hat) - `se(mu-hat)`,\n      `SE(SE-hat)` = sd(se_hat),\n      coverage = mean(`confint covers true mu`),\n      power = mean(`test rejects null hypothesis`)\n    )\n  \n}\n\n\nLet’s try it out:\n\nShow R code\nsim_summary = summarize_sim(\n  sim_results, \n  mu = mean(glucose_data), \n  # this function needs to know the true parameter values in order to assess bias\n  sigma2 = var(glucose_data), \n  n = 100)\n\nsim_summary\n\n\n  \n\n\n\nFrom this simulation, we observe that our estimate of \\(\\mu\\), \\(\\hat\\mu\\), has minimal bias, and so does our estimate of \\(SE(\\hat\\mu)\\), \\(\\hat{SE}(\\hat\\mu)\\).\nThe confidence intervals captured the true value even more often than they were supposed to, and the hypothesis test always rejected the null hypothesis.\nI wonder what would happen with a different sample size, a different true \\(\\mu\\) value, or a different \\(\\sigma^2\\) value…\n\n\n\n\n\n\nCasella, George, and Roger Berger. 2002. Statistical Inference. 2nd ed. Cengage Learning. https://www.cengage.com/c/statistical-inference-2e-casella-berger/9780534243128/.\n\n\nDobson, Annette J, and Adrian G Barnett. 2018. An Introduction to Generalized Linear Models. 4th ed. CRC press. https://doi.org/10.1201/9781315182780.\n\n\nEfron, Bradley, and David V Hinkley. 1978. “Assessing the Accuracy of the Maximum Likelihood Estimator: Observed Versus Expected Fisher Information.” Biometrika 65 (3): 457–83.\n\n\nMcLachlan, Geoffrey J, and Thriyambakam Krishnan. 2007. The EM Algorithm and Extensions. 2nd ed. John Wiley & Sons. https://doi.org/10.1002/9780470191613.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Introduction to Maximum Likelihood Inference</span>"
    ]
  },
  {
    "objectID": "intro-MLEs.html#footnotes",
    "href": "intro-MLEs.html#footnotes",
    "title": "Appendix E — Introduction to Maximum Likelihood Inference",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/Does_exactly_what_it_says_on_the_tin↩︎\na.k.a. the gradient↩︎\nI might sometimes switch the order of \\(x,\\) \\(\\theta\\); this is unintentional and not meaningful.↩︎\nnamed after mathematician Otto Hesse↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Introduction to Maximum Likelihood Inference</span>"
    ]
  },
  {
    "objectID": "common-mistakes.html",
    "href": "common-mistakes.html",
    "title": "Appendix F — Common Mistakes",
    "section": "",
    "text": "Configuring R\nFunctions from these packages will be used throughout this document:\nShow R codelibrary(conflicted) # check for conflicting function definitions\n# library(printr) # inserts help-file output into markdown output\nlibrary(rmarkdown) # Convert R Markdown documents into a variety of formats.\nlibrary(pander) # format tables for markdown\nlibrary(ggplot2) # graphics\nlibrary(ggeasy) # help with graphics\nlibrary(ggfortify) # help with graphics\nlibrary(dplyr) # manipulate data\nlibrary(tibble) # `tibble`s extend `data.frame`s\nlibrary(magrittr) # `%&gt;%` and other additional piping tools\nlibrary(haven) # import Stata files\nlibrary(knitr) # format R output for markdown\nlibrary(tidyr) # Tools to help to create tidy data\nlibrary(plotly) # interactive graphics\nlibrary(dobson) # datasets from Dobson and Barnett 2018\nlibrary(parameters) # format model output tables for markdown\nlibrary(haven) # import Stata files\nlibrary(latex2exp) # use LaTeX in R code (for figures and tables)\nlibrary(fs) # filesystem path manipulations\nlibrary(survival) # survival analysis\nlibrary(survminer) # survival analysis graphics\nlibrary(KMsurv) # datasets from Klein and Moeschberger\nlibrary(parameters) # format model output tables for\nlibrary(webshot2) # convert interactive content to static for pdf\nlibrary(forcats) # functions for categorical variables (\"factors\")\nlibrary(stringr) # functions for dealing with strings\nlibrary(lubridate) # functions for dealing with dates and times\nHere are some R settings I use in this document:\nShow R coderm(list = ls()) # delete any data that's already loaded into R\n\nconflicts_prefer(dplyr::filter)\nggplot2::theme_set(\n  ggplot2::theme_bw() + \n        # ggplot2::labs(col = \"\") +\n    ggplot2::theme(\n      legend.position = \"bottom\",\n      text = ggplot2::element_text(size = 12, family = \"serif\")))\n\nknitr::opts_chunk$set(message = FALSE)\noptions('digits' = 4)\n\npanderOptions(\"big.mark\", \",\")\npander::panderOptions(\"table.emphasize.rownames\", FALSE)\npander::panderOptions(\"table.split.table\", Inf)\nconflicts_prefer(dplyr::filter) # use the `filter()` function from dplyr() by default\nlegend_text_size = 9",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Common Mistakes</span>"
    ]
  },
  {
    "objectID": "common-mistakes.html#parameters-versus-random-variables",
    "href": "common-mistakes.html#parameters-versus-random-variables",
    "title": "Appendix F — Common Mistakes",
    "section": "\nF.1 Parameters versus random variables",
    "text": "F.1 Parameters versus random variables\nThe parameters of a probability distribution shouldn’t involve the random variables being modeled:\n\n\n\n\n\n\nThis is wrong\n\n\n\n\\[X \\sim Pois(\\lambda)\\] \\[\\hat{\\lambda}_{ML} \\rightarrow_D N(\\bar{X}, \\lambda/n)\\]\n\n\n\nSolution. \\[\\hat{\\lambda}_{ML} \\rightarrow_D N(\\lambda, \\lambda/n)\\]\n\nExpectations are means, not sums, despite the similarity of \\(\\Sigma\\) and \\(\\text{E}\\). Really, we should use \\(\\mu\\) instead of \\(\\text{E}\\).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Common Mistakes</span>"
    ]
  },
  {
    "objectID": "common-mistakes.html#quarto",
    "href": "common-mistakes.html#quarto",
    "title": "Appendix F — Common Mistakes",
    "section": "\nF.2 Quarto",
    "text": "F.2 Quarto\n\nF.2.1 \nMake sure not to put a div ::: on the next line after a slide break ---:\n---\n::: notes\n:::\nThere needs to be an empty line between them:\n---\n\n::: notes\n:::\n\nF.2.2 library(printr) currently breaks df-print: paged\n\nSee https://github.com/yihui/printr/issues/41",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Common Mistakes</span>"
    ]
  },
  {
    "objectID": "common-mistakes.html#latex",
    "href": "common-mistakes.html#latex",
    "title": "Appendix F — Common Mistakes",
    "section": "\nF.3 LaTeX",
    "text": "F.3 LaTeX\nDouble superscript issues: https://www.overleaf.com/learn/latex/Errors/Double_superscript",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>F</span>  <span class='chapter-title'>Common Mistakes</span>"
    ]
  },
  {
    "objectID": "notation.html",
    "href": "notation.html",
    "title": "Appendix G — Notation",
    "section": "",
    "text": "G.1 Information matrices\nThere is no consistency in the notation for observed and expected information.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Notation</span>"
    ]
  },
  {
    "objectID": "notation.html#information-matrices",
    "href": "notation.html#information-matrices",
    "title": "Appendix G — Notation",
    "section": "",
    "text": "book\nobserved information\nexpected information",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Notation</span>"
    ]
  },
  {
    "objectID": "notation.html#the-percent-sign",
    "href": "notation.html#the-percent-sign",
    "title": "Appendix G — Notation",
    "section": "G.2 The percent sign",
    "text": "G.2 The percent sign\nThe percent sign “%” is just a shorthand for “\\(\\times \\frac{1}{100}\\)”. The word “percent” comes from the Latin “per centum”; “centum” means 100 in Latin, so “percent” means “per hundred” (c.f., https://en.wikipedia.org/wiki/Percentage)\nSo, contrary to what you may have learned previously, \\(10\\% = 0.1\\) is a true and correct equality.\n\nProof. \\[\n\\begin{aligned}\n10\\% &= 10 \\times \\frac{1}{100}\n\\\\ &= \\frac{10}{100}\n\\\\ &= 0.1\n\\end{aligned}\n\\]",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Notation</span>"
    ]
  },
  {
    "objectID": "notation.html#footnotes",
    "href": "notation.html#footnotes",
    "title": "Appendix G — Notation",
    "section": "",
    "text": "depending on whether it is applied to a matrix or a function↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>G</span>  <span class='chapter-title'>Notation</span>"
    ]
  },
  {
    "objectID": "intro-to-R.html",
    "href": "intro-to-R.html",
    "title": "Appendix H — Statistical computing in R",
    "section": "",
    "text": "H.1 Functions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Statistical computing in R</span>"
    ]
  },
  {
    "objectID": "intro-to-R.html#functions",
    "href": "intro-to-R.html#functions",
    "title": "Appendix H — Statistical computing in R",
    "section": "",
    "text": "Read this ASAP: https://r4ds.hadley.nz/functions.html\nUse this as a reference: https://adv-r.hadley.nz/functions.html\n\n\nH.1.1 Methods versus functions\nSee https://adv-r.hadley.nz/oo.html#oop-systems\n\n\nH.1.2 Debugging R and C code\nSee https://www.maths.ed.ac.uk/~swood34/RCdebug/RCdebug.html",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Statistical computing in R</span>"
    ]
  },
  {
    "objectID": "intro-to-R.html#data.frames-and-tibbles",
    "href": "intro-to-R.html#data.frames-and-tibbles",
    "title": "Appendix H — Statistical computing in R",
    "section": "H.2 data.frames and tibbles",
    "text": "H.2 data.frames and tibbles\n\nH.2.1 Displaying tibbles\nSee vignette(\"digits\", package = \"tibble\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Statistical computing in R</span>"
    ]
  },
  {
    "objectID": "intro-to-R.html#the-tidyverse",
    "href": "intro-to-R.html#the-tidyverse",
    "title": "Appendix H — Statistical computing in R",
    "section": "H.3 The tidyverse",
    "text": "H.3 The tidyverse\n\nThe tidyverse is an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures.\n\n\nhttps://www.tidyverse.org/\n\nThese packages are being actively developed by Hadley Wickham and his colleagues at posit1.\nDetails:\n\nWickham et al. (2019)\nWickham, Çetinkaya-Rundel, and Grolemund (2023)\nKuhn and Silge (2022)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Statistical computing in R</span>"
    ]
  },
  {
    "objectID": "intro-to-R.html#piping",
    "href": "intro-to-R.html#piping",
    "title": "Appendix H — Statistical computing in R",
    "section": "H.4 Piping",
    "text": "H.4 Piping\nSee Wickham, Çetinkaya-Rundel, and Grolemund (2023) for details.\nThere are currently (2024) two commonly-used pipe operators in R:\n\n%&gt;%: the “magrittr pipe”, from the magrittr package (Bache and Wickham (2022); re-exported by dplyr and others) .\n|&gt;: the “native pipe”, from base R (≥4.1.0)\n\n\nH.4.1 Which pipe should I use?\nWickham, Çetinkaya-Rundel, and Grolemund (2023) recommends the native pipe:\n\nFor simple cases, |&gt; and %&gt;% behave identically. So why do we recommend the base pipe? Firstly, because it’s part of base R, it’s always available for you to use, even when you’re not using the tidyverse. Secondly, |&gt; is quite a bit simpler than %&gt;%: in the time between the invention of %&gt;% in 2014 and the inclusion of |&gt; in R 4.1.0 in 2021, we gained a better understanding of the pipe. This allowed the base implementation to jettison infrequently used and less important features.\n\n\n\nH.4.2 Why doesn’t ggplot2 use piping?\nHere’s tidyverse creator Hadley Wickham’s answer (from 2018):\n\nI think it’s worth unpacking this question into a few smaller pieces:\n\nShould ggplot2 use the pipe? IMO, yes.\nCould ggplot2 support both the pipe and plus? No\nWould it be worth it to create a ggplot3 that uses the pipe? No.\n\n\nhttps://forum.posit.co/t/why-cant-ggplot2-use/4372/7",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Statistical computing in R</span>"
    ]
  },
  {
    "objectID": "intro-to-R.html#quarto",
    "href": "intro-to-R.html#quarto",
    "title": "Appendix H — Statistical computing in R",
    "section": "H.5 Quarto",
    "text": "H.5 Quarto\nQuarto is a system for writing documents with embedded R code and/or results:\n\nRead this ASAP: https://r4ds.hadley.nz/communicate\nThen use this for reference: https://quarto.org/docs/reference/",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Statistical computing in R</span>"
    ]
  },
  {
    "objectID": "intro-to-R.html#packages",
    "href": "intro-to-R.html#packages",
    "title": "Appendix H — Statistical computing in R",
    "section": "H.6 Packages",
    "text": "H.6 Packages\n\nThis book espouses our philosophy of package development: anything that can be automated, should be automated. Do as little as possible by hand. Do as much as possible with functions. The goal is to spend your time thinking about what you want your package to do rather than thinking about the minutiae of package structure.\n\n\nhttps://r-pkgs.org/introduction.html#:~:text=This%20book%20espouses,of%20package%20structure.\nRead this ASAP: https://r-pkgs.org/whole-game.html\nUse the rest of Wickham and Bryan (2023) as a reference",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Statistical computing in R</span>"
    ]
  },
  {
    "objectID": "intro-to-R.html#git",
    "href": "intro-to-R.html#git",
    "title": "Appendix H — Statistical computing in R",
    "section": "H.7 Git",
    "text": "H.7 Git\n94% of respondents to a 2022 Stack Overflow survey reported using git for version control link\nMore details\n\nhttps://happygitwithr.com/\nhttps://usethis.r-lib.org/articles/pr-functions.html",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Statistical computing in R</span>"
    ]
  },
  {
    "objectID": "intro-to-R.html#spatial-data-science",
    "href": "intro-to-R.html#spatial-data-science",
    "title": "Appendix H — Statistical computing in R",
    "section": "H.8 Spatial data science",
    "text": "H.8 Spatial data science\n\nPebesma and Bivand (2023)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Statistical computing in R</span>"
    ]
  },
  {
    "objectID": "intro-to-R.html#shiny-apps",
    "href": "intro-to-R.html#shiny-apps",
    "title": "Appendix H — Statistical computing in R",
    "section": "H.9 Shiny apps",
    "text": "H.9 Shiny apps\n\nRead this first: Wickham (2021)\nUse this as a reference: Fay et al. (2021)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Statistical computing in R</span>"
    ]
  },
  {
    "objectID": "intro-to-R.html#other-resources-for-learning-r",
    "href": "intro-to-R.html#other-resources-for-learning-r",
    "title": "Appendix H — Statistical computing in R",
    "section": "H.10 Other resources for learning R",
    "text": "H.10 Other resources for learning R\n\nNahhas (2023) (same author as Nahhas (2024))\n\n\n\n\n\n\n\nBache, Stefan Milton, and Hadley Wickham. 2022. Magrittr: A Forward-Pipe Operator for r. https://CRAN.R-project.org/package=magrittr.\n\n\nChang, Winston. 2024. R Graphics Cookbook: Practical Recipes for Visualizing Data. O’Reilly Media. https://r-graphics.org/.\n\n\nFay, Colin, Sébastien Rochette, Vincent Guyader, and Cervan Girard. 2021. Engineering Production-Grade Shiny Apps. Chapman; Hall/CRC. https://engineering-shiny.org/.\n\n\nKleinman, Ken, and Nicholas J Horton. 2009. SAS and r: Data Management, Statistical Analysis, and Graphics. Chapman; Hall/CRC. https://www.routledge.com/SAS-and-R-Data-Management-Statistical-Analysis-and-Graphics-Second-Edition/Kleinman-Horton/p/book/9781466584495.\n\n\nKuhn, Max, and Julia Silge. 2022. Tidy Modeling with r. \" O’Reilly Media, Inc.\". https://www.tmwr.org/.\n\n\nNahhas, Ramzi W. 2023. An Introduction to r for Research. https://bookdown.org/rwnahhas/IntroToR/.\n\n\n———. 2024. Introduction to Regression Methods for Public Health Using r. CRC Press. https://www.bookdown.org/rwnahhas/RMPH/.\n\n\nPebesma, Edzer, and Roger Bivand. 2023. Spatial Data Science: With Applications in R. Boca Raton: Chapman; Hall/CRC. https://doi.org/10.1201/9780429459016.\n\n\nWickham, Hadley. 2019. Advanced r. Chapman; Hall/CRC. https://adv-r.hadley.nz/index.html.\n\n\n———. 2021. Mastering Shiny. \" O’Reilly Media, Inc.\". https://mastering-shiny.org/.\n\n\nWickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686.\n\n\nWickham, Hadley, and Jennifer Bryan. 2023. R Packages. \" O’Reilly Media, Inc.\". https://r-pkgs.org/.\n\n\nWickham, Hadley, Mine Çetinkaya-Rundel, and Garrett Grolemund. 2023. R for Data Science. \" O’Reilly Media, Inc.\". https://r4ds.hadley.nz/.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Statistical computing in R</span>"
    ]
  },
  {
    "objectID": "intro-to-R.html#footnotes",
    "href": "intro-to-R.html#footnotes",
    "title": "Appendix H — Statistical computing in R",
    "section": "",
    "text": "the company formerly known as RStudio↩︎",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>H</span>  <span class='chapter-title'>Statistical computing in R</span>"
    ]
  },
  {
    "objectID": "Contributing.html",
    "href": "Contributing.html",
    "title": "Appendix I — Contributing to rme",
    "section": "",
    "text": "I.1 Style guide",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Contributing to `rme`</span>"
    ]
  },
  {
    "objectID": "Contributing.html#style-guide",
    "href": "Contributing.html#style-guide",
    "title": "Appendix I — Contributing to rme",
    "section": "",
    "text": "Every abstract concept (definition or theorem) should have at least one concrete example immediately following it.\nMore structure (headers, labels) is better.\nMake each conceptual chunk as compact as possible:\n\nDecompose large, complicated, difficult concepts into smaller, simpler, and easier pieces.\nDecompose long derivations into smaller lemmas.\nWhen manipulating part of a larger expression, isolate that part in a lemma.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Contributing to `rme`</span>"
    ]
  },
  {
    "objectID": "Contributing.html#fixing-typos",
    "href": "Contributing.html#fixing-typos",
    "title": "Appendix I — Contributing to rme",
    "section": "I.2 Fixing typos",
    "text": "I.2 Fixing typos\nThis book is written using Quarto. You can fix typos, spelling mistakes, or grammatical errors directly using the GitHub web interface by making changes in the corresponding source file. This generally means you’ll need to edit a .qmd file.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Contributing to `rme`</span>"
    ]
  },
  {
    "objectID": "Contributing.html#bigger-changes",
    "href": "Contributing.html#bigger-changes",
    "title": "Appendix I — Contributing to rme",
    "section": "I.3 Bigger changes",
    "text": "I.3 Bigger changes\nIf you want to make a bigger change, it’s a good idea to first file an issue and make sure someone from the development team agrees that it’s needed.\n\nI.3.1 Pull request process\n\nFork the package and clone onto your computer. If you haven’t done this before, we recommend using usethis::create_from_github(\"d-morrison/rme\", fork = TRUE).\nInstall all development dependencies with devtools::install_dev_deps(). Make sure you can build the book by running quarto render in a Terminal.\nCreate a Git branch for your pull request (PR). We recommend using usethis::pr_init(\"brief-description-of-change\"). Details at https://usethis.r-lib.org/articles/pr-functions.html\nMake your changes, commit to git, and then create a PR by running usethis::pr_push(), and following the prompts in your browser. The title of your PR should briefly describe the change. The body of your PR should contain Fixes #issue-number.\nAdd a bullet to the top of NEWS.md (i.e. just below the first header). Follow the style described in https://style.tidyverse.org/news.html.\n\n\n\nI.3.2 Code style\n\nNew code should follow the tidyverse style guide. You can use the styler package to apply these styles, but please don’t restyle code that has nothing to do with your PR.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Contributing to `rme`</span>"
    ]
  },
  {
    "objectID": "Contributing.html#code-of-conduct",
    "href": "Contributing.html#code-of-conduct",
    "title": "Appendix I — Contributing to rme",
    "section": "I.4 Code of Conduct",
    "text": "I.4 Code of Conduct\nPlease note that the rme project is released with a Contributor Code of Conduct. By contributing to this project you agree to abide by its terms.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Contributing to `rme`</span>"
    ]
  },
  {
    "objectID": "Contributing.html#additional-references",
    "href": "Contributing.html#additional-references",
    "title": "Appendix I — Contributing to rme",
    "section": "I.5 Additional references",
    "text": "I.5 Additional references\nFor a detailed discussion on contributing to this and other projects, please see the Tidyverse development contributing guide and the Tidyverse code review principles. This project is not part of the tidyverse, but we have borrowed their development processes.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>I</span>  <span class='chapter-title'>Contributing to `rme`</span>"
    ]
  }
]